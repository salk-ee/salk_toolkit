{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Pipeline\n",
    "> Pipeline from raw survey data file up to creating the plot\n",
    "> Built around the use of plot registry from plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import scipy.stats as sps\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "from salk_toolkit.utils import *\n",
    "from salk_toolkit.io import load_parquet_with_metadata, extract_column_meta, group_columns_dict, list_aliases, read_annotated_data, read_json, read_annotated_data_lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Augment each draw with bootstrap data from across whole population to make sure there are at least <threshold> samples\n",
    "def augment_draws(data, factors=None, n_draws=None, threshold=50):\n",
    "    if n_draws == None: n_draws = data.draw.max()+1\n",
    "    \n",
    "    if factors: # Run recursively on each factor separately and concatenate results\n",
    "        if data[ ['draw']+factors ].value_counts().min() >= threshold: return data # This takes care of large datasets fast\n",
    "        return data.groupby(factors,observed=False).apply(augment_draws,n_draws=n_draws,threshold=threshold).reset_index(drop=True) # Slow-ish, but only needed on small data now\n",
    "    \n",
    "    # Get count of values for each draw\n",
    "    draw_counts = data['draw'].value_counts() # Get value counts of existing draws\n",
    "    if len(draw_counts)<n_draws: # Fill in completely missing draws\n",
    "        draw_counts = (draw_counts + pd.Series(0,index=range(n_draws))).fillna(0).astype(int)\n",
    "        \n",
    "    # If no new draws needed, just return original\n",
    "    if draw_counts.min()>=threshold: return data\n",
    "    \n",
    "    # Generate an index for new draws\n",
    "    new_draws = [ d for d,c in draw_counts[draw_counts<threshold].items() for _ in range(threshold-c) ]\n",
    "\n",
    "    # Generate new draws\n",
    "    new_rows = data.iloc[np.random.choice(len(data),len(new_draws)),:].copy()\n",
    "    new_rows['draw'] = new_draws\n",
    "    \n",
    "    return pd.concat([data, new_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Get the numerical values to map categories to\n",
    "def get_cat_num_vals(res_meta,pp_desc):\n",
    "    try: # First try to convert categories themselves to numbers. Because they might be in some use cases ;) \n",
    "        nvals = [ float(x) for x in res_meta['categories'] ]\n",
    "    except ValueError: # Instead default to 0,1,2,3... scale\n",
    "        nvals = res_meta.get('num_values',range(len(res_meta['categories'])))\n",
    "    if 'num_values' in pp_desc: nvals = pp_desc['num_values'] \n",
    "    return nvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot registry functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "special_columns = ['id', 'weight', 'draw', 'training_subsample', 'original_inds', '__index_level_0__', 'group_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "registry = {}\n",
    "registry_meta = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For in-notebook testing we need to import the regisry from the py files\n",
    "from salk_toolkit.pp import registry, registry_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "stk_plot_defaults = { 'data_format': 'longform' }\n",
    "\n",
    "# Decorator for registering a plot type with metadata\n",
    "def stk_plot(plot_name, **r_kwargs):\n",
    "    \n",
    "    def decorator(gfunc):\n",
    "        # In theory, we could do transformations in wrapper\n",
    "        # In practice, it would only obfuscate already complicated code\n",
    "        #def wrapper(*args,**kwargs) :\n",
    "        #    return gfunc(*args,**kwargs)\n",
    "\n",
    "        # Register the function\n",
    "        registry[plot_name] = gfunc\n",
    "        registry_meta[plot_name] = { 'name': plot_name, **stk_plot_defaults, **r_kwargs }\n",
    "        \n",
    "        return gfunc\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "def stk_deregister(plot_name):\n",
    "    del registry[plot_name]\n",
    "    del registry_meta[plot_name]\n",
    "\n",
    "def get_plot_fn(plot_name):\n",
    "    return registry[plot_name]\n",
    "\n",
    "def get_plot_meta(plot_name):\n",
    "    return registry_meta[plot_name].copy()\n",
    "\n",
    "def get_all_plots():\n",
    "    return sorted(list(registry.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# First is weight if not matching, second if match\n",
    "# This is very much a placeholder right now\n",
    "n_a = -1000000\n",
    "priority_weights = {\n",
    "    'draws': [n_a, 50],\n",
    "    'nonnegative': [n_a, 50],\n",
    "    'hidden': [n_a, 0],\n",
    "    \n",
    "    'ordered': [n_a, 100],\n",
    "    'likert': [n_a, 200],\n",
    "    'required_meta': [n_a, 500],\n",
    "}\n",
    "\n",
    "# Method for choosing a sensible default plot based on the data and plot metadata\n",
    "def calculate_priority(plot_meta, match):\n",
    "    priority, reasons = plot_meta.get('priority',0), []\n",
    "\n",
    "    facet_metas = match['facet_metas']\n",
    "    if plot_meta.get('no_question_facet'):\n",
    "        facet_metas = [ f for f in facet_metas if f['name'] not in ['question',match['res_col']]]\n",
    "\n",
    "    # Plots with raw data assume numerical values so remove them as options\n",
    "    if match['categorical'] and plot_meta.get('data_format')=='raw': return n_a, ['raw_data']\n",
    "\n",
    "    if len(facet_metas)<plot_meta.get('n_facets',(0,0))[0]: \n",
    "        return n_a, ['n_facets'] # Not enough factors\n",
    "    else: # Prioritize plots that have the right number of factors\n",
    "        priority += 10*abs(len(facet_metas)-plot_meta.get('n_facets',(0,0))[1])\n",
    "\n",
    "    for k in ['draws','nonnegative','hidden']:\n",
    "        if plot_meta.get(k):\n",
    "            val = priority_weights[k][1 if match.get(k) else 0]\n",
    "            if val < 0: reasons.append(k)\n",
    "            priority += val\n",
    "\n",
    "    for i, d in enumerate(plot_meta.get('requires',[])):\n",
    "        md = facet_metas[i]\n",
    "        for k, v in d.items():\n",
    "            if v!='pass': val = priority_weights[k][1 if md.get(k)==v else 0]\n",
    "            else: val = priority_weights['required_meta'][1 if k in md else 0] # Use these weights for things plots require from metadata\n",
    "\n",
    "            if k == 'ordered' and md.get('continuous'): val = priority_weights[k][1] # Continuous is turned into ordered categoricals for facets\n",
    "            if val < 0: reasons.append(k)\n",
    "            priority += val\n",
    "\n",
    "    return priority, reasons\n",
    "\n",
    "\n",
    "# Allow pp_desc to modify data meta\n",
    "def update_data_meta_with_pp_desc(data_meta, pp_desc):\n",
    "    if pp_desc.get('res_meta'):\n",
    "        data_meta, rmeta = deepcopy(data_meta), deepcopy(pp_desc['res_meta'])\n",
    "        rmeta = replace_constants(rmeta,data_meta.get('constants',{}))\n",
    "        data_meta['structure'].append(rmeta)\n",
    "    return data_meta\n",
    "\n",
    "# Get a list of plot types matching required spec\n",
    "def matching_plots(pp_desc, df, data_meta, details=False, list_hidden=False):\n",
    "    data_meta = update_data_meta_with_pp_desc(data_meta, pp_desc)\n",
    "    col_meta = extract_column_meta(data_meta)\n",
    "    \n",
    "    rc = pp_desc['res_col']\n",
    "    rcm = col_meta[rc]\n",
    "\n",
    "    lazy = isinstance(df,pl.LazyFrame)\n",
    "    if lazy: df_cols = df.collect_schema().names()\n",
    "    else: df_cols = df.columns\n",
    "\n",
    "    # Determine if values are non-negative\n",
    "    ocols = rcm['columns'] if 'columns' in rcm else [rc]\n",
    "    cols = [ c for c in ocols if c in df_cols ]\n",
    "    if not cols: raise ValueError(f\"Columns {ocols} not found in data\")\n",
    "\n",
    "    nonneg = ('categories' in rcm) or (\n",
    "        df[cols].min(axis=None)>=0 if not lazy else \n",
    "        df.select(pl.min_horizontal(pl.col(cols).min())).collect().item()>=0)\n",
    "\n",
    "    if pp_desc.get('convert_res')=='continuous' and ('categories' in rcm):\n",
    "        nonneg = min([ v for v in get_cat_num_vals(rcm,pp_desc) if v is not None ])>=0\n",
    "\n",
    "    match = {\n",
    "        'draws': ('draw' in df_cols),\n",
    "        'nonnegative': nonneg,\n",
    "        'hidden': list_hidden,\n",
    "\n",
    "        'res_col': rc,\n",
    "        'categorical': ('categories' in rcm) and pp_desc.get('convert_res')!='continuous',\n",
    "        'facet_metas': [ {'name':cn, **col_meta[cn]} for cn in pp_desc['factor_cols']]\n",
    "    }\n",
    "    \n",
    "    res = [ ( pn, *calculate_priority(get_plot_meta(pn),match)) for pn in registry.keys() ]\n",
    "    \n",
    "    if details: return { n: (p, i) for (n, p, i) in res } # Return dict with priorities and failure reasons\n",
    "    else: return [ n for (n,p,i) in sorted(res,key=lambda t: t[1], reverse=True) if p >= 0 ] # Return list of possibilities in decreasing order of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a basic bootstrapped datase\n",
    "data_uri = '../../salk_internal_package/samples/bootstrap.parquet'\n",
    "full_df, f_meta = load_parquet_with_metadata(data_uri)\n",
    "data_meta = f_meta['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_plots({\n",
    "  \"res_col\": \"party_preference\",\n",
    "  \"factor_cols\": [\n",
    "    \"party_preference\"\n",
    "  ],\n",
    "  \"internal_facet\": True,\n",
    "  #\"plot\": \"density\",\n",
    "  \"plot_args\": {\n",
    "    \"stacked\": False\n",
    "  },\n",
    "  \"filter\": {}\n",
    "},full_df, data_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metafile directly - allows faster iteration\n",
    "data_metafile = '../data/master_meta.json'\n",
    "if data_metafile:\n",
    "    from salk_toolkit.utils import replace_constants\n",
    "    data_meta = read_json(data_metafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Mechanics to allow row-wise numpy transformations here\n",
    "# They are noticeably slower, so only use them if polars expression is infeasible\n",
    "custom_row_transforms = {}\n",
    "def apply_npf_on_pl_df(df,cols,npf):\n",
    "    df[cols] = npf(df[cols].to_numpy())\n",
    "    return df\n",
    "\n",
    "# Polars is annoyingly verbose for these but it is fast enough to be worth it\n",
    "def transform_cont(data, cols, transform, val_format='.1f', val_range=None):\n",
    "    if not transform: return data, val_format, val_range\n",
    "    elif transform == 'center': \n",
    "        return data.with_columns(pl.col(cols) - pl.col(cols).mean()), val_format, None\n",
    "    elif transform == 'zscore': \n",
    "        return data.with_columns((pl.col(cols) - pl.col(cols).mean()) / pl.col(cols).std(0)), '.2f', None\n",
    "    elif transform == 'proportion': \n",
    "        return data.with_columns(pl.col(cols)/pl.sum_horizontal(pl.col(cols).abs())), '.1%', (0.0,1.0)\n",
    "    elif transform in ['softmax','softmax-ratio']: \n",
    "        mult, val_format = (len(cols),'.1f') if transform == 'softmax-ratio' else (1.0,'.1%') # Ratio is just a multiplier\n",
    "        return data.with_columns(pl.col(cols).exp()*mult / pl.sum_horizontal(pl.col(cols).exp())), val_format, (0.0,1.0*mult)\n",
    "    elif transform in custom_row_transforms:\n",
    "        tfunc, fmt = custom_row_transforms[transform]\n",
    "        data = data.map_batches(lambda bdf: \n",
    "                    apply_npf_on_pl_df(bdf, cols, tfunc),\n",
    "                    streamable=True, validate_output_schema=False) # NB! Set validate to true if debugging this\n",
    "        return data,fmt,None\n",
    "        \n",
    "    else: raise Exception(f\"Unknown transform '{transform}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Expected rank given Plackett-luce (softmax) log-odds\n",
    "# Relies on the fact that sum of probs of pairwise comparisons is average rank\n",
    "def softmax_expected_ranks(p):\n",
    "    \n",
    "    # Convert from log-odds to proportions, but reverse probabilities\n",
    "    p = np.exp(-p)\n",
    "    \n",
    "    # Create a matrix where element [i,j] is p[j]/(p[i] + p[j])\n",
    "    sum_matrix = p[...,:,None] + p[...,None,:] + 1e-10 # Shape (..., n, n)\n",
    "    m = p[...,None,:] / sum_matrix\n",
    "\n",
    "    # Sum over columns\n",
    "    sums = m.sum(axis=-1)\n",
    "\n",
    "    # Subtract diagonal term (0.5) and add 1\n",
    "    expected_ranks = 1 + (sums - 0.5)  \n",
    "    return expected_ranks\n",
    "\n",
    "custom_row_transforms['softmax-avgrank'] = softmax_expected_ranks,'.1f'\n",
    "\n",
    "# Average rank order \n",
    "def avg_rank(ovs):\n",
    "    return 1+np.argsort(np.argsort(ovs,axis=1),axis=1) \n",
    "    # Rankdata is insanely slow for some reason\n",
    "    #return sps.rankdata(ovs, axis=1, method='average')\n",
    "\n",
    "def highest_ranked(ovs):\n",
    "    return (ovs == np.max(ovs,axis=1)[:,None]).astype('int')\n",
    "\n",
    "def topk_ranked(ovs,k=3): # Todo: make this k changeable with pp_desc\n",
    "    return (np.argsort(np.argsort(ovs,axis=1),axis=1)>=ovs.shape[1]-k)\n",
    "\n",
    "custom_row_transforms['ordered-avgrank'] = avg_rank,'.1f'\n",
    "custom_row_transforms['ordered-top1'] = highest_ranked,'.1%'\n",
    "custom_row_transforms['ordered-top2'] = lambda ovs: topk_ranked(ovs,2),'.1%'\n",
    "custom_row_transforms['ordered-top3'] = lambda ovs: topk_ranked(ovs,3),'.1%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cont_transform_options = ['center','zscore','proportion','softmax','softmax-ratio'] + list(custom_row_transforms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Get categories from a lazy frame. \n",
    "def ensure_ldf_categories(col_meta, col, ldf):\n",
    "    cats = col_meta[col]['categories']\n",
    "    if cats == 'infer':\n",
    "        # This is slow and is intended as a fallback as categories should be available in the data_meta\n",
    "        cats =  np.sort(ldf.select(pl.col(col).unique()).collect().to_pandas()[col].values)\n",
    "        col_meta[col]['categories'] = cats\n",
    "    return col_meta[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def pp_filter_data_lz(df, filter_dict, c_meta):\n",
    "\n",
    "    colnames = df.collect_schema().names()\n",
    "\n",
    "    inds = True\n",
    "\n",
    "    for k, v in filter_dict.items():\n",
    "        \n",
    "        # Range filters have form [None,start,end]\n",
    "        is_range = isinstance(v,list) and v[0] is None and len(v)==3\n",
    "\n",
    "        # Handle continuous variables separately\n",
    "        if is_range and (not isinstance(v[1],str) or c_meta[k].get('continuous') or c_meta[k].get('datetime')): # Only special case where we actually need a range\n",
    "            if v[1] is not None: inds = (pl.col(k)>=v[1]) & inds\n",
    "            if v[2] is not None: inds = (pl.col(k)<=v[2]) & inds\n",
    "            continue # NB! this approach does not work for ordered categoricals with polars LazyDataFrame, hence handling that separately below\n",
    "        \n",
    "        # Handle categoricals\n",
    "        if is_range: # Range of values over ordered categorical\n",
    "            cats = ensure_ldf_categories(c_meta,k,df)['categories']\n",
    "            if set(v[1:]) & set(cats) != set(v[1:]): \n",
    "                warn(f'Column {k} values {v} not found in {cats}, not filtering')\n",
    "                flst = cats\n",
    "            else:\n",
    "                bi, ei = cats.index(v[1]), cats.index(v[2])\n",
    "                flst = cats[bi:ei+1] # \n",
    "        elif isinstance(v,list): flst = v # List indicates a set of values\n",
    "        elif 'groups' in c_meta[k] and v in c_meta[k]['groups']:\n",
    "            flst = c_meta[k]['groups'][v]\n",
    "        else: flst = [v] # Just filter on single value    \n",
    "            \n",
    "        inds &=  pl.col(k).is_in(flst) & ~pl.col(k).is_null()\n",
    "            \n",
    "    filtered_df = df.filter(inds)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# This is a wrapper that allows the filter to work on pandas DataFrames\n",
    "def pp_filter_data(df, filter_dict, c_meta):\n",
    "    return pp_filter_data_lz(pl.DataFrame(df).lazy(), filter_dict, c_meta).collect().to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# While polars-ized, it is still slow because of the collects. \n",
    "# This can likely be improved by batching all of the required collects into a single select (over all columns)\n",
    "# In practice, this is probably not worth it because this is not used very often\n",
    "def discretize_continuous(ldf, col, col_meta={}):\n",
    "    if 'bin_breaks' in col_meta and 'bin_labels' in col_meta:\n",
    "        breaks, labels = col_meta['bin_breaks'], col_meta['bin_labels']\n",
    "        ldf = ldf.with_columns(pl.col(col).cut(breaks[1:-1], labels=labels, left_closed=True).cast(pl.Categorical))\n",
    "    else:\n",
    "        breaks = col_meta.get('bin_breaks',5)\n",
    "        fmt = col_meta.get('val_format','.1f') \n",
    "        if False: # Precise computation is slow\n",
    "            if isinstance(breaks,int): # This requires computing quantiles for each break - slow\n",
    "                breaks = list(np.unique([\n",
    "                    ldf.select([pl.col(col).quantile(br).alias(str(br))  \n",
    "                    for br in np.linspace(0,1,breaks+1) ]).collect().to_pandas().values.T\n",
    "                ]))\n",
    "            mima = ldf.select([pl.col(col).min().alias('min'),pl.col(col).max().alias('max')]).collect().to_pandas()\n",
    "            mi, ma = mima['min'].values[0], mima['max'].values[0]\n",
    "        else: # Approximate computation is considerably faster\n",
    "            nbreaks = len(breaks) if isinstance(breaks,list) else breaks\n",
    "            vals = ldf.select(pl.col(col).sample(200*nbreaks,with_replacement=True)).collect().to_pandas()[col].values\n",
    "            if isinstance(breaks,int):\n",
    "                breaks = list(np.unique(np.quantile(vals,np.linspace(0,1,breaks+1))))\n",
    "            mi, ma = vals.min(), vals.max()\n",
    "\n",
    "        isint = ldf.collect_schema()[col].is_integer()\n",
    "        breaks, labels = cut_nice_labels(breaks, mi, ma, isint, fmt)\n",
    "        ldf = ldf.with_columns(pl.col(col).cut(breaks[1:-1], labels=labels, left_closed=True).cast(pl.Categorical))\n",
    "        \n",
    "    return ldf, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Get all data required for a given graph\n",
    "# Only return columns and rows that are needed, aggregated to the format plot requires\n",
    "# Internally works with polars LazyDataFrame for large data set performance\n",
    "\n",
    "def pp_transform_data(full_df, data_meta, pp_desc, columns=[]):\n",
    "    pl.enable_string_cache() # So we can work on categorical columns\n",
    "\n",
    "    plot_meta = get_plot_meta(pp_desc['plot'])\n",
    "    \n",
    "    data_meta = update_data_meta_with_pp_desc(data_meta, pp_desc)\n",
    "    gc_dict = group_columns_dict(data_meta)\n",
    "    c_meta = extract_column_meta(data_meta)\n",
    "\n",
    "    # Setup lazy frame if not already:\n",
    "    if not isinstance(full_df,pl.LazyFrame):\n",
    "        full_df = pl.DataFrame(full_df).lazy()\n",
    "\n",
    "    schema = full_df.collect_schema()\n",
    "    all_col_names = schema.names()\n",
    "    \n",
    "    # Figure out which columns we actually need\n",
    "    weight_col = data_meta.get('weight_col','row_weights')\n",
    "    factor_cols = pp_desc.get('factor_cols',[]).copy()\n",
    "\n",
    "    # Ensure weight column is present (fill with 1.0 if not)\n",
    "    if weight_col not in all_col_names: \n",
    "        full_df = full_df.with_columns(pl.lit(1.0).alias(weight_col))\n",
    "        all_col_names += [weight_col]\n",
    "    else: \n",
    "        full_df = full_df.with_columns(pl.col(weight_col).fill_null(1.0))\n",
    "        \n",
    "    # For transforming purposest, res_col is not a factor. \n",
    "    # It will be made one for categorical plots for plotting part, but for pp_transform_data, remove it\n",
    "    if pp_desc['res_col'] in factor_cols: factor_cols.remove(pp_desc['res_col']) \n",
    "    \n",
    "    extra_cols = columns + ([ weight_col ] +\n",
    "                    (['training_subsample'] if not pp_desc.get('poststrat',True) else []) +\n",
    "                    (['draw'] if plot_meta.get('draws') else []))\n",
    "    cols = [ pp_desc['res_col'] ]  + factor_cols + list(pp_desc.get('filter',{}).keys())\n",
    "    cols += [ c for c in extra_cols if c in all_col_names and c not in cols ]\n",
    "\n",
    "    # If any aliases are used, cconvert them to column names according to the data_meta\n",
    "    cols = [ c for c in np.unique(list_aliases(cols,gc_dict)) if c in all_col_names ]\n",
    "\n",
    "    # Remove draws_data if calcualted_draws is disabled       \n",
    "    if not pp_desc.get('calculated_draws',True):\n",
    "        data_meta = data_meta.copy()\n",
    "        del data_meta['draws_data']\n",
    "    \n",
    "    df = full_df.select(cols) # Select only the columns we need\n",
    "    total_n = df.select(pl.len()).collect().item()\n",
    "\n",
    "    # Add row id-s - needs to happen before filtering\n",
    "    df = df.with_row_count('id')\n",
    "    \n",
    "    # Filter the data with given filters\n",
    "    if pp_desc.get('filter'):\n",
    "        filtered_df = pp_filter_data_lz(df, pp_desc.get('filter',{}), c_meta)\n",
    "    else: filtered_df = df\n",
    "\n",
    "    # If we want to approximate original data without poststrat, filter to training subsample\n",
    "    if (not pp_desc.get('poststrat',True)) and 'training_subsample' in cols:\n",
    "        filtered_df = filtered_df.filter(pl.col('training_subsample'))\n",
    "\n",
    "    # Discretize factor columns that are numeric\n",
    "    for c in factor_cols:\n",
    "        if c in cols and schema[c].is_numeric():    \n",
    "            filtered_df, labels = discretize_continuous(filtered_df,c,c_meta.get(c,{}))\n",
    "            # Make sure it gets restored to pandas properly\n",
    "            c_meta[c].update({ 'categories': labels, 'ordered': True, 'continuous': False })\n",
    "\n",
    "    # Sample from filtered data\n",
    "    if 'sample' in pp_desc: filtered_df = filtered_df.sample(n=pp_desc['sample'], with_replacement=True)\n",
    "    \n",
    "    # Convert ordered categorical to continuous if we can\n",
    "    rcl = gc_dict.get(pp_desc['res_col'], [pp_desc['res_col']])\n",
    "    rcl = [ c for c in rcl if c in cols]\n",
    "    for rc in rcl:\n",
    "        res_meta = c_meta[rc]\n",
    "        if pp_desc.get('convert_res') == 'continuous' and res_meta.get('ordered'):\n",
    "            res_meta = ensure_ldf_categories(c_meta,rc,filtered_df)\n",
    "            nvals = get_cat_num_vals(res_meta,pp_desc)\n",
    "            cmap = dict(zip(res_meta['categories'],nvals))\n",
    "            filtered_df = filtered_df.with_columns(pl.col(rc).cast(pl.String).replace(cmap).cast(pl.Float32).fill_nan(None))\n",
    "            nvals = np.array(nvals,dtype='float') # To handle null as nan\n",
    "            update = {  'continuous': True, 'categories': None, \n",
    "                        'val_range': (np.nanmin(nvals),np.nanmax(nvals)) }\n",
    "            c_meta[rc].update(update); c_meta[pp_desc['res_col']].update(update)\n",
    "            \n",
    "    # Apply continuous transformation - needs to happen when data still in table form\n",
    "    if c_meta[rcl[0]].get('continuous'):\n",
    "        val_format, val_range = c_meta[rcl[0]].get('val_format') or '.1f', None\n",
    "        if 'cont_transform' in pp_desc:\n",
    "            filtered_df, val_format, val_range = transform_cont(filtered_df, rcl, transform=pp_desc['cont_transform'], \n",
    "                                                                val_format=val_format, val_range=c_meta[rcl[0]].get('val_range'))\n",
    "    else: val_format, val_range = '.1%', None # Categoricals report %\n",
    "    val_format = pp_desc.get('val_format') or val_format # Plot can override the default\n",
    "    val_range = pp_desc.get('val_range') or val_range\n",
    "\n",
    "    # Compute draws if needed - Nb: also applies if the draws are shared for the group of questions\n",
    "    if 'draw' in cols and pp_desc['res_col'] in data_meta.get('draws_data',{}):\n",
    "        uid, ndraws = data_meta['draws_data'][pp_desc['res_col']]\n",
    "        draws = stable_draws(total_n, ndraws, uid)\n",
    "        draw_df = pl.DataFrame({ 'draw': draws, 'id': np.arange(0, total_n) })\n",
    "        filtered_df = filtered_df.drop('draw').join(draw_df.lazy(), on=['id'], how='left')\n",
    "\n",
    "    # If res_col is a group of questions, melt i.e. unpivot the questions and handle draws if needed\n",
    "    if pp_desc['res_col'] in gc_dict:\n",
    "        n_questions = len(gc_dict[pp_desc['res_col']])\n",
    "\n",
    "        # Melt i.e. unpivot the questions\n",
    "        value_vars = [ c for c in gc_dict[pp_desc['res_col']] if c in cols ]\n",
    "        id_vars = ['id'] + [ c for c in cols if (c not in value_vars or c in factor_cols) ]\n",
    "        filtered_df = filtered_df.unpivot(\n",
    "            variable_name='question',\n",
    "            value_name=pp_desc['res_col'],\n",
    "            index=id_vars,\n",
    "            on=value_vars,\n",
    "        )\n",
    "\n",
    "        # Handle draws for each question\n",
    "        if 'draw' in cols and data_meta.get('draws_data') is not None:\n",
    "            draw_dfs = []\n",
    "            for c in value_vars:\n",
    "                if c in data_meta.get('draws_data',{}):\n",
    "                    uid, ndraws = data_meta['draws_data'][c]\n",
    "                    draws = stable_draws(total_n, ndraws, uid)\n",
    "                    draw_df = pl.DataFrame({ 'draw': draws, 'question': c, 'id': np.arange(0, total_n) })\n",
    "                    draw_dfs.append(draw_df)\n",
    "            \n",
    "            if len(draw_dfs)>0:\n",
    "                filtered_df = filtered_df.rename({'draw':'old_draw'}).join(\n",
    "                    pl.concat(draw_dfs).lazy(),\n",
    "                    on=['id', 'question'],\n",
    "                    how='left'\n",
    "                ).with_columns(pl.col('draw').fill_null(pl.col('old_draw'))).drop('old_draw')\n",
    "            \n",
    "        # Convert question to categorical with correct order\n",
    "        filtered_df = filtered_df.with_columns(pl.col('question').cast(pl.Enum(value_vars)))\n",
    "    else:\n",
    "        n_questions = 1\n",
    "        if 'question' in factor_cols:\n",
    "            filtered_df = filtered_df.with_columns(\n",
    "                pl.lit(pp_desc['res_col']).alias('question').cast(pl.Categorical)\n",
    "            )\n",
    "        \n",
    "    # Aggregate the data into right shape\n",
    "    pparams = wrangle_data(filtered_df, c_meta, factor_cols, weight_col, pp_desc, n_questions)\n",
    "\n",
    "    pparams['val_format'] = val_format\n",
    "    pparams['val_range'] = val_range # Currently not used \n",
    "    \n",
    "    # Remove prefix from question names in plots\n",
    "    if 'col_prefix' in c_meta[pp_desc['res_col']] and 'question' in pparams['data'].columns:\n",
    "        prefix = c_meta[pp_desc['res_col']]['col_prefix']\n",
    "        cmap = { c: c.replace(prefix,'') for c in pparams['data']['question'].dtype.categories }\n",
    "        pparams['data']['question'] = pparams['data']['question'].cat.rename_categories(cmap)\n",
    "\n",
    "    return pparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we need for tests:\n",
    "# * a df with draws computed for each question separately\n",
    "# * a df with draws computed for all questions together\n",
    "# * ordinal likerts we turn into continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Helper function that handles reformating data for create_plot\n",
    "def wrangle_data(raw_df, col_meta, factor_cols, weight_col, pp_desc, n_questions):\n",
    "    \n",
    "    plot_meta = get_plot_meta(pp_desc['plot'])\n",
    "    schema = raw_df.collect_schema() \n",
    "    res_col = pp_desc.get('res_col')\n",
    "    \n",
    "    draws, continuous, data_format = (plot_meta.get(vn, False) for vn in ['draws','continuous','data_format'])\n",
    "\n",
    "    #if pp_desc['res_col'] in factor_cols: factor_cols.remove(pp_desc['res_col']) # Res cannot also be a factor\n",
    "    \n",
    "    # Determine the groupby dimensions\n",
    "    gb_dims = (factor_cols + (['draw'] if draws else []) + \n",
    "                (['id'] if plot_meta.get('data_format') == 'raw' else []))\n",
    "\n",
    "    # If we have no groupby dimensions, add a dummy one so we don't have to handle the empty case\n",
    "    if len(gb_dims)==0:\n",
    "        raw_df = raw_df.with_columns(pl.lit('dummy').alias('dummy_col'))\n",
    "        gb_dims = ['dummy_col']\n",
    "    \n",
    "    # if draws and 'draw' in schema.names() and 'augment_to' in pp_desc: # Should we try to bootstrap the data to always have augment_to points. Note this is relatively slow\n",
    "    #     raw_df = augment_draws(raw_df,gb_dims[1:],threshold=pp_desc['augment_to'])\n",
    "        \n",
    "    pparams = { 'value_col': 'value' }\n",
    "\n",
    "    if data_format=='raw':\n",
    "        pparams['value_col'] = res_col\n",
    "        if plot_meta.get('sample'):\n",
    "            data = (raw_df\n",
    "                    .select(gb_dims + [res_col])\n",
    "                    .groupby(gb_dims)\n",
    "                    .sample(n=plot_meta['sample'], with_replacement=True))\n",
    "        else: \n",
    "            data = raw_df.select(gb_dims + [res_col])\n",
    "        \n",
    "    elif data_format=='longform':\n",
    "        rc_meta = col_meta.get(res_col,{})\n",
    "\n",
    "        agg_fn = pp_desc.get('agg_fn','mean')\n",
    "        agg_fn = plot_meta.get('agg_fn',agg_fn)\n",
    "        \n",
    "        # Check if categorical by looking at schema\n",
    "        is_categorical = isinstance(schema[res_col], (pl.Categorical, pl.Enum, pl.String))\n",
    "\n",
    "        if is_categorical:\n",
    "            pparams['cat_col'] = res_col \n",
    "            pparams['value_col'] = 'percent'\n",
    "            \n",
    "            # Aggregate the data\n",
    "            data = (raw_df\n",
    "                    .group_by(gb_dims + [res_col])\n",
    "                    .agg(pl.col(weight_col).sum().alias('percent')))\n",
    "\n",
    "            # Add weight_col to the data\n",
    "            totals = raw_df.group_by(gb_dims).agg(pl.col(weight_col).sum())\n",
    "            data = data.join(totals, on=gb_dims)\n",
    "                \n",
    "            if agg_fn == 'mean':\n",
    "                data = data.with_columns( pl.col('percent') / pl.col(weight_col) )\n",
    "            elif agg_fn != 'sum':\n",
    "                raise Exception(f\"Unknown agg_fn: {agg_fn}\")\n",
    "\n",
    "        else: # Continuous\n",
    "            \n",
    "            if agg_fn in ['mean','sum']: # Use weighted sum to compute both sum and mean\n",
    "                data = (raw_df\n",
    "                        .with_columns((pl.col(res_col)*pl.col(weight_col)).alias(res_col))\n",
    "                        .group_by(gb_dims)\n",
    "                        .agg(pl.col([res_col,weight_col]).sum()))\n",
    "                if agg_fn == 'mean':\n",
    "                    data = data.with_columns(pl.col(res_col)/pl.col(weight_col).alias(res_col))\n",
    "            else:  # median, min, max, etc. - ignore weight_col\n",
    "                data = (raw_df\n",
    "                        .group_by(gb_dims)\n",
    "                        .agg([getattr(pl.col(res_col), agg_fn)().alias(res_col), pl.col(weight_col).sum()]))                    \n",
    "                \n",
    "            pparams['value_col'] = res_col\n",
    "\n",
    "        if plot_meta.get('group_sizes'): \n",
    "            data = data.rename({weight_col:'group_size'})\n",
    "        else: data = data.drop(weight_col)\n",
    "    else:\n",
    "        raise Exception(\"Unknown data_format\")\n",
    "\n",
    "    # Remove dummy column after aggregation\n",
    "    if gb_dims == ['dummy_col']: data = data.drop('dummy_col')\n",
    "\n",
    "    # For old streaming, the query does not generally seem to stream\n",
    "    # For new_stream, polars 1.23 considers categoricals to still be broken\n",
    "    # TODO: Check back here when 1.24+ is released\n",
    "    #print(\"final\\n\",data.explain(streaming=True))\n",
    "    #data = data.collect(engine='streaming').to_pandas()\n",
    "    data = data.collect(streaming=True).to_pandas()\n",
    "\n",
    "    # Force immediate garbage collection\n",
    "    gc.collect() # Does not help much, but unlikely to hurt either\n",
    "\n",
    "    # How many datapoints the plot is based on. This is useful metainfo to display sometimes\n",
    "    pparams['filtered_size'] = raw_df.select(pl.col(weight_col).sum()).collect().item()/n_questions\n",
    "\n",
    "    # Fix categorical types that polars does not read properly from parquet\n",
    "    # Also filter out unused categories so plots are cleaner\n",
    "    for c in data.columns:\n",
    "        if col_meta.get(c,{}).get('categories'): \n",
    "            m_cats = col_meta[c]['categories'] if col_meta[c].get('categories','infer')!='infer' else sorted(list(data[c].unique()))\n",
    "            if len(set(data[c].dtype.categories)-set(m_cats))>0: m_cats = col.dtype.categories\n",
    "\n",
    "            # Get the categories that are in use\n",
    "            if c != pp_desc['res_col'] or not col_meta[c].get('likert'):\n",
    "                u_cats = [ cv for cv in m_cats if cv in data[c].unique() ]\n",
    "            else: u_cats = m_cats\n",
    "\n",
    "            data[c] = pd.Categorical(data[c],u_cats,ordered=col_meta[c].get('ordered',False))\n",
    "\n",
    "    pparams['col_meta'] = col_meta # As this has been adjusted for discretization etc\n",
    "    pparams['data'] = data\n",
    "\n",
    "    return pparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting part of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Create a color scale\n",
    "def meta_color_scale(scale: Optional[Dict], column=None, translate=None):\n",
    "    cats = column.dtype.categories if column.dtype.name=='category' else None\n",
    "    if scale is None and column is not None and column.dtype.name=='category' and column.dtype.ordered:\n",
    "        scale = dict(zip(cats,gradient_to_discrete_color_scale(default_bidirectional_gradient, len(cats))))\n",
    "    if translate and cats is not None:\n",
    "        remap = dict(zip(cats,[ translate(c) for c in cats ]))\n",
    "        scale = { (remap[k] if k in remap else k) : v for k,v in scale.items() } if scale else scale\n",
    "        cats = [ remap[c] for c in cats ]\n",
    "    return to_alt_scale(scale,cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def translate_df(df, translate):\n",
    "    df.columns = [ (translate(c) if c not in special_columns else c) for c in df.columns ]\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.name == 'category':\n",
    "            cats = df[c].dtype.categories\n",
    "            remap = dict(zip(cats,[ translate(c) for c in cats ]))\n",
    "            df[c] = df[c].cat.rename_categories(remap)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def create_tooltip(pparams,tc_meta):\n",
    "    \n",
    "    data, tfn = pparams['data'], pparams['translate']\n",
    "    \n",
    "    label_dict = {}\n",
    "    \n",
    "    # Determine the columns we need tooltips for:\n",
    "    tcols = [ f['col'] for f in pparams['facets'] if f['col'] in data.columns ]\n",
    "            \n",
    "    # Find labels mappings for regular columns\n",
    "    for cn in tcols:\n",
    "        if cn in tc_meta and tc_meta[cn].get('labels'): label_dict[cn] = tc_meta[cn]['labels']\n",
    "    \n",
    "    # Find a mapping for multi-column questions\n",
    "    question_tn = tfn('question')\n",
    "    if question_tn in data.columns and any([ tc_meta[c].get('label') for c in data[question_tn].unique() if c in tc_meta ]):\n",
    "        label_dict[question_tn] = { c: tc_meta[c].get('label') or '' for c in data[question_tn].unique() if c in tc_meta and tc_meta[c].get('label') }\n",
    "    \n",
    "    # Create the tooltips\n",
    "    tooltips = [ alt.Tooltip(f\"{pparams['value_col']}:Q\", format=pparams['val_format']) ]\n",
    "    for cn in tcols:\n",
    "        if label_dict.get(cn):\n",
    "            data[cn+'_label'] = data[cn].astype('object').replace({ k:tfn(v) for k,v in label_dict[cn].items() })\n",
    "            t = alt.Tooltip(f\"{cn}_label:N\",title=cn)\n",
    "        else:\n",
    "            t = alt.Tooltip(f\"{cn}:N\")\n",
    "        tooltips.append(t)\n",
    "            \n",
    "    return tooltips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Small helper function to move columns from internal to external columns\n",
    "def remove_from_internal_fcols(cname, factor_cols, n_inner):\n",
    "    if cname not in factor_cols[:n_inner]: return n_inner\n",
    "    factor_cols.remove(cname)\n",
    "    if n_inner>len(factor_cols): n_inner-=1\n",
    "    factor_cols.insert(n_inner,cname)\n",
    "    return n_inner\n",
    "\n",
    "def inner_outer_factors(factor_cols, pp_desc, plot_meta):\n",
    "    # Determine how many factors to use as inner facets\n",
    "    in_f = pp_desc.get('internal_facet',False)\n",
    "    n_min_f, n_rec_f = plot_meta.get('n_facets',(0,0))\n",
    "    n_inner =  (n_rec_f if in_f else n_min_f) if isinstance(in_f,bool) else in_f\n",
    "    if n_inner>len(factor_cols): n_inner = len(factor_cols)\n",
    "\n",
    "    # If question facet as inner facet for a no_question_facet plot, just move it out\n",
    "    if plot_meta.get('no_question_facet'):\n",
    "        n_inner = remove_from_internal_fcols('question',factor_cols,n_inner)\n",
    "        n_inner = remove_from_internal_fcols(pp_desc['res_col'],factor_cols,n_inner)\n",
    "    \n",
    "    return factor_cols, n_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Function that takes filtered raw data and plot information and outputs the plot\n",
    "# Handles all of the data wrangling and parameter formatting\n",
    "def create_plot(pparams, data_meta, pp_desc, alt_properties={}, alt_wrapper=None, dry_run=False, width=200, height=None, return_matrix_of_plots=False, translate=None):\n",
    "    data, col_meta = pparams['data'], pparams['col_meta']\n",
    "    data_meta = update_data_meta_with_pp_desc(data_meta, pp_desc)\n",
    "    pparams = {**pparams} # Make a shallow copy so we don't mess with the original object\n",
    "\n",
    "    plot_meta = get_plot_meta(pp_desc['plot'])\n",
    "    \n",
    "    if 'question' in data.columns: # TODO: this should be in io.py already, probably\n",
    "        col_meta['question']['colors'] = col_meta[pp_desc['res_col']].get('question_colors',None)\n",
    "\n",
    "    plot_args = pp_desc.get('plot_args',{})\n",
    "    pparams.update(plot_args)\n",
    "\n",
    "    # Get list of factor columns (adding question and category if needed)\n",
    "    factor_cols, n_inner = inner_outer_factors(pp_desc['factor_cols'], pp_desc, plot_meta)\n",
    "\n",
    "    # Reorder categories if required\n",
    "    if pp_desc.get('sort'):\n",
    "        for cn in pp_desc['sort']:\n",
    "            ascending = pp_desc['sort'][cn] if isinstance(pp_desc['sort'],dict) else False\n",
    "            if cn not in data.columns or cn==pparams['value_col']: \n",
    "                raise Exception(f\"Sort column {cn} not found\")\n",
    "            if plot_meta.get('sort_numeric_first_facet'): # Some plots (like likert_bars) need a more complex sort\n",
    "                f0 = factor_cols[0]\n",
    "                nvals = get_cat_num_vals(col_meta[f0],pp_desc)\n",
    "                cats = col_meta[f0]['categories']\n",
    "                cmap = dict(zip(cats,nvals))\n",
    "                sdf = data[ [cn,f0,pparams['value_col']] ]\n",
    "                sdf['sort_val'] = sdf[pparams['value_col']]*sdf[f0].astype('object').replace(cmap)\n",
    "                ordervals = sdf.groupby(cn,observed=True)['sort_val'].mean()\n",
    "            else:\n",
    "                ordervals = data.groupby(cn,observed=True)[pparams['value_col']].mean()\n",
    "            order = ordervals.sort_values(ascending=ascending).index\n",
    "            data[cn] = pd.Categorical(data[cn],list(order))\n",
    "    \n",
    "    # Handle translation funcion\n",
    "    if translate is None: translate = (lambda s: s)\n",
    "    pparams['translate'] = translate\n",
    "\n",
    "    # Handle internal facets (and translate as needed)\n",
    "    pparams['facets'] = []\n",
    "\n",
    "    if n_inner>0:\n",
    "        for cn in factor_cols[:n_inner]:\n",
    "            fd = {\n",
    "                'col': translate(cn),\n",
    "                'ocol': cn,\n",
    "                'order': [ translate(c) for c in data[cn].dtype.categories ],\n",
    "                'colors': meta_color_scale(col_meta[cn].get('colors',None), data[cn], translate=translate), \n",
    "            }\n",
    "            pparams['facets'].append(fd)\n",
    "\n",
    "        # Pass on data from facet column meta if specified by plot\n",
    "        for i,d in enumerate(plot_meta.get('requires',[])):\n",
    "            for k, v in d.items():\n",
    "                if v=='pass': pparams[k] = col_meta[pparams['facets'][i]['ocol']].get(k)\n",
    "        \n",
    "        factor_cols = factor_cols[n_inner:] # Leave rest for external faceting\n",
    "\n",
    "    if plot_meta.get('no_faceting') and len(factor_cols)>0: return_matrix_of_plots = True\n",
    "\n",
    "    pparams['value_range'] = tuple(data[pparams['value_col']].agg(['min','max']))\n",
    "\n",
    "    pparams['outer_colors'] = col_meta[factor_cols[0]].get('colors', {}) if factor_cols else {}\n",
    "\n",
    "    # Rename res_col if label provided (or remove prefix if present)\n",
    "    if col_meta[pparams['value_col']].get('label') or col_meta[pparams['value_col']].get('col_prefix'):\n",
    "        label = col_meta[pparams['value_col']].get('label')\n",
    "        if not label:\n",
    "            prefix = col_meta[pparams['value_col']]['col_prefix']\n",
    "            label = pparams['value_col']\n",
    "            if label.startswith(prefix) and label!=prefix:\n",
    "                label = pparams['value_col'][len(prefix):]\n",
    "        data = data.rename(columns={pparams['value_col']: label})\n",
    "        pparams['value_col'] = label\n",
    "\n",
    "    # Translate the data itself\n",
    "    pparams['data'] = data = translate_df(data,translate)\n",
    "    pparams['value_col'] = translate(pparams['value_col'])  \n",
    "    factor_cols = [ translate(c) for c in factor_cols ]\n",
    "    t_col_meta = { translate(c): v for c,v in col_meta.items() }\n",
    "\n",
    "    # Handle tooltip\n",
    "    pparams['tooltip'] = create_tooltip(pparams,t_col_meta)\n",
    "    \n",
    "    # If we still have more than 1 factor left, merge the rest into one so we have a 2d facet\n",
    "    if len(factor_cols)>1:\n",
    "        n_facet_cols = len(data[factor_cols[-1]].dtype.categories)\n",
    "        if not return_matrix_of_plots and len(factor_cols)>2:\n",
    "\n",
    "            # Preserve ordering of categories we combine\n",
    "            nf_order = [ ', '.join(t) for t in it.product(*[list(data[c].dtype.categories) for c in factor_cols[1:]])]\n",
    "            factor_col = ', '.join(factor_cols[1:])\n",
    "            jfs = data[factor_cols[1:]].agg(', '.join, axis=1)\n",
    "            data.loc[:,factor_col] = pd.Categorical(jfs,nf_order)\n",
    "            pparams['data'] = data\n",
    "            factor_cols = [factor_cols[0], factor_col]\n",
    "\n",
    "        if len(factor_cols)>=2:\n",
    "            factor_cols = list(reversed(factor_cols))\n",
    "            n_facet_cols = len(data[factor_cols[1]].dtype.categories)\n",
    "    else:\n",
    "        n_facet_cols = plot_meta.get('factor_columns',1)\n",
    "        \n",
    "    # Allow value col name to be changed. This can be useful in distinguishing different aggregation options for a column\n",
    "    if 'value_name' in pp_desc: \n",
    "        pparams['data'] = pparams['data'].rename(columns={pparams['value_col']:pp_desc['value_name']})\n",
    "        pparams['value_col'] = pp_desc['value_name']\n",
    "    \n",
    "    # Do width/height calculations\n",
    "    if factor_cols: n_facet_cols = pp_desc.get('n_facet_cols',n_facet_cols) # Allow pp_desc to override col nr\n",
    "    dims = {'width': width//n_facet_cols if factor_cols else width}\n",
    "\n",
    "    if height!=None: dims['height'] = int(height)\n",
    "    elif 'aspect_ratio' in plot_meta:   dims['height'] = int(dims['width']/plot_meta['aspect_ratio'])\n",
    "    \n",
    "    # Make plot properties available to plot function (mostly useful for as_is plots)\n",
    "    pparams.update({'width':width}); pparams['alt_properties'] = alt_properties; pparams['outer_factors'] = factor_cols\n",
    "\n",
    "    # Create the plot using it's function\n",
    "    if dry_run: return pparams\n",
    "    \n",
    "    # Trim down parameters list if needed\n",
    "    plot_fn = get_plot_fn(pp_desc['plot'])\n",
    "    pparams = clean_kwargs(plot_fn,pparams)\n",
    "    if alt_wrapper is None: alt_wrapper = lambda p: p\n",
    "    if plot_meta.get('as_is'): # if as_is set, just return the plot as-is\n",
    "        return plot_fn(**pparams)\n",
    "    elif factor_cols:\n",
    "        if return_matrix_of_plots: # return a 2d list of plots which can be rendeed one plot at a time\n",
    "            del pparams['data']\n",
    "            combs = it.product( *[data[fc].dtype.categories for fc in factor_cols ])\n",
    "            return list(batch([\n",
    "                alt_wrapper(plot_fn(data[(data[factor_cols]==c).all(axis=1)],**pparams)\n",
    "                            .properties(title='-'.join(map(str,c)),**dims, **alt_properties)\n",
    "                            .configure_view(discreteHeight={'step':20}))\n",
    "                for c in combs\n",
    "                ], n_facet_cols))\n",
    "        else: # Use faceting\n",
    "            if n_facet_cols==1:\n",
    "                plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties).facet(\n",
    "                    row=alt.Row(f'{factor_cols[0]}:O', sort=list(data[factor_cols[0]].dtype.categories), header=alt.Header(labelOrient='top'))))\n",
    "            elif len(factor_cols)>1:\n",
    "                plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties).facet(\n",
    "                    column=alt.Column(f'{factor_cols[1]}:O', sort=list(data[factor_cols[1]].dtype.categories)),\n",
    "                    row=alt.Row(f'{factor_cols[0]}:O', sort=list(data[factor_cols[0]].dtype.categories), header=alt.Header(labelOrient='top'))))\n",
    "            else: # n_facet_cols!=1 but just one facet\n",
    "                plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties).facet(f'{factor_cols[0]}:O',columns=n_facet_cols))\n",
    "            plot = plot.configure_view(discreteHeight={'step':20})\n",
    "    else:\n",
    "        plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties)\n",
    "                            .configure_view(discreteHeight={'step':20}))\n",
    "\n",
    "        if return_matrix_of_plots: plot = [[plot]]\n",
    "\n",
    "    return plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_desc = {\n",
    "    'res_col' : 'thermometer',\n",
    "    'factor_cols': ['question','party_preference'],\n",
    "    'filter': { 'nationality': 'Estonian' },\n",
    "    'plot': 'matrix',\n",
    "    'internal_facet': True\n",
    "}\n",
    "from salk_toolkit.dashboard import default_translate\n",
    "\n",
    "fdf = pp_transform_data(full_df, data_meta, pp_desc)\n",
    "#wdf = wrangle_data(fdf, **args, **get_plot_meta(args['plot']))\n",
    "#fdf['data'].sample(5)\n",
    "create_plot(fdf,data_meta,pp_desc,width=800,translate=default_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Compute the full factor_cols list, including question and res_col as needed\n",
    "def impute_factor_cols(pp_desc, col_meta, plot_meta=None):\n",
    "    factor_cols = pp_desc.get('factor_cols',[]).copy()\n",
    "\n",
    "    # Determine if res is categorical\n",
    "    cat_res = 'categories' in col_meta[pp_desc['res_col']] and pp_desc.get('convert_res')!='continuous' \n",
    "\n",
    "    # Add res_col if we are working with a categorical input (and not converting it to continuous)\n",
    "    if cat_res and pp_desc['res_col'] not in factor_cols: \n",
    "        factor_cols.insert(0,pp_desc['res_col'])\n",
    "\n",
    "    # Determine if we have 'question' as a column\n",
    "    has_q = 'columns' in col_meta[pp_desc['res_col']] # Check if res_col is a group of questions\n",
    "    if len(factor_cols)<1 and not has_q: has_q = True # Create 'question' as a dummy dimension so we have at least one factor (generally required for plotting)\n",
    "    \n",
    "    # If we need to, add question as a factor to list\n",
    "    if has_q and 'question' not in factor_cols:\n",
    "        if cat_res: factor_cols.append('question') # Put it last for categorical values\n",
    "        else: factor_cols.insert(0,'question') # And first for continuous values, as it then often represents the \"category\"\n",
    "\n",
    "    # Pass the factor_cols through the same changes done inside plot pipeline to make more explicit what happens\n",
    "    if plot_meta: factor_cols, _ = inner_outer_factors(factor_cols, pp_desc, plot_meta)\n",
    "\n",
    "    return factor_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# A convenience function to draw a plot straight from a dataset\n",
    "def e2e_plot(pp_desc, data_file=None, full_df=None, data_meta=None, width=800, height=None, check_match=True, impute=True, plot_cache=None, **kwargs):\n",
    "    if data_file is None and full_df is None:\n",
    "        raise Exception('Data must be provided either as data_file or full_df')\n",
    "    if data_file is None and data_meta is None:\n",
    "        raise Exception('If data provided as full_df then data_meta must also be given')\n",
    "        \n",
    "    if full_df is None: \n",
    "        full_df, dm = read_annotated_data_lazy(data_file)\n",
    "        if data_meta is None: data_meta = dm\n",
    "\n",
    "    pp_desc = pp_desc.copy()\n",
    "    if impute: pp_desc['factor_cols'] = impute_factor_cols(pp_desc, extract_column_meta(data_meta), get_plot_meta(pp_desc['plot']))\n",
    "\n",
    "    if check_match:\n",
    "        matches = matching_plots(pp_desc, full_df, data_meta, details=True, list_hidden=True)    \n",
    "        if pp_desc['plot'] not in matches: \n",
    "            raise Exception(f\"Plot not registered: {pp_desc['plot']}\")\n",
    "        \n",
    "        fit, imp = matches[pp_desc['plot']]\n",
    "        if  fit<0:\n",
    "            raise Exception(f\"Plot {pp_desc['plot']} not applicable in this situation because of flags {imp}\")\n",
    "\n",
    "    if plot_cache is not None:\n",
    "        key = json.dumps(pp_desc, sort_keys=True)\n",
    "        if key in plot_cache:\n",
    "            pparams = plot_cache[key]\n",
    "        else:\n",
    "            pparams = pp_transform_data(full_df, data_meta, pp_desc)\n",
    "            plot_cache[key] = pparams\n",
    "    else: # No caching\n",
    "        pparams = pp_transform_data(full_df, data_meta, pp_desc)\n",
    "    return create_plot(pparams, data_meta, pp_desc, width=width,height=height,**kwargs)\n",
    "\n",
    "# Another convenience function to simplify testing new plots\n",
    "def test_new_plot(fn, pp_desc, *args, plot_meta={}, **kwargs):\n",
    "    stk_plot(**{**plot_meta,'plot_name':'test'})(fn) # Register the plot under name 'test'\n",
    "    pp_desc = {**pp_desc, 'plot': 'test'}\n",
    "    res = e2e_plot(pp_desc,*args,**kwargs)\n",
    "    stk_deregister('test') # And de-register it again\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../samples/master_bootstrap.parquet'\n",
    "data_metafile = '../../salk_internal_package/data/master_meta.json'\n",
    "if data_metafile:\n",
    "    data_meta = read_json(data_metafile)\n",
    "\n",
    "td = { 'unit': 'ksus', 'Keskerakond':'Kekre', 'education': 'Haridus', 'Basic education':'Phiharidus', 'party_preference': 'Party preference', 'age_group':'agg' }\n",
    "\n",
    "#def translate(s):\n",
    "#    return (td[s] if s in td else s)\n",
    "\n",
    "def translate(s):\n",
    "    return (s[0].upper() + s[1:]).replace('_',' ') if isinstance(s,str) else s\n",
    "\n",
    "e2e_plot({\n",
    "    'res_col' : 'age_group',\n",
    "    'factor_cols': ['party_preference'],\n",
    "    'filter': {},\n",
    "    'plot': 'boxplots',\n",
    "    'sort': ['party_preference'],\n",
    "    'internal_facet': True\n",
    "}, data_file, data_meta=data_meta,width=800, translate=translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test e2e_plot\n",
    "#alt.data_transformers.disable_max_rows()\n",
    "pp_desc = {\n",
    "    'res_col' : 'income',\n",
    "    #'factor_cols': ['gender'],\n",
    "    'filter': { 'nationality': 'Estonian' },\n",
    "    'plot': 'boxplots',\n",
    "    'internal_facet': True\n",
    "}\n",
    "e2e_plot(pp_desc,data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Test test_new_plot\n",
    "def smooth(data, cat_col, value_col='value', color_scale=alt.Undefined, factor_col=None):\n",
    "    options_cols = list(data[cat_col].dtype.categories)\n",
    "    ldict = dict(zip(options_cols, range(len(options_cols))))\n",
    "    data.loc[:,'order'] = data[cat_col].astype('object').replace(ldict)\n",
    "    plot=alt.Chart(data\n",
    "        ).mark_area(interpolate='natural').encode(\n",
    "            x=alt.X(f'{factor_col}:O', title=None),\n",
    "            y=alt.Y(f'{value_col}:Q', title=None, stack='normalize',\n",
    "                 scale=alt.Scale(domain=[0, 1]), axis=alt.Axis(format='%')\n",
    "                 ),\n",
    "            order=\"order:O\",\n",
    "            color=alt.Color(cat_col, legend=alt.Legend(orient='top', title=None),\n",
    "                sort=alt.SortField(\"order\", \"descending\"), scale=color_scale\n",
    "                )\n",
    "        )\n",
    "    return plot\n",
    "\n",
    "test_new_plot(smooth, {\n",
    "    'res_col' : 'party_preference',\n",
    "    'factor_cols': ['age_group','gender'],  'filter': {},\n",
    "    'plot': 'area_smooth',\n",
    "    'internal_facet': True\n",
    "}, full_df=full_df, data_meta=data_meta, plot_meta={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../samples/w25_bootstrap.parquet'\n",
    "data_metafile = '../../salk_internal_package/data/master_meta.json'\n",
    "if data_metafile:\n",
    "    data_meta = read_json(data_metafile)\n",
    "    \n",
    "e2e_plot({\n",
    "    'res_col' : 'party_preference',\n",
    "    'factor_cols': ['unit'],\n",
    "    'filter': {},\n",
    "    'plot': 'geoplot',\n",
    "    'internal_facet': True\n",
    "}, data_file, data_meta=data_meta,width=800, return_matrix_of_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
