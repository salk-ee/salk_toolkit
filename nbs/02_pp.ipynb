{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Pipeline\n",
    "> Pipeline from raw survey data file up to creating the plot\n",
    "> Built around the use of plot registry from plots.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import scipy.stats as sps\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "from salk_toolkit.plots import stk_plot, stk_deregister, matching_plots, get_plot_fn, get_plot_meta\n",
    "from salk_toolkit.utils import *\n",
    "from salk_toolkit.io import load_parquet_with_metadata, extract_column_meta, group_columns_dict, list_aliases, read_annotated_data, read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple args value for testing individual functions\n",
    "args = {\n",
    "    'res_col' : 'age_group',\n",
    "    'factor_cols': ['EKRE'],\n",
    "    'filter': { 'nationality': 'Estonian', 'EKRE': (-3,2), 'age_group': ('35-44', '75+'), 'party_preference': ['SDE','EKRE','Reformierakond'] },\n",
    "    'plot': 'boxplots',\n",
    "    'internal_facet': False\n",
    "}\n",
    "\n",
    "data_uri = '../../salk_internal_package/samples/bootstrap.parquet'\n",
    "\n",
    "# Load a basic bootstrapped dataset\n",
    "full_df, f_meta = load_parquet_with_metadata(data_uri)\n",
    "data_meta = f_meta['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metafile directly - allows faster iteration\n",
    "data_metafile = '../data/master_meta.json'\n",
    "if data_metafile:\n",
    "    from salk_toolkit.utils import replace_constants\n",
    "    data_meta = read_json(data_metafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Augment each draw with bootstrap data from across whole population to make sure there are at least <threshold> samples\n",
    "def augment_draws(data, factors=None, n_draws=None, threshold=50):\n",
    "    if n_draws == None: n_draws = data.draw.max()+1\n",
    "    \n",
    "    if factors: # Run recursively on each factor separately and concatenate results\n",
    "        if data[ ['draw']+factors ].value_counts().min() >= threshold: return data # This takes care of large datasets fast\n",
    "        return data.groupby(factors,observed=False).apply(augment_draws,n_draws=n_draws,threshold=threshold).reset_index(drop=True) # Slow-ish, but only needed on small data now\n",
    "    \n",
    "    # Get count of values for each draw\n",
    "    draw_counts = data['draw'].value_counts() # Get value counts of existing draws\n",
    "    if len(draw_counts)<n_draws: # Fill in completely missing draws\n",
    "        draw_counts = (draw_counts + pd.Series(0,index=range(n_draws))).fillna(0).astype(int)\n",
    "        \n",
    "    # If no new draws needed, just return original\n",
    "    if draw_counts.min()>=threshold: return data\n",
    "    \n",
    "    # Generate an index for new draws\n",
    "    new_draws = [ d for d,c in draw_counts[draw_counts<threshold].items() for _ in range(threshold-c) ]\n",
    "\n",
    "    # Generate new draws\n",
    "    new_rows = data.iloc[np.random.choice(len(data),len(new_draws)),:].copy()\n",
    "    new_rows['draw'] = new_draws\n",
    "    \n",
    "    return pd.concat([data, new_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Get the categories that are in use\n",
    "def get_cats(col, cats=None):\n",
    "    if cats is None or len(set(col.dtype.categories)-set(cats))>0: cats = col.dtype.categories\n",
    "    return [ c for c in cats if c in col.unique() ]\n",
    "\n",
    "def transform_cont(data, transform):\n",
    "    if not transform: return data\n",
    "    elif transform == 'center': return data - data.mean(skipna=True)\n",
    "    elif transform == 'zscore': return sps.zscore(data,nan_policy='omit')\n",
    "    else: raise Exception(f\"Unknown transform '{transform}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Get all data required for a given graph\n",
    "# Only return columns and rows that are needed\n",
    "# This can handle either a pandas DataFrame or a polars LazyDataFrame (to allow for loading only needed data)\n",
    "def get_filtered_data(full_df, data_meta, pp_desc, columns=[]):\n",
    "    \n",
    "    # Figure out which columns we actually need\n",
    "    meta_cols = ['weight', 'training_subsample', '__index_level_0__'] + (['draw'] if vod(get_plot_meta(pp_desc['plot']),'draws') else []) + columns\n",
    "    cols = [ pp_desc['res_col'] ]  + vod(pp_desc,'factor_cols',[]) + list(vod(pp_desc,'filter',{}).keys())\n",
    "    cols += [ c for c in meta_cols if c in full_df.columns and c not in cols ]\n",
    "    \n",
    "    # If any aliases are used, cconvert them to column names according to the data_meta\n",
    "    gc_dict = group_columns_dict(data_meta)\n",
    "    c_meta = extract_column_meta(data_meta)\n",
    "    \n",
    "    # Dict to remap (short) category names to longer descriptions in tooltips\n",
    "    label_dict = {}\n",
    "    \n",
    "    cols = [ c for c in np.unique(list_aliases(cols,gc_dict)) if c in full_df.columns ]\n",
    "    \n",
    "    #print(\"C\",cols)\n",
    "    \n",
    "    lazy = isinstance(full_df,pl.LazyFrame)\n",
    "    if lazy: pl.enable_string_cache() # Needed for categories to be comparable to strings\n",
    "    \n",
    "    df = full_df.select(cols) if lazy else full_df[cols]\n",
    "    \n",
    "    # Filter using demographics dict. This is very clever but hard to read. See:\n",
    "    filter_dict = vod(pp_desc,'filter',{})\n",
    "    inds = True if lazy else np.full(len(df),True) \n",
    "    for k, v in filter_dict.items():\n",
    "        \n",
    "        # Handle continuous variables separately\n",
    "        if isinstance(v,tuple) and (vod(c_meta[k],'continuous') or vod(c_meta[k],'datetime')): # Only special case where we actually need a range\n",
    "            if lazy: inds = (((pl.col(k)>=v[0]) & (pl.col(k)<=v[1])) | pl.col(k).is_null()) & inds\n",
    "            else: inds = (((df[k]>=v[0]) & (df[k]<=v[1])) | df[k].isna()) & inds\n",
    "            continue # NB! this approach does not work for ordered categoricals with polars LazyDataFrame, hence handling that separately below\n",
    "        \n",
    "        # Filter by list of values:\n",
    "        if isinstance(v,tuple):\n",
    "            if vod(c_meta[k],'categories','infer')=='infer': raise Exception(f'Ordering unknown for column {k}')\n",
    "            cats = list(c_meta[k]['categories'])\n",
    "            if set(v) & set(cats) != set(v): raise Exception(f'Column {k} values {v} not found in {cats}')\n",
    "            bi, ei = cats.index(v[0]), cats.index(v[1])\n",
    "            flst = cats[bi:ei+1] # \n",
    "        elif isinstance(v,list): flst = v # List indicates a set of values\n",
    "        elif 'groups' in c_meta[k] and v in c_meta[k]['groups']:\n",
    "            flst = c_meta[k]['groups'][v]\n",
    "        else: flst = [v] # Just filter on single value    \n",
    "            \n",
    "        inds =  (pl.col(k).is_in(flst) if lazy else df[k].isin(flst)) & inds\n",
    "            \n",
    "    filtered_df = df.filter(inds).collect().to_pandas() if lazy else df[inds].copy()\n",
    "    if lazy and '__index_level_0__' in filtered_df.columns: # Fix index, if provided. This is a hack but seems to be needed as polars does not handle index properly by default\n",
    "        filtered_df.index = filtered_df['__index_level_0__'] \n",
    "    \n",
    "    # Replace draw with the draws used in modelling - NB! does not currenlty work for group questions\n",
    "    if 'draw' in filtered_df.columns and pp_desc['res_col'] in vod(data_meta,'draws_data',{}):\n",
    "        uid, ndraws = data_meta['draws_data'][pp_desc['res_col']]\n",
    "        filtered_df = deterministic_draws(filtered_df, ndraws, uid, n_total = data_meta['total_size'] )\n",
    "    \n",
    "    # If not poststratisfied\n",
    "    if not vod(pp_desc,'poststrat',True):\n",
    "        filtered_df = filtered_df.assign(weight = 1.0) # Remove weighting\n",
    "        if 'training_subsample' in filtered_df.columns:\n",
    "            filtered_df = filtered_df[filtered_df['training_subsample']]\n",
    "    \n",
    "    n_datapoints = len(filtered_df)\n",
    "\n",
    "    # Convert ordered categorical to continuous if we can\n",
    "    res_meta = c_meta[pp_desc['res_col']]\n",
    "    if vod(pp_desc,'convert_res') == 'continuous' and vod(res_meta,'ordered') and vod(res_meta,'categories','infer') != 'infer':\n",
    "        nvals = vod(res_meta,'num_values',range(len(res_meta['categories'])))\n",
    "        if 'num_values' in pp_desc: nvals = pp_desc['num_values'] # Allow manually specifying them for exploring all sorts of interesting aggregation options\n",
    "        cmap = dict(zip(res_meta['categories'],nvals))\n",
    "        rc = gc_dict[pp_desc['res_col']] if pp_desc['res_col'] in gc_dict else [pp_desc['res_col']]\n",
    "        for col in rc:\n",
    "            filtered_df[col] = pd.to_numeric(filtered_df[col].astype('object').replace(cmap))\n",
    "    \n",
    "    # If res_col is a group of questions\n",
    "    # This might move to wrangle but currently easier to do here as we have gc_dict handy\n",
    "    if pp_desc['res_col'] in gc_dict:\n",
    "        value_vars = [ c for c in gc_dict[pp_desc['res_col']] if c in cols ]\n",
    "        \n",
    "        if filtered_df[value_vars[0]].dtype.name != 'category':\n",
    "            #filtered_df.loc[:,value_vars] = filtered_df.loc[:,value_vars].apply(transform_cont,axis=0,transform=vod(pp_desc,'cont_transform'))\n",
    "            for cn in value_vars:\n",
    "                filtered_df[cn] = transform_cont(filtered_df[cn],transform=vod(pp_desc,'cont_transform'))\n",
    "        \n",
    "        id_vars = [ c for c in cols if c not in value_vars ]\n",
    "        filtered_df = filtered_df.melt(id_vars=id_vars, value_vars=value_vars, var_name='question', value_name=pp_desc['res_col'])\n",
    "                \n",
    "        # Convert to proper category with correct order\n",
    "        filtered_df['question'] = pd.Categorical(filtered_df['question'],gc_dict[pp_desc['res_col']])\n",
    "        \n",
    "    elif filtered_df[pp_desc['res_col']].dtype.name != 'category':\n",
    "        filtered_df[pp_desc['res_col']] = transform_cont(filtered_df[pp_desc['res_col']],transform=vod(pp_desc,'cont_transform'))\n",
    "        \n",
    "    # Filter out the unused categories so plots are cleaner\n",
    "    for k in filtered_df.columns:\n",
    "        if filtered_df[k].dtype.name == 'category':\n",
    "            m_cats = c_meta[k]['categories'] if vod(c_meta[k],'categories','infer')!='infer' else None\n",
    "            f_cats = get_cats(filtered_df[k],m_cats) if k != pp_desc['res_col'] or not vod(c_meta[k],'likert') else m_cats # Do not trim likert as plots need to be symmetric\n",
    "            \n",
    "            #vals = filtered_df[k]\n",
    "            filtered_df[k] = pd.Categorical(filtered_df[k],f_cats,ordered=vod(c_meta[k],'ordered',False))\n",
    "    \n",
    "    # Aggregate the data into right shape\n",
    "    pparams = wrangle_data(filtered_df, data_meta, pp_desc)\n",
    "    \n",
    "    # How many datapoints the plot is based on. This is useful metainfo to display sometimes\n",
    "    pparams['n_datapoints'] = n_datapoints\n",
    "    \n",
    "    if lazy: pl.disable_string_cache()\n",
    "    \n",
    "    return pparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "\n",
    "def discretize_continuous(col, col_meta={}):\n",
    "    # NB! qcut might be a better default - see where testing leads us\n",
    "    cut = pd.cut(col, bins = vod(col_meta,'bins',5), labels = vod(col_meta,'bin_labels',None) )\n",
    "    cut = pd.Categorical(cut.astype(str), map(str,cut.dtype.categories), True) # Convert from intervals to strings for it to play nice with altair\n",
    "    return cut\n",
    "\n",
    "# Helper function that handles reformating data for create_plot\n",
    "def wrangle_data(raw_df, data_meta, pp_desc):\n",
    "    \n",
    "    plot_meta = get_plot_meta(pp_desc['plot'])\n",
    "    col_meta = extract_column_meta(data_meta)\n",
    "    \n",
    "    res_col, factor_cols = vod(pp_desc,'res_col'), vod(pp_desc,'factor_cols')\n",
    "    \n",
    "    draws, continuous, data_format = (vod(plot_meta, n, False) for n in ['draws','continuous','data_format'])\n",
    "    \n",
    "    gb_dims = (['draw'] if draws else []) + (factor_cols if factor_cols else []) + (['question'] if 'question' in raw_df.columns else [])\n",
    "    \n",
    "    if 'weight' not in raw_df.columns: raw_df = raw_df.assign(weight=1.0) # This also works for empty df-s\n",
    "    else: raw_df.loc[:,'weight'] = raw_df['weight'].fillna(1.0)\n",
    "\n",
    "    if draws and 'draw' in raw_df.columns and 'augment_to' in pp_desc: # Should we try to bootstrap the data to always have augment_to points. Note this is relatively slow\n",
    "        raw_df = augment_draws(raw_df,gb_dims[1:],threshold=pp_desc['augment_to'])\n",
    "        \n",
    "    pparams = { 'value_col': 'value' }\n",
    "    data = None\n",
    "    \n",
    "    if data_format=='raw':\n",
    "        pparams['value_col'] = res_col\n",
    "        if vod(plot_meta,'sample'):\n",
    "            data = gb_in(raw_df[gb_dims+[res_col]],gb_dims).sample(plot_meta['sample'],replace=True)\n",
    "        else: data = raw_df[gb_dims+[res_col]]\n",
    "\n",
    "    elif False and data_format=='table': # TODO: Untested. Fix when first needed\n",
    "        ddf = pd.get_dummies(raw_df[res_col])\n",
    "        res_cols = list(ddf.columns)\n",
    "        ddf.loc[:,gb_dims] = raw_df[gb_dims]\n",
    "        data = gb_in(ddf,gb_dims)[res_cols].mean().reset_index()\n",
    "        \n",
    "    elif data_format=='longform':\n",
    "        rc_meta = vod(col_meta,res_col,{})\n",
    "        if raw_df[res_col].dtype == 'category':  #'categories' in rc_meta: # categorical\n",
    "            pparams['cat_col'] = res_col \n",
    "            pparams['value_col'] = 'percent'\n",
    "            \n",
    "            # Aggregate the data\n",
    "            data = raw_df.groupby(gb_dims+[res_col],observed=False)['weight'].sum()\n",
    "            if vod(plot_meta,'agg_fn')!='sum': data /= gb_in(raw_df,gb_dims)['weight'].sum()\n",
    "            data = data.rename(pparams['value_col']).dropna().reset_index()\n",
    "            \n",
    "        else: # Continuous\n",
    "            agg_fn = vod(pp_desc,'agg_fn','mean') # We may want to try median vs mean or plot sd-s or whatever\n",
    "            agg_fn = vod(plot_meta,'agg_fn',agg_fn) # Some plots mandate this value (election model for instance)\n",
    "            data = getattr(gb_in(raw_df,gb_dims)[res_col],agg_fn)().dropna().reset_index() \n",
    "            pparams['value_col'] = res_col\n",
    "            \n",
    "        if vod(plot_meta,'group_sizes'):\n",
    "            data = data.merge(gb_in(raw_df,gb_dims).size().rename('group_size').reset_index(),on=gb_dims,how='left')\n",
    "    else:\n",
    "        raise Exception(\"Unknown data_format\")\n",
    "        \n",
    "    # Ensure all rv columns other than value are categorical\n",
    "    for c in data.columns:\n",
    "        if c in ['group_size']: continue # bypass some columns added above\n",
    "        if data[c].dtype.name != 'category' and c!=pparams['value_col']:\n",
    "            if vod(vod(col_meta,c,{}),'continuous'):\n",
    "                data[c] = discretize_continuous(data[c],vod(col_meta,c,{}))\n",
    "            else: # Just assume it's categorical by any other name\n",
    "                data[c] = pd.Categorical(data[c])\n",
    "            \n",
    "    pparams['data'] = data\n",
    "    return pparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pparams = get_filtered_data(full_df, data_meta, args)\n",
    "fdf = pparams['data']\n",
    "fdf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Create a color scale\n",
    "ordered_gradient = [\"#c30d24\", \"#f3a583\", \"#94c6da\", \"#1770ab\"]\n",
    "def meta_color_scale(cmeta,argname='colors',column=None, translate=None):\n",
    "    scale = vod(cmeta,argname)\n",
    "    cats = column.dtype.categories if column.dtype.name=='category' else None\n",
    "    if scale is None and column is not None and column.dtype.name=='category' and column.dtype.ordered:\n",
    "        scale = dict(zip(cats,gradient_to_discrete_color_scale(ordered_gradient, len(cats))))\n",
    "    if translate and cats is not None:\n",
    "        remap = dict(zip(cats,[ translate(c) for c in cats ]))\n",
    "        scale = { (remap[k] if k in remap else k) : v for k,v in scale.items() } if scale else scale\n",
    "        cats = [ remap[c] for c in cats ]\n",
    "    return to_alt_scale(scale,cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def translate_df(df, translate):\n",
    "    df.columns = [ translate(c) for c in df.columns ]\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.name == 'category':\n",
    "            cats = df[c].dtype.categories\n",
    "            remap = dict(zip(cats,[ translate(c) for c in cats ]))\n",
    "            df[c] = df[c].cat.rename_categories(remap)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def create_tooltip(pparams,c_meta):\n",
    "    \n",
    "    data, tfn = pparams['data'], pparams['translate']\n",
    "    \n",
    "    label_dict = {}\n",
    "    \n",
    "    # Determine the columns we need tooltips for:\n",
    "    ttcols = ['question_col', 'cat_col', 'factor_col']\n",
    "    tcols = [ pparams[ct] for ct in ttcols if ct in pparams and pparams[ct] is not None and pparams[ct] in data.columns ] \n",
    "            \n",
    "    # Find labels mappings for regular columns\n",
    "    for cn in tcols:\n",
    "        if cn in c_meta and 'labels' in c_meta[cn]: label_dict[cn] = c_meta[cn]['labels']\n",
    "    \n",
    "    # Find a mapping for multi-column questions\n",
    "    if 'question' in data.columns and any([ 'label' in c_meta[c] for c in data['question'].unique() if c in c_meta ]):\n",
    "        label_dict['question'] = { c: vod(c_meta[c],'label','') for c in data['question'].unique() if c in c_meta and 'label' in c_meta[c] }\n",
    "    \n",
    "    # Create the tooltips\n",
    "    tooltips = [ alt.Tooltip(f\"{pparams['value_col']}:Q\", format=pparams['val_format']) ]\n",
    "    for cn in tcols:\n",
    "        if cn in label_dict:\n",
    "            data[cn+'_label'] = data[cn].astype('object').replace({ k:tfn(v) for k,v in label_dict[cn].items() })\n",
    "            t = alt.Tooltip(f\"{cn}_label:N\",title=cn)\n",
    "        else:\n",
    "            t = alt.Tooltip(f\"{cn}:N\")\n",
    "        tooltips.append(t)\n",
    "            \n",
    "    return tooltips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Function that takes filtered raw data and plot information and outputs the plot\n",
    "# Handles all of the data wrangling and parameter formatting\n",
    "def create_plot(pparams, data_meta, pp_desc, alt_properties={}, alt_wrapper=None, dry_run=False, width=200, return_matrix_of_plots=False, translate=None):\n",
    "    \n",
    "    data = pparams['data']\n",
    "\n",
    "    plot_meta = get_plot_meta(pp_desc['plot'])\n",
    "    col_meta = extract_column_meta(data_meta)\n",
    "    \n",
    "    if 'plot_args' in pp_desc: pparams.update(pp_desc['plot_args'])\n",
    "    pparams['color_scale'] = meta_color_scale(col_meta[pp_desc['res_col']],'colors',data[pp_desc['res_col']],translate=translate)\n",
    "    if data[pp_desc['res_col']].dtype.name=='category':\n",
    "        pparams['cat_order'] = list(data[pp_desc['res_col']].dtype.categories) \n",
    "        \n",
    "    pparams['val_format'] = vod(pp_desc,'value_format','.1%' if pparams['value_col'] == 'percent' else '.1f')\n",
    "    pparams['translate'] = translate if translate is not None else (lambda s: s)\n",
    "\n",
    "    # Handle factor columns \n",
    "    factor_cols = vod(pp_desc,'factor_cols',[])\n",
    "    \n",
    "    # If we have a question column not handled by the plot, add it to factors:\n",
    "    pparams['question_col'] = 'question'\n",
    "    if 'question' in data.columns and not vod(plot_meta,'question'):\n",
    "        factor_cols = factor_cols + ['question']\n",
    "    # If we don't have a question column but need it, just fill it with res_col name\n",
    "    elif 'question' not in data.columns and vod(plot_meta,'question'):\n",
    "        data.loc[:,'question'] = pd.Categorical([pp_desc['res_col']]*len(pparams['data']))\n",
    "        \n",
    "    if vod(plot_meta,'question'):\n",
    "        pparams['question_color_scale'] = meta_color_scale(col_meta[pp_desc['res_col']],'question_colors',data['question'],translate=translate)\n",
    "        pparams['question_order'] = list(data['question'].dtype.categories) \n",
    "    \n",
    "    if vod(plot_meta,'continuous') and 'cat_col' in pparams:\n",
    "        to_ind = 1 if len(factor_cols)>0 and vod(pp_desc,'internal_facet') else 0\n",
    "        factor_cols = factor_cols.copy()\n",
    "        factor_cols.insert(to_ind,pparams['cat_col'])\n",
    "    \n",
    "    if factor_cols:\n",
    "        # See if we should use it as an internal facet?\n",
    "        plot_args = vod(pp_desc,'plot_args',{})\n",
    "        if vod(pp_desc,'internal_facet'):\n",
    "            pparams['factor_col'] = factor_cols[0]\n",
    "            if factor_cols[0] == 'question':\n",
    "                pparams['factor_color_scale'] = meta_color_scale(col_meta[pp_desc['res_col']],'question_colors',data['question'],translate=translate)\n",
    "            else:\n",
    "                pparams['factor_color_scale'] = meta_color_scale(col_meta[factor_cols[0]],'colors',data[factor_cols[0]],translate=translate)\n",
    "            pparams['factor_order'] = list(data[factor_cols[0]].dtype.categories) \n",
    "            factor_cols = factor_cols[1:] # Leave rest for external faceting\n",
    "            if 'factor_meta' in plot_meta: \n",
    "                for kw in plot_meta['factor_meta']: pparams[kw] = vod(col_meta[pparams['factor_col']],kw)\n",
    "                \n",
    "        # If needed, use the next facet to replace the question dimension\n",
    "        if vod(plot_meta,'question') and factor_cols and vod(pp_desc,'replace_question'):\n",
    "            pparams['question_col'] = factor_cols[0]\n",
    "            pparams['question_order'] = list(data[factor_cols[0]].dtype.categories)\n",
    "            pparams['question_color_scale'] = meta_color_scale(col_meta[factor_cols[0]],'colors',data[factor_cols[0]],translate=translate)\n",
    "            factor_cols = factor_cols[1:]\n",
    "                \n",
    "    # Handle tooltip\n",
    "    pparams['tooltip'] = create_tooltip(pparams,col_meta)\n",
    "    \n",
    "    # Handle translations\n",
    "    if translate:\n",
    "        # Translate data - column names, categorical columns\n",
    "        pparams['data'] = data = translate_df(data,translate)\n",
    "        \n",
    "        # Provide a list of translated params - translate either direct if string or elemwise if list\n",
    "        translate_list = ['res_col','value_col','factor_col', 'question_col', 'cat_order', 'factor_order', 'question_order']\n",
    "        for k, v in pparams.items():\n",
    "            if k not in translate_list: continue\n",
    "            if isinstance(v,str): pparams[k] = translate(v)\n",
    "            else: pparams[k] = [ translate(c) for c in v ]\n",
    "                \n",
    "        # Translate facets too\n",
    "        factor_cols = [ translate(c) for c in factor_cols ]\n",
    "    \n",
    "    # If we still have more than 1 factor - merge the rest\n",
    "    if len(factor_cols)>1:\n",
    "        n_facet_cols = len(data[factor_cols[-1]].dtype.categories)\n",
    "        if not return_matrix_of_plots:\n",
    "            factor_col = '+'.join(factor_cols)\n",
    "            jfs = data[factor_cols].agg(', '.join, axis=1)\n",
    "            data.loc[:,factor_col] = pd.Categorical(jfs,jfs.unique())\n",
    "            pparams['data'] = data\n",
    "            n_facet_cols = len(data[factor_cols[-1]].dtype.categories)\n",
    "            factor_cols = [factor_col]\n",
    "    else:\n",
    "        n_facet_cols = vod(plot_meta,'factor_columns',1)\n",
    "        \n",
    "    # Allow value col name to be changed. This can be useful in distinguishing different aggregation options for a column\n",
    "    if 'value_name' in pp_desc: \n",
    "        pparams['data'] = pparams['data'].rename(columns={pparams['value_col']:pp_desc['value_name']})\n",
    "        pparams['value_col'] = pp_desc['value_name']\n",
    "    \n",
    "    # Create the plot using it's function\n",
    "    if dry_run: return pparams\n",
    "\n",
    "    if factor_cols: n_facet_cols = vod(plot_args,'n_facet_cols',n_facet_cols) # Allow plot_args to override col nr\n",
    "    dims = {'width': width//n_facet_cols if factor_cols else width}\n",
    "    if 'aspect_ratio' in plot_meta:   dims['height'] = int(dims['width']/plot_meta['aspect_ratio'])        \n",
    "    \n",
    "    # Make plot properties available to plot function (mostly useful for as_is plots)\n",
    "    pparams.update({'width':width}); pparams['alt_properties'] = alt_properties; pparams['outer_factors'] = factor_cols\n",
    "    \n",
    "    # Trim down parameters list if needed\n",
    "    plot_fn = get_plot_fn(pp_desc['plot'])\n",
    "    pparams = clean_kwargs(plot_fn,pparams)\n",
    "    \n",
    "    \n",
    "    if alt_wrapper is None: alt_wrapper = lambda p: p\n",
    "    if vod(plot_meta,'as_is'): # if as_is set, just return the plot as-is\n",
    "        return plot_fn(**pparams)\n",
    "    elif factor_cols:\n",
    "        if return_matrix_of_plots: \n",
    "            del pparams['data']\n",
    "            combs = it.product( *[data[fc].dtype.categories for fc in factor_cols ])\n",
    "            #print( [ data[(data[factor_cols]==c).all(axis=1)] for c in combs ] )\n",
    "            #print(list(combs))\n",
    "            return list(batch([\n",
    "                alt_wrapper(plot_fn(data[(data[factor_cols]==c).all(axis=1)],**pparams).properties(title='-'.join(map(str,c)),**dims, **alt_properties))\n",
    "                for c in combs\n",
    "                ], n_facet_cols))\n",
    "        else: # Use faceting:\n",
    "            plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties).facet(f'{factor_cols[0]}:O',columns=n_facet_cols))\n",
    "    else:\n",
    "        plot = alt_wrapper(plot_fn(**pparams).properties(**dims, **alt_properties))\n",
    "        if return_matrix_of_plots: plot = [[plot]]\n",
    "\n",
    "    return plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_desc = {\n",
    "    'res_col' : 'thermometer',\n",
    "    'factor_cols': ['party_preference'],\n",
    "    'filter': { 'nationality': 'Estonian' },\n",
    "    'plot': 'matrix-cont',\n",
    "    'internal_facet': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fdf = get_filtered_data(full_df, data_meta, pp_desc)\n",
    "#wdf = wrangle_data(fdf, **args, **get_plot_meta(args['plot']))\n",
    "#fdf['data'].sample(5)\n",
    "create_plot(fdf,data_meta,pp_desc,width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# A convenience function to draw a plot straight from a dataset\n",
    "def e2e_plot(pp_desc, data_file=None, full_df=None, data_meta=None, width=800, check_match=True,lazy=False,**kwargs):\n",
    "    if data_file is None and full_df is None:\n",
    "        raise Exception('Data must be provided either as data_file or full_df')\n",
    "    if data_file is None and data_meta is None:\n",
    "        raise Exception('If data provided as full_df then data_meta must also be given')\n",
    "        \n",
    "    if full_df is None: \n",
    "        if data_file.endswith('.parquet'): # Try lazy loading as it only loads what it needs from disk\n",
    "            full_df, full_meta = load_parquet_with_metadata(data_file,lazy=lazy)\n",
    "            dm = full_meta['data']\n",
    "        else: full_df, dm = read_annotated_data(data_file)\n",
    "        if data_meta is None: data_meta = dm\n",
    "    \n",
    "    matches = matching_plots(pp_desc, full_df, data_meta, details=True, list_hidden=True)\n",
    "    \n",
    "    if pp_desc['plot'] not in matches: \n",
    "        raise Exception(f\"Plot not registered: {pp_desc['plot']}\")\n",
    "    \n",
    "    fit, imp = matches[pp_desc['plot']]\n",
    "    if  fit<0:\n",
    "        raise Exception(f\"Plot {pp_desc['plot']} not applicable in this situation because of flags {imp}\")\n",
    "        \n",
    "    pparams = get_filtered_data(full_df, data_meta, pp_desc)\n",
    "    return create_plot(pparams, data_meta, pp_desc, width=width,**kwargs)\n",
    "\n",
    "# Another convenience function to simplify testing new plots\n",
    "def test_new_plot(fn, pp_desc, *args, plot_meta={}, **kwargs):\n",
    "    stk_plot(**{**plot_meta,'plot_name':'test'})(fn) # Register the plot under name 'test'\n",
    "    pp_desc = {**pp_desc, 'plot': 'test'}\n",
    "    res = e2e_plot(pp_desc,*args,**kwargs)\n",
    "    stk_deregister('test') # And de-register it again\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../samples/w25_bootstrap.parquet'\n",
    "data_metafile = '../../salk_internal_package/data/master_meta.json'\n",
    "if data_metafile:\n",
    "    data_meta = read_json(data_metafile)\n",
    "\n",
    "td = { 'unit': 'Üksus', 'Keskerakond':'Kekre', 'education': 'Haridus', 'Basic education':'Põhiharidus' }\n",
    "\n",
    "def translate(s):\n",
    "    return (td[s] if s in td else s)\n",
    "    \n",
    "e2e_plot({\n",
    "    'res_col' : 'age_group',\n",
    "    'factor_cols': ['party_preference'],\n",
    "    'filter': {},\n",
    "    'plot': 'boxplots',\n",
    "    'internal_facet': True\n",
    "}, data_file, data_meta=data_meta,width=800, translate=translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test e2e_plot\n",
    "#alt.data_transformers.disable_max_rows()\n",
    "pp_desc = {\n",
    "    'res_col' : 'income',\n",
    "    #'factor_cols': ['gender'],\n",
    "    'filter': { 'nationality': 'Estonian' },\n",
    "    'plot': 'boxplots',\n",
    "    'internal_facet': True\n",
    "}\n",
    "e2e_plot(pp_desc,data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Test test_new_plot\n",
    "def smooth(data, cat_col, value_col='value', color_scale=alt.Undefined, factor_col=None):\n",
    "    options_cols = list(data[cat_col].dtype.categories)\n",
    "    ldict = dict(zip(options_cols, range(len(options_cols))))\n",
    "    data.loc[:,'order'] = data[cat_col].astype('object').replace(ldict)\n",
    "    plot=alt.Chart(data\n",
    "        ).mark_area(interpolate='natural').encode(\n",
    "            x=alt.X(f'{factor_col}:O', title=None),\n",
    "            y=alt.Y(f'{value_col}:Q', title=None, stack='normalize',\n",
    "                 scale=alt.Scale(domain=[0, 1]), axis=alt.Axis(format='%')\n",
    "                 ),\n",
    "            order=\"order:O\",\n",
    "            color=alt.Color(cat_col, legend=alt.Legend(orient='top', title=None),\n",
    "                sort=alt.SortField(\"order\", \"descending\"), scale=color_scale\n",
    "                )\n",
    "        )\n",
    "    return plot\n",
    "\n",
    "test_new_plot(smooth, {\n",
    "    'res_col' : 'party_preference',\n",
    "    'factor_cols': ['age_group','gender'],  'filter': {},\n",
    "    'plot': 'area_smooth',\n",
    "    'internal_facet': True\n",
    "}, full_df=full_df, data_meta=data_meta, plot_meta={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../samples/w25_bootstrap.parquet'\n",
    "data_metafile = '../../salk_internal_package/data/master_meta.json'\n",
    "if data_metafile:\n",
    "    data_meta = read_json(data_metafile)\n",
    "    \n",
    "e2e_plot({\n",
    "    'res_col' : 'party_preference',\n",
    "    'factor_cols': ['unit'],\n",
    "    'filter': {},\n",
    "    'plot': 'geoplot',\n",
    "    'internal_facet': True\n",
    "}, data_file, data_meta=data_meta,width=800, return_matrix_of_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
