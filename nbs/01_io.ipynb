{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O\n",
    "> Functions to handle reading and writing datasets and model descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os, warnings\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyreadstat\n",
    "\n",
    "import salk_toolkit as stk\n",
    "from salk_toolkit.utils import replace_constants, is_datetime, warn, cached_fn, read_yaml\n",
    "from salk_toolkit.validation import DataMeta, DataDescription, soft_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Ignore fragmentation warnings\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        \"DataFrame is highly fragmented.*\",\n",
    "                        pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_json(fname,replace_const=True):\n",
    "    with open(fname,'r') as jf:\n",
    "        meta = json.load(jf)\n",
    "    if replace_const:\n",
    "        meta = replace_constants(meta)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def str_from_list(val):\n",
    "    if isinstance(val,list):\n",
    "        return '\\n'.join(val)\n",
    "    return str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This is here so we can easily track which files would be needed for a model\n",
    "# so we can package them together if needed\n",
    "\n",
    "# NB! for unpacking to work, the processing needs to not be changed w.r.t. paths\n",
    "# For this, we only map values when loading actual files, not when calling other functions here\n",
    "\n",
    "#  a global list of files that have been loaded\n",
    "stk_loaded_files_set = set()\n",
    "\n",
    "\n",
    "def get_loaded_files():\n",
    "    global stk_loaded_files_set\n",
    "    return list(stk_loaded_files_set)\n",
    "\n",
    "\n",
    "def reset_file_tracking():\n",
    "    global stk_loaded_files_set\n",
    "    stk_loaded_files_set.clear()\n",
    "\n",
    "\n",
    "# a global map that allows remapping file paths/names to different paths\n",
    "stk_file_map = {}\n",
    "\n",
    "\n",
    "def get_file_map():\n",
    "    global stk_file_map\n",
    "    return stk_file_map.copy()\n",
    "\n",
    "\n",
    "def set_file_map(file_map):\n",
    "    global stk_file_map\n",
    "    stk_file_map = file_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Read files listed in meta['file'] or meta['files']\n",
    "def read_concatenate_files_list(meta,data_file=None,path=None,**kwargs):\n",
    "    global stk_loaded_files_set, stk_file_map\n",
    "\n",
    "    opts = meta['read_opts'] if 'read_opts' in meta else {}\n",
    "    if data_file: data_files = [{ 'file': data_file, 'opts': opts}]\n",
    "    elif meta.get('file'): data_files = [{ 'file': meta['file'], 'opts': opts }]\n",
    "    elif meta.get('files'): data_files = meta['files']\n",
    "    else: raise Exception(\"No files provided\")\n",
    "\n",
    "    data_files = [  {'opts': opts, **f } if isinstance(f,dict) else\n",
    "                    {'opts': opts, 'file': f } for f in data_files ]\n",
    "\n",
    "    cat_dtypes = {}\n",
    "    raw_dfs, metas, einfo = [], [], {}\n",
    "    for fi, fd in enumerate(data_files):\n",
    "\n",
    "        data_file, opts = fd['file'], fd['opts']\n",
    "        if path: data_file = os.path.join(os.path.dirname(path),data_file)\n",
    "        mapped_file = stk_file_map.get(data_file,data_file)\n",
    "\n",
    "        extension = os.path.splitext(data_file)[1][1:].lower()\n",
    "        if extension in ['json', 'parquet', 'yaml']: # Allow loading metafiles or annotated data\n",
    "            if extension == 'json': warn(f\"Processing {data_file}\") # Print this to separate warnings for input jsons from main\n",
    "            # Pass in orig_data_file here as it might loop back to this function here and we need to preserve paths\n",
    "            raw_data, meta = read_annotated_data(data_file, infer=False, return_meta=True, **kwargs)\n",
    "            if meta is not None: metas.append(meta)\n",
    "        elif extension in ['csv', 'gz']:\n",
    "            raw_data = pd.read_csv(mapped_file, low_memory=False, **opts)\n",
    "        elif extension in ['sav','dta']:\n",
    "            read_fn = getattr(pyreadstat,'read_'+(mapped_file[-3:]).lower())\n",
    "            with warnings.catch_warnings(): # While pyreadstat has not been updated to pandas 2.2 standards\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw_data, fmeta = read_fn(mapped_file, **{ 'apply_value_formats':True, 'dates_as_pandas_datetime':True },**opts)\n",
    "                einfo.update(fmeta.__dict__) # Allow the fields in meta to be used just like self-defined constants\n",
    "        elif extension in ['xls', 'xlsx', 'xlsm', 'xlsb', 'odf', 'ods', 'odt']:\n",
    "            raw_data = pd.read_excel(mapped_file, **opts)\n",
    "        else:\n",
    "            raise Exception(f\"Not a known file format for {data_file}: {extension}\")\n",
    "\n",
    "        stk_loaded_files_set.add(mapped_file)\n",
    "\n",
    "        # If data is multi-indexed, flatten the index\n",
    "        if isinstance(raw_data.columns,pd.MultiIndex): raw_data.columns = [\" | \".join(tpl) for tpl in raw_data.columns]\n",
    "\n",
    "        # Add extra columns to raw data that contain info about the file. Always includes column 'file' with filename and file_ind with index\n",
    "        # Can be used to add survey_date or other useful metainfo\n",
    "        if len(data_files)>1: raw_data['file_ind'] = fi\n",
    "        for k,v in fd.items():\n",
    "            if k in ['opts']: continue\n",
    "            if len(data_files)<=1 and k in ['file']: continue\n",
    "            raw_data[k] = v\n",
    "            if isinstance(v,str): cat_dtypes[k] = None\n",
    "\n",
    "        # Handle categorical types for more complex situations\n",
    "        for c in raw_data.columns:\n",
    "            if raw_data[c].dtype.name == 'object' and not isinstance(raw_data[c].dropna().iloc[0],list):\n",
    "                cat_dtypes[c] = cat_dtypes.get(c,None) # Infer a categorical type unless already given\n",
    "            elif raw_data[c].dtype.name == 'category' and len(data_files) > 1: # Strip categories when multiple files involved\n",
    "                if c not in cat_dtypes or len(cat_dtypes[c].categories)<=len(raw_data[c].dtype.categories):\n",
    "                    cat_dtypes[c] = raw_data[c].dtype\n",
    "                raw_data[c] = raw_data[c].astype('object')\n",
    "\n",
    "        raw_dfs.append(raw_data)\n",
    "\n",
    "    fdf = pd.concat(raw_dfs)\n",
    "\n",
    "    # Restore categoricals\n",
    "    if len(cat_dtypes)>0:\n",
    "        for c, dtype in cat_dtypes.items():\n",
    "            if dtype is None: # Added as an extra field, infer categories\n",
    "                s = fdf[c].dropna()\n",
    "                if (s.dtype.name == 'object' and\n",
    "                    not isinstance(s.iloc[0],str) and # Check for string as string is also iterable\n",
    "                    isinstance(s.iloc[0],Iterable)): continue # Skip if it's a list or tuple or ndarray\n",
    "                dtype = pd.Categorical([],list(s.unique())).dtype\n",
    "            elif not set(fdf[c].dropna().unique()) <= set(dtype.categories): # If the categories are not the same, create a new dtype\n",
    "                #print(set(fdf[c].dropna().unique()), set(dtype.categories))\n",
    "                dtype = pd.Categorical([],list(fdf[c].dropna().unique())).dtype\n",
    "                warn(f\"Categories for {c} are different between files - merging to total {len(dtype.categories)} cats\")\n",
    "            fdf[c] = pd.Categorical(fdf[c],dtype=dtype)\n",
    "\n",
    "    if metas: # Do we have any metainfo?\n",
    "        meta = metas[-1]\n",
    "        # This will fix categories inside meta too\n",
    "        fix_meta_categories(meta,fdf,warnings=False)\n",
    "        # TODO: one should also merge the structures in case the columns don't match\n",
    "        return fdf, meta, einfo\n",
    "    else: return fdf, None, einfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# convert number series to categorical, avoiding long and unweildy fractions like 24.666666666667\n",
    "# This is a practical judgement call right now - round to two digits after comma and remove .00 from integers\n",
    "def convert_number_series_to_categorical(s):\n",
    "    return s.astype('float').map('{:.2f}'.format).str.replace('.00','').replace({'nan':None})\n",
    "\n",
    "\n",
    "def is_series_of_lists(s):\n",
    "    s_rep = s.dropna().iloc[0] # Find a non-na element\n",
    "    return isinstance(s_rep,list) or isinstance(s_rep,np.ndarray)\n",
    "\n",
    "\n",
    "def throw_vals_left(df):\n",
    "    # Helper fun to move inplace all nan values to right.\n",
    "    df.iloc[:,:] = df.apply(lambda l: sorted(l, key=pd.isna),axis=1).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# We want to show docs for this\n",
    "def create_topk_metas_and_dfs(df, dict_with_create):\n",
    "    r\"\"\" Create new columns and metas.\n",
    "\n",
    "    Meta args:\n",
    "        from_columns (str)          Regex template with groups to select df \n",
    "            cols from. If more than one group, then separate into subgroups.\n",
    "        res_cols (str)              Regex template with groups to create new \n",
    "            cols. Note that new structure is created for each subgroup.\n",
    "            Currently res_cols assumes same number of groups in from_columns.\n",
    "        na_vals (list)              List of values to consider as NA.\n",
    "        scale (dict, optional)      Scale block to be added to each subgroup.\n",
    "        name (str, optional)        Name of the new columns. Defaults to\n",
    "            dict_with_create['name'] + \"_topk\".\n",
    "        k (int, optional)           Number of new cols to create per subgroup.\n",
    "            Defaults to max and selects columns that include other vals than NA.\n",
    "        agg_index (int, optional)   Index of the from_cols group to be \n",
    "            aggregated. Note that 1 is first group, defaults to last group.\n",
    "\n",
    "    Example:\n",
    "        df = pd.DataFrame({\n",
    "            'A_1': ['Mentioned'],\n",
    "            'A_2': ['Not mentioned'],\n",
    "            'B_1': ['Not mentioned'],\n",
    "            'B_2': ['Mentioned']\n",
    "        }, columns = ['A_1', 'A_2', 'B_1', 'B_2'])\n",
    "        meta = {\n",
    "            'from_columns': r'(.+)_(\\d+)',\n",
    "            'res_cols': r'\\1_R\\2',\n",
    "            'name': 'R10',\n",
    "            'k': 1\n",
    "        }\n",
    "        Creates subgroup ['A_1', 'A_2'] that map to ['A_R1'] \n",
    "            and subgroup ['B_1', 'B_2'] that map to ['B_R1']\n",
    "            and creates metas with name 'R10_A' and 'R10_B'.\n",
    "        res = pd.DataFrame({\n",
    "            'A_R1': [1], #if scale has translate, 1 will be translated\n",
    "            'B_R1': [2]  #if scale has translate, 2 will be translated\n",
    "        }, columns = ['A_R1', 'B_R1'])\n",
    "        meta1 = {\n",
    "            'name': 'R10_A',\n",
    "            'columns': [['A_R1']]\n",
    "        }\n",
    "        meta2 = {\n",
    "            'name': 'R10_B',\n",
    "            'columns': [['B_R1']]\n",
    "        }\n",
    "    \"\"\"\n",
    "    create = dict_with_create['create']\n",
    "    name = create.get('name', f\"{dict_with_create['name']}_{create['type']}\")\n",
    "    # Compile regex to catch df.columns that we want to apply topk for.\n",
    "    regex_from = re.compile(create['from_columns'])\n",
    "    from_cols = list(filter(lambda s: regex_from.match(s), df.columns))\n",
    "    # agg_ind is index of the regex group that we want to aggregate over.\n",
    "    # All other indeces are unique identifiers for each subgroup\n",
    "    # e.g. ['A'] and ['B'] (that are also added to meta names)\n",
    "    # Recall that regex group 0 is the whole match.\n",
    "    # Note that re.Match.groups() does not include the whole match.\n",
    "    # This means that we need to subtract 1 from agg_ind.\n",
    "    agg_ind = create.get('agg_index', -1)\n",
    "    agg_ind = agg_ind-1 if agg_ind > 0 else agg_ind\n",
    "    n_groups = len(regex_from.match(from_cols[0]).groups())\n",
    "    has_subgroups = n_groups >= 2 # Multiple aggregations needed?\n",
    "    regex_to = create.get('res_cols', '')\n",
    "    kmax = create.get('k',None)\n",
    "    na_vals = create.get('na_vals', [])\n",
    "    if has_subgroups:\n",
    "        # collect all subgroups, later aggregate each subgroup separately\n",
    "        def get_subgroup_id(column):\n",
    "            \"Discard the aggregation index and collect the rest as identifier.\"\n",
    "            subgroup_id = list(regex_from.match(column).groups())\n",
    "            subgroup_id.pop(agg_ind)\n",
    "            return tuple(subgroup_id)\n",
    "        subgroups_ids = set(map(get_subgroup_id,from_cols))\n",
    "        subgroups = [\n",
    "            [col for col in from_cols if get_subgroup_id(col)==subgroup_id]\n",
    "            for subgroup_id in subgroups_ids\n",
    "            ]\n",
    "    else: subgroups = [from_cols]\n",
    "    topk_dfs, subgroup_metas = [], []\n",
    "\n",
    "    # select group at agg_ind in col name to allow translate if spec-d in scale\n",
    "    # e.g. {A_11: selected} |-> {11: selected}, later by using mask |-> {11: 11}\n",
    "    # this fun is def-d in current fun, so agg_ind acts as global var\n",
    "    get_regex_group_at_agg_ind = lambda s: regex_from.match(s).groups()[agg_ind]\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "        sdf = df[subgroup].astype('object').replace(na_vals,None)\n",
    "        newcols = [\n",
    "            # from_cols names map to res_cols names\n",
    "            # note regex groups stay the same: e.g A_11 |-> A_R11\n",
    "            regex_from.match(col).expand(regex_to) for col in sdf.columns\n",
    "            ]\n",
    "\n",
    "        # Convert one-hot encoded columns into a list-of-selected format\n",
    "        sdf.columns = sdf.columns.map(get_regex_group_at_agg_ind)\n",
    "        sdf = sdf.mask(~sdf.isna(), other=pd.Series(sdf.columns, index=sdf.columns), axis=1) # replace cell with column name where not NA\n",
    "        throw_vals_left(sdf) # changes df in place, Nones go to rightmost side\n",
    "\n",
    "        sdf.columns = newcols # set column names per the regex_to template\n",
    "        sdf = sdf.dropna(axis=1,how='all') # drop rightmost cols that are all NA\n",
    "        sdf = sdf.iloc[:,:kmax] if kmax else sdf # up to kmax columns if spec-d\n",
    "        sname = name\n",
    "        if has_subgroups:\n",
    "            sname += '_'+'_'.join(map(str,get_subgroup_id(subgroup[0])))\n",
    "        meta_subgroup = {\n",
    "            'name': sname,\n",
    "            'scale': create.get('scale', {}),\n",
    "            'columns': sdf.columns.tolist()\n",
    "            }\n",
    "        topk_dfs.append(sdf)\n",
    "        subgroup_metas.append(meta_subgroup)\n",
    "    return subgroup_metas, topk_dfs #note: each df has one meta for zip later\n",
    "\n",
    "\n",
    "create_block_type_to_create_fn = {\n",
    "    'topk': create_topk_metas_and_dfs,\n",
    "    'maxdiff': NotImplementedError(\"Maxdiff not implemented yet\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def create_new_columns_and_metas(df,group):\n",
    "    \"One group can create multiple metas if it has >1 groups spec-d in regex.\"\n",
    "    type = group['create']['type']\n",
    "    if type not in create_block_type_to_create_fn:\n",
    "        raise NotImplementedError(f\"Create block {type} not supported\")\n",
    "    metas, dfs = create_block_type_to_create_fn[type](df,group)\n",
    "    return zip(dfs, metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default usage with mature metafile: process_annotated_data(<metafile name>)\n",
    "# When figuring out the metafile, it can also be run as: process_annotated_data(meta=<dict>, data_file=<>)\n",
    "def process_annotated_data(meta_fname=None, meta=None, data_file=None, raw_data=None,\n",
    "                        return_meta=False, ignore_exclusions=False, only_fix_categories=False, return_raw=False, add_original_inds=False):\n",
    "    # Read metafile\n",
    "    metafile = stk_file_map.get(meta_fname,meta_fname)\n",
    "    if meta_fname is not None:\n",
    "        ext = os.path.splitext(metafile)[1]\n",
    "        if ext == '.yaml':\n",
    "            meta = read_yaml(metafile)\n",
    "        elif ext == '.json':\n",
    "            meta = read_json(metafile)\n",
    "        else:\n",
    "            raise Exception(f\"Unknown meta file format {ext} for file: {meta_fname}\")\n",
    "\n",
    "    # Print any issues with the meta without raising an error - for now\n",
    "    soft_validate(meta,DataMeta)\n",
    "\n",
    "    # Setup constants with a simple replacement mechanic\n",
    "    constants = meta['constants'] if 'constants' in meta else {}\n",
    "    meta = replace_constants(meta)\n",
    "\n",
    "    # Read datafile(s)\n",
    "    if raw_data is None:\n",
    "        raw_data, inp_meta, einfo = read_concatenate_files_list(meta,data_file,path=meta_fname)\n",
    "        if inp_meta is not None: warn(f\"Processing main meta file\") # Print this to separate warnings for input jsons from main\n",
    "    else: einfo = {}\n",
    "\n",
    "    if return_raw: return (raw_data, meta) if return_meta else raw_data\n",
    "\n",
    "    globs = {'pd':pd, 'np':np, 'sp':sp, 'stk':stk, 'df':raw_data, **einfo, **constants }\n",
    "\n",
    "    if 'preprocessing' in meta and not only_fix_categories:\n",
    "        exec(str_from_list(meta['preprocessing']),globs)\n",
    "        raw_data = globs['df']\n",
    "\n",
    "    ndf = pd.DataFrame()\n",
    "    all_cns = dict()\n",
    "    for group in meta['structure']:\n",
    "        if group['name'] in all_cns:\n",
    "            raise Exception(f\"Group name {group['name']} duplicates a column name in group {all_cns[cn]}\")\n",
    "        all_cns[group['name']] = group['name']\n",
    "        g_cols = []\n",
    "        if 'create' in group:\n",
    "            for newdf, newmeta in create_new_columns_and_metas(raw_data, group):\n",
    "                # If same name cols, we overwrite the old ones\n",
    "                raw_data = newdf.combine_first(raw_data)\n",
    "                # Note we are appending to the list that we are iterating over\n",
    "                meta['structure'].append(newmeta)\n",
    "            del group['create'] # clean up the meta\n",
    "        for tpl in group['columns']:\n",
    "            if type(tpl)==list:\n",
    "                cn = tpl[0] # column name\n",
    "                sn = tpl[1] if len(tpl)>1 and type(tpl[1])==str else cn # source column\n",
    "                o_cd = tpl[2] if len(tpl)==3 else tpl[1] if len(tpl)==2 and type(tpl[1])==dict else {} # metadata\n",
    "            else:\n",
    "                cn = sn = tpl\n",
    "                o_cd = {}\n",
    "\n",
    "            cd = {**group.get('scale',{}),**o_cd}\n",
    "\n",
    "            # Col prefix is used to avoid name clashes when different groups naturally share same column names\n",
    "            if 'col_prefix' in cd: cn = cd['col_prefix']+cn\n",
    "\n",
    "            # Detect duplicate columns in meta - including among those missing or generated\n",
    "            # Only flag if they are duplicates even after prefix\n",
    "            if cn in all_cns:\n",
    "                raise Exception(f\"Duplicate column name found: '{cn}' in {all_cns[cn]} and {group['name']}\")\n",
    "            all_cns[cn] = group['name']\n",
    "\n",
    "            if only_fix_categories: sn = cn\n",
    "            g_cols.append(cn)\n",
    "\n",
    "            if sn not in raw_data:\n",
    "                if not group.get('generated'): # bypass warning for columns marked as being generated later\n",
    "                    warn(f\"Column {sn} not found\")\n",
    "                continue\n",
    "\n",
    "            if raw_data[sn].isna().all():\n",
    "                warn(f\"Column {sn} is empty and thus ignored\")\n",
    "                continue\n",
    "\n",
    "            s = raw_data[sn]\n",
    "            if not only_fix_categories and not is_series_of_lists(s):\n",
    "                if s.dtype.name=='category': s = s.astype('object') # This makes it easier to use common ops like replace and fillna\n",
    "                if 'translate' in cd:\n",
    "                    s = s.astype('str').replace(cd['translate']).replace('nan',None).replace('None',None)\n",
    "                if 'transform' in cd: s = eval(cd['transform'],{ 's':s, 'df':raw_data, 'ndf':ndf, 'pd':pd, 'np':np, 'stk':stk , **constants })\n",
    "                if 'translate_after' in cd:\n",
    "                    s = pd.Series(s).astype('str').replace(cd['translate_after']).replace('nan',None).replace('None',None)\n",
    "\n",
    "                if cd.get('datetime'): s = pd.to_datetime(s,errors='coerce')\n",
    "                elif cd.get('continuous'): s = pd.to_numeric(s,errors='coerce')\n",
    "\n",
    "            s = pd.Series(s,name=cn) # In case transformation removes the name or renames it\n",
    "\n",
    "            if cd.get('categories') and not is_series_of_lists(s):\n",
    "                na_sum = s.isna().sum()\n",
    "\n",
    "                if cd['categories'] == 'infer':\n",
    "                    if s.dtype.name=='category': cd['categories'] = list(s.dtype.categories) # Categories come from data file\n",
    "                    elif 'translate' in cd and 'transform' not in cd and set(cd['translate'].values()) >= set(s.dropna().unique()): # Infer order from translation dict\n",
    "                        cats = [ str(c) for c in cd['translate'].values() if c in s.unique() ]\n",
    "                        cd['categories'] = list(dict.fromkeys(cats)) # As mapping can be many-to-one, we need to use unique\n",
    "                        s = s.astype('str')\n",
    "                    else: # Just use lexicographic ordering\n",
    "                        if cd.get('ordered',False) and not pd.api.types.is_numeric_dtype(s):\n",
    "                            warn(f\"Ordered category {cn} had category: infer. This only works correctly if you want lexicographic ordering!\")\n",
    "                        if not pd.api.types.is_numeric_dtype(s): s.loc[~s.isna()] = s[~s.isna()].astype(str) # convert all to string to avoid type issues in sorting for mixed columns\n",
    "                        cinds = s.drop_duplicates().sort_values().index # NB! Important to do this still with numbers before converting them to strings\n",
    "                        if pd.api.types.is_numeric_dtype(s): s = convert_number_series_to_categorical(s)\n",
    "                        cd['categories'] = [ c for c in s[cinds] if pd.notna(c) ] # Also propagates it into meta (unless shared scale)\n",
    "\n",
    "                    # Replace categories with those inferred in the output meta\n",
    "                    # Many things in pp and model pipeline assume categories are set so this is a necessity\n",
    "                    #o_cd['categories'] = cd['categories'] # Done later in fix_meta_categories\n",
    "                elif pd.api.types.is_numeric_dtype(s): # Numeric datatype being coerced into categorical - map to nearest category value\n",
    "                    try:\n",
    "                        fcats = np.array(cd['categories']).astype(float)\n",
    "                        s = pd.Series(np.array(cd['categories'])[np.abs(s.values[:,None] - fcats[None,:]).argmin(axis=1)],\n",
    "                                index=s.index, name=s.name,\n",
    "                                dtype=pd.CategoricalDtype(categories=cd['categories'],ordered=cd.get('ordered')))\n",
    "                    except:\n",
    "                        raise ValueError(f\"Categories for {cn} are not numeric: {cd['categories']}\")\n",
    "\n",
    "                cats = cd['categories']\n",
    "\n",
    "                ns = pd.Series(pd.Categorical(s, # NB! conversion to str already done before. Doing it here kills NA values\n",
    "                                                    categories=cats,ordered=cd['ordered'] if 'ordered' in cd else False), name=cn, index=raw_data.index)\n",
    "                # Check if the category list provided was comprehensive\n",
    "                new_nas = ns.isna().sum() - na_sum\n",
    "\n",
    "                if new_nas > 0:\n",
    "                    unlisted_cats = set(s.dropna().unique())-set(cats)\n",
    "                    warn(f\"Column {cn} {f'({sn}) ' if cn != sn else ''} had unknown categories {unlisted_cats} for { new_nas/len(ns) :.1%} entries\")\n",
    "\n",
    "                s = ns\n",
    "\n",
    "            # Update ndf in real-time so it would be usable in transforms for next columns\n",
    "            if s.name in ndf.columns: ndf = ndf.drop(columns=s.name) # Overwrite existing instead of duplicates\n",
    "            ndf = pd.concat([ndf,s],axis=1)\n",
    "\n",
    "        if 'subgroup_transform' in group:\n",
    "            subgroups = group.get('subgroups',[g_cols])\n",
    "            for sg in subgroups:\n",
    "                ndf[sg] = eval(group['subgroup_transform'],{ 'gdf':ndf[sg], 'df':raw_data, 'ndf':ndf, 'pd':pd, 'np':np, 'stk':stk , **constants })\n",
    "\n",
    "    if 'postprocessing' in meta and not only_fix_categories:\n",
    "        globs['df'] = ndf\n",
    "        exec(str_from_list(meta['postprocessing']),globs)\n",
    "        ndf = globs['df']\n",
    "\n",
    "    # Fix categories after postprocessing\n",
    "    # Also replaces infer with the actual categories\n",
    "    fix_meta_categories(meta,ndf,warnings=True)\n",
    "\n",
    "    ndf['original_inds'] = np.arange(len(ndf))\n",
    "    if 'excluded' in meta and not ignore_exclusions:\n",
    "        excl_inds = [ i for i,_ in meta['excluded'] ]\n",
    "        ndf = ndf[~ndf['original_inds'].isin(excl_inds)]\n",
    "    if not add_original_inds: ndf.drop(columns=['original_inds'],inplace=True)\n",
    "\n",
    "    return (ndf, meta) if return_meta else ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Read either a json annotation and process the data, or a processed parquet with the annotation attached\n",
    "# Return_raw is here for easier debugging of metafiles and is not meant to be used in production\n",
    "def read_annotated_data(fname, infer=True, return_raw=False, return_meta=False, **kwargs):\n",
    "    _, ext = os.path.splitext(fname)\n",
    "    meta = None\n",
    "    if ext == '.json' or ext == '.yaml':\n",
    "        data, meta =  process_annotated_data(fname, return_meta=True, return_raw=return_raw, **kwargs)\n",
    "    elif ext == '.parquet':\n",
    "        data, full_meta = read_parquet_with_metadata(fname)\n",
    "        meta = (full_meta or {}).get('data')\n",
    "\n",
    "    if meta is not None or not infer:\n",
    "        return (data, meta) if return_meta else data\n",
    "    else:\n",
    "        warn(f\"Warning: using inferred meta for {fname}\")\n",
    "        meta = infer_meta(fname,meta_file=False)\n",
    "        return process_annotated_data(data_file=fname, meta=meta, return_meta=return_meta)\n",
    "\n",
    "# Fix df dtypes etc using meta - needed after a lazy load\n",
    "\n",
    "\n",
    "def fix_df_with_meta(df, dmeta):\n",
    "    cmeta = extract_column_meta(dmeta)\n",
    "    for c in df.columns:\n",
    "        if c not in cmeta: continue\n",
    "        cd = cmeta[c]\n",
    "        if cd.get('categories'):\n",
    "            cats = list(df[c].unique()) if cd['categories'] == 'infer' else cd['categories']\n",
    "            df[c] = pd.Categorical(df[c],categories=cats,ordered=cd.get('ordered',False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Helper functions designed to be used with the annotations\n",
    "\n",
    "# Convert data_meta into a dict where each group and column maps to their metadata dict\n",
    "def extract_column_meta(data_meta):\n",
    "    res = defaultdict(lambda: {})\n",
    "    for g in data_meta['structure']:\n",
    "        base = g['scale'].copy() if 'scale' in g else {}\n",
    "        res[g['name']] = {**base, 'columns': [base.get('col_prefix','')+(t[0] if type(t)!=str else t) for t in g['columns']] }\n",
    "        base['label'] = None # Don't let that be carried over to individual columns\n",
    "        for cd in g['columns']:\n",
    "            if isinstance(cd,str): cd = [cd]\n",
    "            res[base.get('col_prefix','')+cd[0]] = {**base,**cd[-1]} if isinstance(cd[-1],dict) else base.copy()\n",
    "    return res\n",
    "\n",
    "# Convert data_meta into a dict of group_name -> [column names]\n",
    "# TODO: deprecate - info available in extract_column_meta\n",
    "\n",
    "\n",
    "def group_columns_dict(data_meta):\n",
    "    return { k: d['columns'] for k,d in extract_column_meta(data_meta).items() if 'columns' in d }\n",
    "\n",
    "    #return { g['name'] : [(t[0] if type(t)!=str else t) for t in g['columns']] for g in data_meta['structure'] }\n",
    "\n",
    "# Take a list and a dict and replace all dict keys in list with their corresponding lists in-place\n",
    "\n",
    "\n",
    "def list_aliases(lst, da):\n",
    "    return [ fv for v in lst for fv in (da[v] if isinstance(v,str) and v in da else [v]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list_aliases(['a','b','c'],{'b': ['x','y']}) == ['a', 'x', 'y', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Creates a mapping old -> new\n",
    "def get_original_column_names(dmeta):\n",
    "    res = {}\n",
    "    for g in dmeta['structure']:\n",
    "        for c in g['columns']:\n",
    "            if isinstance(c,str):\n",
    "                res[c] = c\n",
    "            elif isinstance(c, list):\n",
    "                if len(c)==1:\n",
    "                    res[c[0]] = c[0]\n",
    "                elif len(c)>=2 and isinstance(c[1],str):\n",
    "                    res[c[1]] = c[0]  # This is a rename: [new_name, old_name, ...]\n",
    "                elif len(c)>=1:\n",
    "                    res[c[0]] = c[0]  # This is a regular column with metadata: [name, {...}]\n",
    "    return res\n",
    "\n",
    "# Map ot backwards and nt forwards to move from one to the other\n",
    "\n",
    "\n",
    "def change_mapping(ot, nt, only_matches=False):\n",
    "    # Todo: warn about non-bijective mappings\n",
    "    matches = { v: nt[k] for k, v in ot.items() if k in nt and v!=nt[k] } # change those that are shared\n",
    "    if only_matches: return matches\n",
    "    else:\n",
    "        return { **{ v:k for k, v in ot.items() if k not in nt }, # undo those in ot not in nt\n",
    "                 **{ k:v for k, v in nt.items() if k not in ot }, # do those in nt not in ot\n",
    "                 **matches }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Change an existing dataset to correspond better to a new meta_data\n",
    "# This is intended to allow making small improvements in the meta even after a model has been run\n",
    "# It is by no means perfect, but is nevertheless a useful tool to avoid re-running long pymc models for simple column/translation changes\n",
    "def change_df_to_meta(df, old_dmeta, new_dmeta):\n",
    "    warn(\"This tool handles only simple cases of column name, translation and category order changes.\")\n",
    "\n",
    "    # Ready the metafiles for parsing\n",
    "    old_dmeta = replace_constants(old_dmeta); new_dmeta = replace_constants(new_dmeta)\n",
    "\n",
    "    # Rename columns\n",
    "    ocn, ncn = get_original_column_names(old_dmeta), get_original_column_names(new_dmeta)\n",
    "    name_changes = change_mapping(ocn,ncn,only_matches=True)\n",
    "    if name_changes != {}: print(f\"Renaming columns: {name_changes}\")\n",
    "    df.rename(columns=name_changes,inplace=True)\n",
    "\n",
    "    rev_name_changes = { v: k for k,v in name_changes.items() }\n",
    "\n",
    "    # Get metadata for each column\n",
    "    ocm = extract_column_meta(old_dmeta)\n",
    "    ncm = extract_column_meta(new_dmeta)\n",
    "\n",
    "    for c in ncm.keys():\n",
    "        if c not in df.columns: continue # probably group\n",
    "        if c not in ocm.keys(): continue # new column\n",
    "\n",
    "        ncd, ocd = ncm[c], ocm[rev_name_changes[c] if c in rev_name_changes else c]\n",
    "\n",
    "        # Warn about transformations and don't touch columns where those change\n",
    "        if ocd.get('transform') != ncd.get('transform'):\n",
    "            warn(f\"Column {c} has a different transformation. Leaving it unchanged\")\n",
    "            continue\n",
    "\n",
    "        # Handle translation changes\n",
    "        ot, nt = ocd.get('translate',{}), ncd.get('translate',{})\n",
    "        remap = change_mapping(ot,nt)\n",
    "        if remap != {}:\n",
    "            # Validate that mapping keys exist in current categories\n",
    "            invalid_keys = set(remap.keys()) - set(df[c].cat.categories)\n",
    "            if invalid_keys:\n",
    "                raise ValueError(f\"Translation mapping keys {invalid_keys} not found in current categories {list(df[c].cat.categories)} for column {c}\")\n",
    "            print(f\"Remapping {c} with {remap}\")\n",
    "            df[c] = df[c].cat.rename_categories(remap)\n",
    "\n",
    "        # Reorder categories and/or change ordered status\n",
    "        if ((ncd.get('categories','infer')!='infer' and\n",
    "            list(df[c].dtype.categories) != ncd.get('categories'))\n",
    "            or ocd.get('ordered') != ncd.get('ordered')):\n",
    "            cats = ncd.get('categories') if ncd.get('categories','infer') != 'infer' else df[c].dtype.categories\n",
    "            if isinstance(cats,list):\n",
    "                print(f\"Changing {c} to Cat({cats},ordered={ncd.get('ordered')})\")\n",
    "                df[c] = pd.Categorical(df[c],categories=cats,ordered=ncd.get('ordered'))\n",
    "\n",
    "    # Column order changes\n",
    "    gcdict = group_columns_dict(new_dmeta)\n",
    "\n",
    "    cols = ['draw','obs_idx','training_subsample'] + [ c for g in new_dmeta['structure'] for c in gcdict[g['name']]]\n",
    "    cols.append( new_dmeta['weight_col'] if new_dmeta.get('weight_col') else 'N')\n",
    "    cols = [ c for c in cols if c in df.columns ]\n",
    "\n",
    "    if len(set(df.columns) - set(cols)) > 0:\n",
    "        print(\"Dropping columns:\",set(df.columns) - set(cols))\n",
    "\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def update_meta_with_model_fields(meta, donor):\n",
    "    # Add the groups added by the model before to data_meta\n",
    "    existing_grps = { g['name'] for g in meta['structure'] }\n",
    "    meta['structure'] += [ grp for grp in donor['structure']\n",
    "        if grp.get('generated') and grp['name'] not in existing_grps ]\n",
    "\n",
    "    # Add back the fields added/changed by the model in sampling\n",
    "    meta['draws_data'] = donor.get('draws_data',[])\n",
    "    if 'total_size' in donor: meta['total_size'] = donor['total_size']\n",
    "    if 'weight_col' in donor: meta['weight_col'] = donor['weight_col']\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def replace_data_meta_in_parquet(parquet_name,metafile_name,advanced=True):\n",
    "    df, meta = read_parquet_with_metadata(parquet_name)\n",
    "\n",
    "    ometa = meta['data']\n",
    "    nmeta = read_json(metafile_name, replace_const=True)\n",
    "\n",
    "    nmeta = update_meta_with_model_fields(nmeta,ometa)\n",
    "\n",
    "    # Perform the column name changes and category translations\n",
    "    # Do this before inferring meta as categories might change in this step\n",
    "    if advanced: df = change_df_to_meta(df,ometa,nmeta)\n",
    "\n",
    "    nmeta = fix_meta_categories(nmeta,df) # replace infer with values\n",
    "\n",
    "    meta['original_data'] = meta.get('original_data',meta['data'])\n",
    "    meta['data'] = nmeta\n",
    "\n",
    "    write_parquet_with_metadata(df,meta,parquet_name)\n",
    "\n",
    "    return df, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# A function to infer categories (and validate the ones already present)\n",
    "# Works in-place\n",
    "def fix_meta_categories(data_meta, df, infers_only=False, warnings=True):\n",
    "    if 'structure' not in data_meta: return\n",
    "\n",
    "    for g in data_meta['structure']:\n",
    "        all_cats = set()\n",
    "        prefix = g.get('scale',{}).get('col_prefix','')\n",
    "        for c in g.get('columns',[]):\n",
    "            if isinstance(c,str): c = [c]\n",
    "            if prefix+c[0] in df.columns and df[prefix+c[0]].dtype.name == 'category':\n",
    "                cats, cm = list(df[prefix+c[0]].dtype.categories), c[-1]\n",
    "                if isinstance(cm,dict) and cm.get('categories') == 'infer':\n",
    "                    cm['categories'] = cats\n",
    "                elif ((not infers_only) and isinstance(cm,dict) and cm.get('categories') and\n",
    "                    not set(cm['categories']) >= set(cats)):\n",
    "                    diff = set(cats) - set(cm['categories'])\n",
    "                    if warnings: warn(f\"Fixing missing categories for {c[0]}: {diff}\")\n",
    "                    cm['categories'] = cats\n",
    "                all_cats |= set(cats)\n",
    "\n",
    "        if g.get('scale') and g['scale'].get('categories')=='infer':\n",
    "            # IF they all share same categories, keep the category order\n",
    "            scats = list(cats) if all_cats == set(cats) else sorted(list(all_cats))\n",
    "            g['scale']['categories'] = scats\n",
    "        elif ((not infers_only) and g.get('scale') and g['scale'].get('categories') and\n",
    "                not set(g['scale']['categories'])>=all_cats):\n",
    "            diff = all_cats - set(g['scale']['categories'])\n",
    "            if warnings: warn(f\"Fixing missing categories for group {g['name']}: {diff}\")\n",
    "            g['scale']['categories'] = list(all_cats)\n",
    "\n",
    "    return data_meta\n",
    "\n",
    "\n",
    "def fix_parquet_categories(parquet_name):\n",
    "    df, meta = read_parquet_with_metadata(parquet_name)\n",
    "    meta['data'] = fix_meta_categories(meta['data'],df,infers_only=False)\n",
    "    write_parquet_with_metadata(df,meta,parquet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def is_categorical(col):\n",
    "    return col.dtype.name in ['object', 'str', 'category'] and not is_datetime(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "max_cats = 50\n",
    "\n",
    "# Create a very basic metafile for a dataset based on it's contents\n",
    "# This is not meant to be directly used, rather to speed up the annotation process\n",
    "\n",
    "\n",
    "def infer_meta(data_file=None, meta_file=True, read_opts={}, df=None, translate_fn=None, translation_blacklist=[]):\n",
    "    meta = { 'constants': {}, 'read_opts': read_opts }\n",
    "\n",
    "    if translate_fn is not None:\n",
    "        otfn = translate_fn\n",
    "        translate_fn = cached_fn(lambda x: otfn(str(x)) if x else '' )\n",
    "    else: translate_fn = str\n",
    "\n",
    "    # Read datafile\n",
    "    col_labels = {}\n",
    "    if data_file is not None:\n",
    "        path, fname = os.path.split(data_file)\n",
    "        ext = os.path.splitext(fname)[1].lower()[1:]\n",
    "        meta['file'] = fname\n",
    "        if ext in ['csv', 'gz']:\n",
    "            df = pd.read_csv(data_file, low_memory=False, **read_opts)\n",
    "        elif ext in ['sav','dta']:\n",
    "            read_fn = getattr(pyreadstat,'read_'+ext)\n",
    "            df, sav_meta = read_fn(data_file, **{ 'apply_value_formats':True, 'dates_as_pandas_datetime':True },**read_opts)\n",
    "            col_labels = dict(zip(sav_meta.column_names, sav_meta.column_labels)) # Make this data easy to access by putting it in meta as constant\n",
    "            if translate_fn: col_labels = { k: translate_fn(v) for k,v in col_labels.items() }\n",
    "        elif ext == 'parquet':\n",
    "            df = pd.read_parquet(data_file, **read_opts)\n",
    "        elif ext in ['xls', 'xlsx', 'xlsm', 'xlsb', 'odf', 'ods', 'odt']:\n",
    "            df = pd.read_excel(data_file, **read_opts)\n",
    "        else:\n",
    "            raise Exception(f\"Not a known file format {data_file}\")\n",
    "\n",
    "    # If data is multi-indexed, flatten the index\n",
    "    if isinstance(df.columns,pd.MultiIndex): df.columns = [\" | \".join(tpl) for tpl in df.columns]\n",
    "\n",
    "    cats, grps = {}, defaultdict(lambda: list())\n",
    "\n",
    "    main_grp = { 'name': 'main', 'columns':[] }\n",
    "    meta['structure'] = [main_grp]\n",
    "\n",
    "    # Remove empty columns\n",
    "    cols = [ c for c in df.columns if df[c].notna().any() ]\n",
    "\n",
    "    # Determine category lists for all categories\n",
    "    for cn in cols:\n",
    "        if not is_categorical(df[cn]): continue\n",
    "        cats[cn] = sorted(list(df[cn].dropna().unique())) if df[cn].dtype.name != 'category' else list(df[cn].dtype.categories)\n",
    "\n",
    "        for cs in grps:\n",
    "            #if cn.startswith('Q2_'): print(len(set(cats[cn]) & cs)/len(cs),set(cats[cn]),cs)\n",
    "            if len(set(cats[cn]) & cs)/len(cs) > 0.75: # match to group if most of the values match\n",
    "                key = frozenset(cs | set(cats[cn]))\n",
    "                if key in grps: cs = key # Check if we already have this exact key (can happen)\n",
    "                lst = grps[cs]\n",
    "                del grps[cs]\n",
    "                grps[key] = lst + [cn]\n",
    "                break\n",
    "        else:\n",
    "            grps[frozenset(cats[cn])].append(cn)\n",
    "\n",
    "    # Fn to create the meta for a categorical column\n",
    "    def cat_meta(cn):\n",
    "        m = { 'categories': cats[cn] if len(cats[cn])<=max_cats else 'infer' }\n",
    "        if cn in df.columns and df[cn].dtype.name=='category' and df[cn].dtype.ordered: m['ordered'] = True\n",
    "        if translate_fn is not None and cn not in translation_blacklist and len(cats[cn])<=max_cats:\n",
    "            tdict = { c: translate_fn(c) for c in m['categories'] }\n",
    "            m['categories'] = 'infer' #[ tdict[c] for c in m['categories'] ]\n",
    "            m['translate'] = tdict\n",
    "        return m\n",
    "\n",
    "    # Create groups from values that share a category\n",
    "    handled_cols = set()\n",
    "    for k,g_cols in grps.items():\n",
    "        if len(g_cols)<2: continue\n",
    "\n",
    "        # Set up the columns part\n",
    "        m_cols = []\n",
    "        for cn in g_cols:\n",
    "            ce = [cn,{'label': col_labels[cn]}] if cn in col_labels else [cn]\n",
    "            if translate_fn is not None: ce = [translate_fn(cn)]+ ce\n",
    "            if len(ce) == 1: ce = ce[0]\n",
    "            m_cols.append(ce)\n",
    "\n",
    "        kl = [ str(c) for c in k]\n",
    "        cats[str(kl)] = kl # so cat_meta would use the full list\n",
    "\n",
    "        grp = { 'name': ';'.join(kl), 'scale': cat_meta(str(kl)), 'columns': m_cols }\n",
    "\n",
    "        meta['structure'].append(grp)\n",
    "        handled_cols.update(g_cols)\n",
    "\n",
    "    # Put the rest of variables into main category\n",
    "    main_cols = [ c for c in cols if c not in handled_cols ]\n",
    "    for cn in main_cols:\n",
    "        if cn in cats: cdesc = cat_meta(cn)\n",
    "        else:\n",
    "            if is_datetime(df[cn]): cdesc = {'datetime':True}\n",
    "            else: cdesc = {'continuous':True}\n",
    "        if cn in col_labels: cdesc['label'] = col_labels[cn]\n",
    "        main_grp['columns'].append([cn,cdesc] if translate_fn is None else [translate_fn(cn),cn,cdesc])\n",
    "\n",
    "    #print(json.dumps(meta,indent=2,ensure_ascii=False))\n",
    "\n",
    "    # Write file to disk\n",
    "    if data_file is not None and meta_file:\n",
    "        if meta_file is True: meta_file = os.path.join(path, os.path.splitext(fname)[0]+'_meta.json')\n",
    "        if not os.path.exists(meta_file):\n",
    "            print(f\"Writing {meta_file} to disk\")\n",
    "            with open(meta_file,'w',encoding='utf8') as jf:\n",
    "                json.dump(meta,jf,indent=2,ensure_ascii=False)\n",
    "        else:\n",
    "            print(f\"{meta_file} already exists, skipping write\")\n",
    "\n",
    "    return meta\n",
    "\n",
    "# Small convenience function to have a meta available for any dataset\n",
    "\n",
    "\n",
    "def data_with_inferred_meta(data_file, **kwargs):\n",
    "    meta = infer_meta(data_file,meta_file=False, **kwargs)\n",
    "    return process_annotated_data(meta=meta, data_file=data_file, return_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - Check if inferred meta is usable by process_annotated_data\n",
    "sdf, meta = data_with_inferred_meta('../data/salk25.sav',translate_fn=lambda t: f'[[{t}]]')\n",
    "sdf, meta = data_with_inferred_meta('../data/master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def perform_merges(df,merges,constants={}):\n",
    "    if not isinstance(merges,list): merges = [merges]\n",
    "    for ms in merges:\n",
    "        ndf = read_and_process_data(ms['file'],constants=constants)\n",
    "        on = ms['on'] if isinstance(ms['on'],list) else [ms['on']]\n",
    "        if ms.get('add'): ndf = ndf[ms['on']+ms['add']]\n",
    "        #print(df.columns,ndf.columns,ms['on'])\n",
    "        mdf = pd.merge(df,ndf,on=on,how=ms.get('how','inner'))\n",
    "\n",
    "        for c in on: mdf[c] = mdf[c].astype(df[c].dtype)\n",
    "        if len(df) != len(mdf):\n",
    "            missing = set(list(df[on].drop_duplicates().itertuples(index=False,name=None))) - set(list(ndf[on].drop_duplicates().itertuples(index=False,name=None)))\n",
    "            warn(f\"Merge with {ms['file']} removes { 1-len(mdf)/len(df):.1%} rows with missing merges on: {missing}\")\n",
    "        df = mdf\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_and_process_data(desc, return_meta=False, constants={}, skip_postprocessing=False, **kwargs):\n",
    "\n",
    "    if isinstance(desc,str): desc = { 'file':desc } # Allow easy shorthand for simple cases\n",
    "\n",
    "    # Validate the data desc format\n",
    "    desc = DataDescription.model_validate(desc).model_dump(mode='json')\n",
    "\n",
    "    if desc.get('data') is not None:\n",
    "        df, meta, einfo = pd.DataFrame(data=desc['data']), None, {}\n",
    "    else:\n",
    "        df, meta, einfo = read_concatenate_files_list(desc, **kwargs)\n",
    "\n",
    "    if meta is None and return_meta:\n",
    "        raise Exception(\"No meta found on any of the files\")\n",
    "\n",
    "    # Perform transformation and filtering\n",
    "    globs = {'pd':pd, 'np':np, 'sp':sp, 'stk':stk, 'df':df, **einfo,**constants}\n",
    "    if desc.get('preprocessing'): exec(str_from_list(desc['preprocessing']), globs)\n",
    "\n",
    "    if desc.get('filter'): globs['df'] = globs['df'][eval(desc['filter'], globs)]\n",
    "    if desc.get('merge'): globs['df'] = perform_merges(globs['df'],desc.get('merge'),constants)\n",
    "    if desc.get('postprocessing') and not skip_postprocessing: exec(str_from_list(desc['postprocessing']),globs)\n",
    "    df = globs['df']\n",
    "\n",
    "    return (df, meta) if return_meta else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf_meta = {\n",
    "    'file': '../data/master_meta.json',\n",
    "    'preprocessing': \"df.age_group.replace({'16-24':'18-24'}, inplace=True)\",\n",
    "    'filter': '(df.citizen) & (df.age>=18) & (df.wave<5)',\n",
    "}\n",
    "\n",
    "sdf = read_and_process_data(dataf_meta)\n",
    "assert len(sdf) == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Small debug tool to help find where jsons become non-serializable\n",
    "def find_type_in_dict(d,dtype,path=''):\n",
    "    print(d,path)\n",
    "    if isinstance(d,dict):\n",
    "        for k,v in d.items():\n",
    "            find_type_in_dict(v,dtype,path+f'{k}:')\n",
    "    if isinstance(d,list):\n",
    "        for i,v in enumerate(d):\n",
    "            find_type_in_dict(v,dtype,path+f'[{i}]')\n",
    "    elif isinstance(d,dtype):\n",
    "        raise Exception(f\"Value {d} of type {dtype} found at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# These two very helpful functions are borrowed from https://towardsdatascience.com/saving-metadata-with-dataframes-71f51f558d8e\n",
    "\n",
    "custom_meta_key = 'salk-toolkit-meta'\n",
    "\n",
    "\n",
    "def write_parquet_with_metadata(df, meta, file_name):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    #find_type_in_dict(meta,np.int64)\n",
    "\n",
    "    custom_meta_json = json.dumps(meta)\n",
    "    existing_meta = table.schema.metadata\n",
    "    combined_meta = {\n",
    "        custom_meta_key.encode() : custom_meta_json.encode(),\n",
    "        **existing_meta\n",
    "    }\n",
    "    table = table.replace_schema_metadata(combined_meta)\n",
    "\n",
    "    pq.write_table(table, file_name, compression='ZSTD')\n",
    "\n",
    "# Just load the metadata from the parquet file\n",
    "\n",
    "\n",
    "def read_parquet_metadata(file_name):\n",
    "    schema = pq.read_schema(file_name)\n",
    "    if custom_meta_key.encode() in schema.metadata:\n",
    "        restored_meta_json = schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "    return restored_meta\n",
    "\n",
    "# Load parquet with metadata\n",
    "\n",
    "\n",
    "def read_parquet_with_metadata(file_name,lazy=False,**kwargs):\n",
    "    if lazy: # Load it as a polars lazy dataframe\n",
    "        meta = read_parquet_metadata(file_name)\n",
    "        ldf = pl.scan_parquet(file_name,**kwargs)\n",
    "        return ldf, meta\n",
    "\n",
    "    # Read it as a normal pandas dataframe\n",
    "    restored_table = pq.read_table(file_name,**kwargs)\n",
    "    restored_df = restored_table.to_pandas()\n",
    "    if custom_meta_key.encode() in restored_table.schema.metadata:\n",
    "        restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "\n",
    "    return restored_df, restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving and loading parquet with metadata\n",
    "sdf = pd.DataFrame([[1,2],[3,4]],columns=['x','y'])\n",
    "meta = { 'test': [1,{'x':2},[3]] }\n",
    "\n",
    "write_parquet_with_metadata(sdf,meta,'test.parquet')\n",
    "ndf, nmeta = read_parquet_with_metadata('test.parquet')\n",
    "\n",
    "assert nmeta == meta\n",
    "assert ndf.equals(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can add tests\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
