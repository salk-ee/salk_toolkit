{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "> Modular pieces for streamlit dashboards\n",
    "> All of this is meant to be run in a streamlit environment and is likely to fail elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 15:02:22.562 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-10 15:02:22.566 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-10 15:02:22.577 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-10 15:02:22.583 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-10 15:02:22.591 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-10 15:02:22.597 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "#| exporti\n",
    "import json, os, csv, re, time, psutil\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import altair as alt\n",
    "import s3fs, polib\n",
    "import __main__ # to get name of py file\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from salk_toolkit.utils import *\n",
    "from salk_toolkit.io import *\n",
    "from salk_toolkit.pp import e2e_plot\n",
    "\n",
    "import streamlit as st\n",
    "from streamlit_option_menu import option_menu\n",
    "from streamlit_dimensions import st_dimensions\n",
    "import streamlit_authenticator as stauth\n",
    "import libsql_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_plot_width(key):\n",
    "    wobj = st_dimensions(key=key) or { 'width': 800 } # Can return none so handle that\n",
    "    return min(800,int(0.8*wobj['width'])) # Needs to be adjusted down  to leave margin for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Open either a local or an s3 file\n",
    "def open_fn(fname, *args, s3_fs=None, **kwargs):\n",
    "    if fname[:3] == 's3:':\n",
    "        if s3_fs is None: s3_fs = s3fs.S3FileSystem(anon=False)\n",
    "        return s3_fs.open(fname,*args,**kwargs)\n",
    "    else:\n",
    "        return open(fname,*args,**kwargs)\n",
    "    \n",
    "def exists_fn(fname, *args, s3_fs=None, **kwargs):\n",
    "    if fname[:3] == 's3:':\n",
    "        if s3_fs is None: s3_fs = s3fs.S3FileSystem(anon=False)\n",
    "        return s3_fs.exists(fname,*args,**kwargs)\n",
    "    else:\n",
    "        return os.path.exists(fname,*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 15:02:22.637 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# ttl=None - never expire. Makes sense for potentially big data files\n",
    "@st.cache_resource(show_spinner=False,ttl=None)\n",
    "def read_annotated_data_lazy_cached(data_source,**kwargs):\n",
    "    print(f\"Reading lazy data from {data_source}\")\n",
    "    return read_annotated_data_lazy(data_source,**kwargs)\n",
    "\n",
    "# Load json uncached - useful for admin pages\n",
    "def load_json(fname, _s3_fs=None, **kwargs):\n",
    "    with open_fn(fname,'r',s3_fs=_s3_fs,encoding='utf8') as jf:\n",
    "        return json.load(jf)\n",
    "\n",
    "# This is cached very short term (1 minute) to avoid downloading it on every page change\n",
    "# while still allowing users to be added / changed relatively responsively\n",
    "@st.cache_data(show_spinner=False,ttl=60)\n",
    "def load_json_cached(fname, _s3_fs=None, **kwargs):\n",
    "    return load_json(fname,_s3_fs,**kwargs)\n",
    "\n",
    "# For saving json back \n",
    "def save_json(d, fname, _s3_fs=None, **kwargs):\n",
    "    with open_fn(fname,'w',s3_fs=_s3_fs,encoding='utf8') as jf:\n",
    "        json.dump(d,jf,indent=2,ensure_ascii=False)\n",
    "        \n",
    "def alias_file(fname, file_map):\n",
    "    if fname[:3]!='s3:' and fname in file_map and not os.path.exists(fname):\n",
    "        #print(f\"Redirecting {fname} to {file_map[fname]}\")\n",
    "        return file_map[fname]\n",
    "    else: return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def log_event(event, username, path, s3_fs=None):\n",
    "    timestamp = dt.datetime.now(dt.timezone.utc).strftime('%d-%m-%Y, %H:%M:%S')\n",
    "    with open_fn(path,'a',s3_fs=s3_fs) as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([timestamp, event, username])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# wrap the first parameter of streamlit function with self.translate\n",
    "# has to be a separate function instead of in a for loop for scoping reasons\n",
    "def wrap_st_with_translate(fn,self):\n",
    "    func = getattr(st,fn)\n",
    "    setattr(self, fn, lambda s, *args, **kwargs: func(self.tf(s),*args,**kwargs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def default_translate(s,**kwargs):\n",
    "    return (s[0].upper() + s[1:]).replace('_',' ') if isinstance(s,str) and len(s)>0 else s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def po_template_updater(pot_file = None):\n",
    "    if pot_file is None:\n",
    "        bname = os.path.splitext(os.path.basename(__main__.__file__))[0]\n",
    "        pot_file = f'locale/{bname}.pot'\n",
    "\n",
    "    if os.path.exists(pot_file):\n",
    "        po  = polib.pofile(pot_file)\n",
    "        td = { entry.msgid for entry in po }\n",
    "    else:\n",
    "        po = polib.POFile()\n",
    "        po.metadata = {\n",
    "            'Project-Id-Version': '1.0',\n",
    "            'Report-Msgid-Bugs-To': 'tarmo@salk.com',\n",
    "            'MIME-Version': '1.0',\n",
    "            'Content-Type': 'text/plain; charset=utf-8',\n",
    "            'Content-Transfer-Encoding': '8bit',\n",
    "        }\n",
    "        td = set()\n",
    "\n",
    "    def translate(s,**kwargs):\n",
    "        if isinstance(s,str) and s not in td:\n",
    "            po.append(polib.POEntry(msgid=s,msgstr=default_translate(s), \n",
    "                                    **{'msgctxt': kwargs.get('context'), 'comment': kwargs.get('comment')}))\n",
    "            po.save(pot_file)\n",
    "            td.add(s)\n",
    "        return s\n",
    "    \n",
    "    return translate\n",
    "\n",
    "def translate_fn_from_po(po_file):\n",
    "    po = polib.pofile(po_file)\n",
    "    td = { entry.msgid: entry.msgstr for entry in po }\n",
    "    return lambda s, **kwargs: td.get(s,s)\n",
    "\n",
    "def load_translate(translate):\n",
    "\n",
    "    if translate is None: return default_translate\n",
    "    elif callable(translate): return translate\n",
    "    elif isinstance(translate,dict): return lambda s, **kwargs: translate.get(s,s)\n",
    "    elif isinstance(translate,str):\n",
    "        if os.path.exists(translate):\n",
    "            ext = os.path.splitext(translate)[1]\n",
    "            if ext == '.po' or ext == '.pot':\n",
    "                return translate_fn_from_po(translate)\n",
    "            elif ext == '.json':\n",
    "                td = load_json_cached(translate)\n",
    "                return lambda s, **kwargs: td.get(s,s)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown translation file type: {ext}\")\n",
    "        elif len(translate)==2: # country code\n",
    "            bname = os.path.splitext(os.path.basename(__main__.__file__))[0]\n",
    "            return translate_fn_from_po(f'locale/{translate}/{bname}.po')\n",
    "        else:\n",
    "            raise ValueError(f\"Translation file not found: {translate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Main dashboard wrapper - WIP\n",
    "class SalkDashboardBuilder:\n",
    "\n",
    "    def __init__(self, data_source, auth_conf, logfile, groups=['guest','user','admin'], org_whitelist=None, public=False, translate=None):\n",
    "        \n",
    "        # Allow deployment.json to redirect files from local to s3 if local missing (i.e. in deployment scenario)\n",
    "        if os.path.exists('./deployment.json'):\n",
    "            dep_meta = load_json_cached('./deployment.json')\n",
    "            self.filemap = dep_meta.get('files',{})\n",
    "            #data_source = alias_file(data_source,self.filemap)\n",
    "            auth_conf = alias_file(auth_conf,self.filemap)\n",
    "        else: self.filemap = {}\n",
    "        \n",
    "        self.log_path = alias_file(logfile, self.filemap)\n",
    "        self.s3fs = s3fs.S3FileSystem(anon=False) # Initialize s3 access. Key in secrets.toml\n",
    "        self.data_source = data_source\n",
    "        self.public = public\n",
    "        self.pages = []\n",
    "        self.sb_info = st.sidebar.empty()\n",
    "        self.info = st.empty()\n",
    "\n",
    "        # Current page name\n",
    "        self.page_name = None\n",
    "        \n",
    "        # Set up translation\n",
    "        pot_updater = po_template_updater()\n",
    "        translate = load_translate(translate)\n",
    "        self.tf = lambda s,**kwargs: translate(pot_updater(s,**kwargs))\n",
    "        \n",
    "        self.p_widths = {}\n",
    "        \n",
    "        # Set up authentication\n",
    "        with st.spinner(self.tf(\"Setting up authentication...\",context='ui')):\n",
    "            self.uam = UserAuthenticationManager(auth_conf, groups, org_whitelist=org_whitelist,\n",
    "                                                s3_fs=self.s3fs, info=self.info, logger=self.log_event, \n",
    "                                                translate_func=lambda t: self.tf(t,context='ui'))\n",
    "\n",
    "        if not public:\n",
    "            self.uam.login_screen()\n",
    "            \n",
    "        # Wrap some streamlit functions with translate\n",
    "        wrap_list = ['write','markdown','title','header','subheader','caption','text','divider','tabs',\n",
    "                     'button','download_button','link_button','checkbox','toggle','radio','selectbox',\n",
    "                     'multiselect','slider','select_slider','text_input','number_input','text_area',\n",
    "                     'date_input','time_input','file_uploader','camera_input','color_picker', 'popover']\n",
    "        for fn in wrap_list:\n",
    "            if hasattr(st,fn): wrap_st_with_translate(fn,self)\n",
    "\n",
    "    # Get the pandas dataframe with given columns\n",
    "    def get_df(self,columns=None):\n",
    "        if columns is None: q = self.ldf\n",
    "        else: q = self.ldf.select(columns)\n",
    "        return fix_df_with_meta(q.collect().to_pandas(),self.meta)\n",
    "    \n",
    "    # For backwards compatibility - this is very inefficient\n",
    "    @property\n",
    "    def df(self):\n",
    "        warn(\"sdb.df is very inefficient. Use sdb.get_df([columns]) instead to get only the columns you need\")\n",
    "        return self.get_df()\n",
    "\n",
    "    @property\n",
    "    def user(self):\n",
    "        return self.uam.user    \n",
    "    \n",
    "    def log_event(self, event, username=None):\n",
    "        log_event(event, username or st.session_state['username'], self.log_path, s3_fs=self.s3fs)\n",
    "\n",
    "    # pos_id is for plot_width to work in columns\n",
    "    def plot(self, pp_desc, pos_id='main', width=None, **kwargs):\n",
    "        if width is None: # Find or reuse auto-width\n",
    "            width = self.p_widths[pos_id] if pos_id in self.p_widths else get_plot_width(pos_id)\n",
    "            self.p_widths[pos_id] = width\n",
    "        \n",
    "        # Draw plot\n",
    "        st_plot(pp_desc,\n",
    "                width=width, translate=lambda s: self.tf(s,context='data'),\n",
    "                full_df=self.ldf,data_meta=self.meta,**kwargs)\n",
    "        \n",
    "    def filter_ui(self, dims, detailed=False, raw=False, force_choice=False, key=''):\n",
    "        return filter_ui(self.ldf, self.meta, uid=f'{key}_{self.page_name}', dims=dims, detailed=detailed, raw=raw, translate=self.tf, force_choice=force_choice)\n",
    "    \n",
    "    def facet_ui(self, dims, two=False, raw=False, force_choice=False, label='Facet', key=''):\n",
    "        return facet_ui(dims, two=two, raw=raw, uid=f'{key}_{self.page_name}', translate=self.tf,force_choice=force_choice,label=label)\n",
    "\n",
    "    def page(self, name, **kwargs):\n",
    "        def decorator(pfunc):\n",
    "            groups = kwargs.get('groups')\n",
    "            if (groups is None or # Page is available to all\n",
    "                self.user.get('group')=='admin' or # Admin sees all\n",
    "                self.user.get('group','guests') in groups): # group is whitelisted\n",
    "                self.pages.append( (name,pfunc,kwargs) )\n",
    "        return decorator\n",
    "\n",
    "    def build(self):\n",
    "        # This is to avoid a bug of the option menu not showing up on reload\n",
    "        # I don't get how this row fixes the issue, but it does\n",
    "        #https://github.com/victoryhb/streamlit-option-menu/issues/68\n",
    "        if st.session_state[\"authentication_status\"] and st.session_state[\"logout\"] is None:\n",
    "            st.session_state[\"logout\"] = True \n",
    "            st.rerun()\n",
    "\n",
    "        # If login failed and is required, don't go any further\n",
    "        if not self.public and not st.session_state[\"authentication_status\"]: return\n",
    "    \n",
    "        # Add user settings page if logged in\n",
    "        if self.user:  self.pages.append( ('Settings',user_settings_page,{'icon': 'sliders'}) )\n",
    "        \n",
    "        # Add admin page for admins\n",
    "        if self.user.get('group')=='admin':  self.pages.append( ('Administration', admin_page,{'icon': 'terminal'}) )\n",
    "        \n",
    "        # Draw the menu listing pages\n",
    "        pnames = [t[0] for t in self.pages]\n",
    "        with st.sidebar:\n",
    "\n",
    "            if self.user:\n",
    "                self.sb_info.info(self.tf('Logged in as **%s**',context='ui') % self.user[\"name\"])\n",
    "                self.uam.auth.logout(self.tf('Log out',context='ui'), 'sidebar')\n",
    "            \n",
    "            t_pnames = [ self.tf(pn,context='ui') for pn in pnames]\n",
    "            menu_choice = option_menu(\"Pages\",\n",
    "                t_pnames,\n",
    "                icons=[t[2].get('icon') for t in self.pages],\n",
    "                styles={\n",
    "                    \"container\": {\"padding\": \"5!important\"}, #, \"background-color\": \"#fafafa\"},\n",
    "                    #\"icon\": {\"color\": \"red\", \"font-size\": \"15px\"},\n",
    "                    \"nav-link\": {\"font-size\": \"12px\", \"text-align\": \"left\", \"margin\":\"0px\", \"--hover-color\": \"#eee\"},\n",
    "                    \"nav-link-selected\": {\"background-color\": \"#red\"},\n",
    "                    \"menu-title\": {\"display\":\"none\"}\n",
    "                })\n",
    "            \n",
    "        # Find the page\n",
    "        pname, pfunc, meta = self.pages[t_pnames.index(menu_choice)]\n",
    "        self.page_name = pname\n",
    "        \n",
    "        # Load data\n",
    "        self.data_source = meta.get('data_source',self.data_source)\n",
    "        with st.spinner(self.tf(\"Loading data...\",context='ui')):\n",
    "\n",
    "            # Download the data if it's not already locally present\n",
    "            # This is done because lazy loading over s3 is very painfully slow as data files are big\n",
    "            if not os.path.exists(self.data_source):\n",
    "                print(f'Downloading {self.filemap[self.data_source]} to {self.data_source}')\n",
    "                self.s3fs.download(self.filemap[self.data_source],self.data_source)\n",
    "\n",
    "            self.ldf, self.meta = read_annotated_data_lazy_cached(self.data_source)\n",
    "            #self.df = self.ldf.collect().to_pandas() # Backwards compatibility\n",
    "        \n",
    "        # Render the chosen page\n",
    "        self.subheader(pname)\n",
    "        pfunc(**clean_kwargs(pfunc,{'sdb':self}))\n",
    "\n",
    "        if self.user.get('group')=='admin':\n",
    "            st.sidebar.write(\"Mem: %.1fMb\" % (psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2))\n",
    "        \n",
    "    # Add enter and exit so it can be used as a context\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    # Render everything once we exit the with block\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stauth.Hasher(['kalasaba']).generate() # To generate passwords hashes manually if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "# TODO\n",
    "# - centralize the db connection, getting url and token from env\n",
    "# - move other conf (cookie token) to streamlit env variables\n",
    "# - create a database with username as id and migrate the auth_conf on the web\n",
    "\n",
    "@st.cache_resource\n",
    "def sqlite_client(url, token):\n",
    "    print(f\"User database from {url}\")\n",
    "    return libsql_client.create_client_sync(url=url, auth_token=token)\n",
    "\n",
    "class UserAuthenticationManager():\n",
    "    \n",
    "    def __init__(self,auth_conf_file,groups,org_whitelist,s3_fs,info,logger,translate_func):\n",
    "        self.groups, self.s3fs, self.info  = groups, s3_fs, info\n",
    "        self.org_whitelist = org_whitelist\n",
    "\n",
    "\n",
    "        self.client = None\n",
    "        self.conf_file = auth_conf_file\n",
    "        self.load_conf()\n",
    "        config = self.conf \n",
    "        \n",
    "        self.auth = stauth.Authenticate(\n",
    "            config['credentials'],\n",
    "            config['cookie']['name'],\n",
    "            config['cookie']['key'],\n",
    "            config['cookie']['expiry_days'],\n",
    "            [] # config['preauthorized'] - not using preauthorization\n",
    "        )\n",
    "        self.user = {} # Filled on login\n",
    "        self.log_event = logger\n",
    "        self.tf = translate_func\n",
    "\n",
    "        # Mark that we should log the next login\n",
    "        if 'log_event' not in st.session_state: st.session_state['log_event'] = True\n",
    "\n",
    "                          \n",
    "    def require_admin(self):\n",
    "        if not self.admin: raise Exception(\"This action requires administrator privileges\")\n",
    "    \n",
    "    def load_conf(self,cached=True):\n",
    "        if cached: self.conf = load_json_cached(self.conf_file, _s3_fs = self.s3fs)\n",
    "        else: self.conf = load_json(self.conf_file, _s3_fs = self.s3fs)\n",
    "\n",
    "        if 'libsql' in self.conf:\n",
    "            url, token = self.conf['libsql']['url'], self.conf['libsql']['token']\n",
    "            self.client = sqlite_client(url=url, token=token)\n",
    "            ures = self.client.execute(\"SELECT * FROM users\")\n",
    "            self.conf['credentials']['usernames'] = { u['username']:dict(zip(ures.columns,u)) for u in ures.rows }\n",
    "        \n",
    "        if self.org_whitelist is not None:\n",
    "            self.conf['credentials']['usernames'] = {\n",
    "                un:ud for un,ud in self.conf['credentials']['usernames'].items()\n",
    "                if ud.get('group') == 'admin' or # Admins have access to everything\n",
    "                    ud.get('organization') in self.org_whitelist\n",
    "            }\n",
    "            \n",
    "        self.users = self.conf['credentials']['usernames']\n",
    "    \n",
    "    def login_screen(self):\n",
    "        tf = self.tf\n",
    "        _, _, username = self.auth.login('sidebar', fields={'Form name':tf('Login page'), 'Username':tf('Username'), 'Password':tf('Password'), 'Log in':tf('Log in')})\n",
    "        \n",
    "        if st.session_state[\"authentication_status\"] is False:\n",
    "            st.error(tf('Username/password is incorrect'))\n",
    "            self.log_event('login-fail', username=username)\n",
    "        if st.session_state[\"authentication_status\"] is None:\n",
    "            st.warning(tf('Please enter your username and password'))\n",
    "            st.session_state['log_event'] = True \n",
    "        elif st.session_state[\"authentication_status\"]:\n",
    "            self.user = {'name': st.session_state['name'], \n",
    "                         'username': username,\n",
    "                         **self.users[username] }\n",
    "            \n",
    "            #check if signing in has been logged - if not, log it and flip the flag\n",
    "            if st.session_state['log_event']:\n",
    "                self.log_event('login-success')\n",
    "                st.session_state['log_event'] = False\n",
    "        \n",
    "        self.admin = (self.user.get('group') == 'admin')\n",
    "        \n",
    "    def update_conf(self):\n",
    "        with open_fn(self.conf_file,'w',s3_fs=self.s3fs) as jf:\n",
    "            json.dump(self.conf,jf)\n",
    "        time.sleep(3) # Give some time for messages to display etc\n",
    "        st.rerun() # Force a rerun to reload the new file\n",
    "\n",
    "    def update_user(self,username):\n",
    "        if 'libsql' in self.conf:\n",
    "            user_data = self.users[username]\n",
    "            self.client.execute('UPDATE users SET name = ?, email = ?, organization = ?, \"group\" = ?, password = ? WHERE username = ?',\n",
    "                         [user_data['name'], user_data['email'], user_data['organization'], \n",
    "                          user_data['group'], user_data['password'], username])\n",
    "        else: self.update_conf()\n",
    "            \n",
    "    def add_user(self, username, password, user_data):\n",
    "        self.require_admin()\n",
    "        if username not in self.users:\n",
    "            user_data['password'] = stauth.Hasher([password]).generate()[0]\n",
    "            self.users[username] = user_data\n",
    "            self.info.success(f'User {username} successfully added.')\n",
    "            self.log_event(f'add-user: {username}')\n",
    "            \n",
    "            if 'libsql' in self.conf:\n",
    "                self.client.execute('INSERT INTO users (username, name, email, organization, \"group\", password) VALUES (?, ?, ?, ?, ?, ?)',\n",
    "                               [username, user_data['name'], user_data['email'], user_data['organization'], \n",
    "                                user_data['group'], user_data['password']])\n",
    "            else: self.update_conf()\n",
    "            return True\n",
    "        else:\n",
    "            self.info.error(f'User **{username}** already exists.')\n",
    "            return False\n",
    "        \n",
    "    def change_user(self, username, user_data):\n",
    "        \n",
    "        # Change username\n",
    "        if 'username' in user_data and username != user_data['username']:\n",
    "            self.users[user_data['username']] = self.users[username]\n",
    "            del self.users[username]\n",
    "            username = user_data['username']\n",
    "            del user_data['username']\n",
    "        \n",
    "        # Handle password change\n",
    "        if user_data.get('password'):\n",
    "            user_data['password'] = stauth.Hasher([user_data['password']]).generate()[0]\n",
    "        else: user_data['password'] = self.users[username]['password']\n",
    "        \n",
    "        # Update everything else\n",
    "        self.users[username].update(user_data)\n",
    "        self.log_event(f'change-user: {username}')\n",
    "        self.info.success(f'User **{username}** changed.')\n",
    "        self.update_user(username)\n",
    "        \n",
    "    def delete_user(self, username): \n",
    "        self.require_admin()\n",
    "        del self.users[username]\n",
    "        self.info.warning(f'User **{username}** deleted.')\n",
    "        self.log_event(f'delete-user: {username}')\n",
    "\n",
    "        if 'libsql' in self.conf:\n",
    "            self.client.execute('DELETE FROM users WHERE username = ?', [username])\n",
    "        else:\n",
    "            self.update_conf()\n",
    "\n",
    "    def list_users(self):\n",
    "        self.require_admin()\n",
    "        return [ censor_dict({'username': k, **v},['password']) for k,v in self.users.items() ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "\n",
    "# Password reset\n",
    "def user_settings_page(sdb):\n",
    "    if not sdb.user: return\n",
    "    try:\n",
    "        tf = lambda s: sdb.tf(s,context='ui')\n",
    "        if sdb.uam.auth.reset_password(st.session_state[\"username\"], \n",
    "                                       fields={'Form name':tf('Reset password'), 'Current password':tf('Current password'), \n",
    "                                               'New password':tf('New password'), 'Repeat password': tf('Repeat password'), \n",
    "                                               'Reset':tf('Reset')}):\n",
    "            sdb.uam.update_user(st.session_state[\"username\"])\n",
    "            st.success(tf('Password modified successfully'))\n",
    "    except Exception as e:\n",
    "        st.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Helper function to highlight log rows\n",
    "def highlight_cells(val):\n",
    "    if 'fail' in val:\n",
    "        color = 'red'\n",
    "    #elif 'add' in val:\n",
    "    elif any(s in val for s in ['delete', 'add', 'change']):\n",
    "        color = 'blue'\n",
    "    elif 'success' in val:\n",
    "        color='green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return 'color: {}'.format(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti \n",
    "\n",
    "# Admin page to manage users\n",
    "\n",
    "def admin_page(sdb):\n",
    "    sdb.uam.require_admin()\n",
    "    sdb.uam.load_conf(cached=False) # so all admin updates would immediately be visible\n",
    "    \n",
    "    menu_choice = option_menu(None,[ 'Log management', 'List users', 'Add user', 'Change user', 'Delete user' ], \n",
    "                              icons=['card-list','people-fill','person-fill-add','person-lines-fill','person-fill-dash'], orientation='horizontal')\n",
    "    st.write(\" \")\n",
    "\n",
    "    if menu_choice=='Log management':\n",
    "        log_data=pd.read_csv(alias_file(sdb.log_path,sdb.filemap),names=['timestamp','event','username'])\n",
    "        st.dataframe(log_data.sort_index(ascending=False\n",
    "            ).style.map(highlight_cells, subset=['event']), width=1200) #use_container_width=True\n",
    "        \n",
    "    elif menu_choice=='List users':\n",
    "        # Read log to get last login:\n",
    "        log_data = pd.read_csv(alias_file(sdb.log_path,sdb.filemap),names=['timestamp','event','username'])\n",
    "        log_data = log_data[log_data['event']=='login-success']\n",
    "        log_data['timestamp'] = pd.to_datetime(log_data['timestamp'], utc=True, format='%d-%m-%Y, %H:%M:%S')\n",
    "        \n",
    "        # Add last login to users\n",
    "        users = sdb.uam.list_users()\n",
    "        for u in users:\n",
    "            last_login = log_data[log_data['username'] == u['username']].timestamp.max()\n",
    "            if pd.notnull(last_login):\n",
    "                u['last_login'] = last_login.strftime('%d-%b-%Y')\n",
    "                \n",
    "        # Display the data\n",
    "        st.dataframe(users, use_container_width=True)\n",
    "\n",
    "    elif menu_choice=='Add user':\n",
    "        with st.form(\"add_user_form\"):\n",
    "            st.subheader(\"Add user:\")\n",
    "            st.markdown(\"\"\"---\"\"\")\n",
    "            col1,col2 = st.columns((1,2))\n",
    "            user_data = {}\n",
    "            with col1:\n",
    "                user_data['group'] = st.radio(\"Group:\", sdb.uam.groups)\n",
    "            with col2:\n",
    "                username = st.text_input(\"Username:\")\n",
    "                password = st.text_input(\"Password:\", type='password')\n",
    "                user_data['name'] = st.text_input(\"Name:\")\n",
    "                st.markdown(\"\"\"---\"\"\")\n",
    "                user_data['email'] = st.text_input(\"E-mail:\")\n",
    "                user_data['organization'] = st.text_input(\"Organization:\")\n",
    "            st.markdown(\"\"\"---\"\"\")\n",
    "            submitted = st.form_submit_button(\"Submit\")\n",
    "            if submitted:\n",
    "                if not '' in [username, password, user_data['email']]:\n",
    "                    sdb.uam.add_user(username, password, user_data)\n",
    "                else:\n",
    "                    sdb.info.error('Must specify username, password and email.')\n",
    "\n",
    "    elif menu_choice=='Change user':\n",
    "        username=st.selectbox('Edit user', list(sdb.uam.users.keys()))\n",
    "        \n",
    "        user_data = sdb.uam.users[username].copy()\n",
    "        #st.write(user_data)\n",
    "        group_index = sdb.uam.groups.index(user_data['group'])\n",
    "\n",
    "        with st.form(\"edit_user_form\"):\n",
    "            st.subheader(\"Edit user data:\")\n",
    "            st.markdown(\"\"\"---\"\"\")\n",
    "            col1,col2 = st.columns((1,2))\n",
    "            with col1:\n",
    "                user_data['username'] = st.text_input(\"Username:\", value=username, disabled=True)\n",
    "                user_data['group'] = st.radio(\"Group:\", sdb.uam.groups, index=group_index) #, disabled=True)\n",
    "            with col2:\n",
    "                #new_user = st.text_input(\"Kasutaja:\", value=username, disabled=True)\n",
    "                user_data['name'] = st.text_input(\"Name:\", value=user_data['name'])\n",
    "                user_data['password'] = st.text_input(\"Password:\", type='password')\n",
    "                st.markdown(\"\"\"---\"\"\")\n",
    "                user_data['email'] = st.text_input(\"E-mail:\", value=user_data['email'])\n",
    "                user_data['organization'] = st.text_input(\"Organization:\", value=user_data.get('organization',''))\n",
    "                \n",
    "            st.markdown(\"\"\"---\"\"\")\n",
    "            submitted = st.form_submit_button(\"Submit\")\n",
    "            if submitted:\n",
    "                sdb.uam.change_user(username,user_data)\n",
    "                \n",
    "    elif menu_choice=='Delete user':\n",
    "        with st.form(\"delete_user_form\"):\n",
    "            st.subheader('Delete user:')\n",
    "            username = st.selectbox('Select username:', list(sdb.uam.users.keys()))\n",
    "            check = st.checkbox('Deletion is FINAL and cannot be undone!')\n",
    "            st.markdown(\"\"\"___\"\"\")\n",
    "            submitted = st.form_submit_button(\"Delete\")\n",
    "            if submitted:\n",
    "                if not check:\n",
    "                    sdb.info.warning(f'Tick the checkbox in order to delete user **{username}**.')\n",
    "                elif username == sdb.user['username']:\n",
    "                    sdb.info.error('Cannot delete the current user.')\n",
    "                else:\n",
    "                    sdb.uam.delete_user(username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snippets copied over from dashboard.py to be re-purposed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - copy the timestamp logic here over to our list_users\n",
    "def list_users():\n",
    "    with fs.open(user_file, 'r') as f:\n",
    "        data=f.read()\n",
    "        u_dict=json.loads(data)\n",
    "        f.close()\n",
    "\n",
    "    log_data=pd.read_csv(log_file)\n",
    "    log_data['timestamp']= pd.to_datetime(log_data['timestamp'],\n",
    "        utc=True, format='%d-%m-%Y, %H:%M:%S')\n",
    "\n",
    "    rows = []\n",
    "    for u in u_dict.keys():\n",
    "        rv = u_dict[u].copy()\n",
    "        last_login = log_data[log_data.user == u].timestamp.max()\n",
    "        if pd.notnull(last_login):\n",
    "            rv['last login'] = last_login.strftime('%d-%b-%Y')\n",
    "        rows.append(rv)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other shared parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This is a horrible workaround to get faceting to work with altair geoplots that do not play well with streamlit\n",
    "# See https://github.com/altair-viz/altair/issues/2369 -> https://github.com/vega/vega-lite/issues/3729\n",
    "\n",
    "# Draw a matrix of plots using separate plots and st columns\n",
    "def draw_plot_matrix(pmat):\n",
    "    if not pmat: return # Do nothing if get None passed to it\n",
    "    if not isinstance(pmat,list): pmat, ucw = [[pmat]], False\n",
    "    else: ucw = True # If we are drawing more than one plot, we want to use the container width\n",
    "    cols = st.columns(len(pmat[0])) if len(pmat[0])>1 else [st]\n",
    "    for j,c in enumerate(cols):\n",
    "        for i, row in enumerate(pmat):\n",
    "            if j>=len(pmat[i]): continue\n",
    "            c.altair_chart(pmat[i][j],use_container_width=ucw)\n",
    "\n",
    "# Draw the plot described by pp_desc \n",
    "def st_plot(pp_desc,**kwargs):\n",
    "    plots = e2e_plot(pp_desc, **kwargs)\n",
    "    draw_plot_matrix(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3581481551.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    template = '''<!DOCTYPE html>\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# TODO: currently only works for a single plot, not matrix\n",
    "def plot_matrix_html(pmat, uid='viz', responsive=True):\n",
    "    if not pmat: return\n",
    "    if not isinstance(pmat,list): pmat, ucw = [[pmat]], False\n",
    "\n",
    "    template = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head></head>\n",
    "<body>\n",
    "  <div id=\"%s\"></div>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega@5\"></script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@5\"></script>\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"></script>\n",
    "  <script type=\"text/javascript\">\n",
    "    function draw_plot() {\n",
    "    width = document.getElementById(\"%s\").parentElement.clientWidth;\n",
    "    var spec = %s\n",
    "    var opt = {\"renderer\": \"svg\", \"actions\": false};\n",
    "    vegaEmbed(\"#%s\", spec, opt);\n",
    "    };\n",
    "    draw_plot();\n",
    "    %s\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "    pdict = json.loads(pmat[0][0].to_json())\n",
    "    pdict['autosize'] = {'type': 'fit', 'contains': 'padding'}\n",
    "\n",
    "    if responsive: \n",
    "        pdict['width'] = 'XYZresponsiveXZY'; # Something we can replace easy\n",
    "        resp_code = 'window.addEventListener(\"resize\", draw_plot);'\n",
    "    else: resp_code = '';\n",
    "\n",
    "    html = template % (uid,uid,json.dumps(pdict, indent=2),uid,resp_code)\n",
    "\n",
    "    if responsive: html = html.replace('\"XYZresponsiveXZY\"', 'width')\n",
    "    return html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Streamlit session state safety - check and clear session state if it has an unfit value\n",
    "def stss_safety(key, opts):\n",
    "    if key in st.session_state and st.session_state[key] not in opts: del st.session_state[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def facet_ui(dims, two=False, uid='base',raw=False, translate=None, force_choice=False, label='Facet'):\n",
    "    # Set up translation\n",
    "    tfc = translate if translate else (lambda s,**kwargs: s)\n",
    "    tf = lambda s: tfc(s,context='data')\n",
    "    \n",
    "    tdims = [ tf(d) for d in dims ]\n",
    "    r_map = dict(zip(tdims,dims))\n",
    "    \n",
    "    none = tf('None')\n",
    "    stc = st.sidebar if not raw else st\n",
    "\n",
    "    stss_safety(f'facet1_{uid}',tdims)\n",
    "    facet_dim = stc.selectbox(tfc(label+':',context='ui'), tdims if force_choice else [none] + tdims, key=f'facet1_{uid}')\n",
    "    fcols = [facet_dim] if facet_dim != none else []\n",
    "    if two and facet_dim != none:\n",
    "        stss_safety(f'facet2_{uid}',tdims)\n",
    "        second_dim = stc.selectbox(tfc(label+' 2:',context='ui'), tdims if force_choice else [none] + tdims, key=f'facet2_{uid}')\n",
    "        if second_dim not in [none,facet_dim]:  fcols = [facet_dim, second_dim]\n",
    "        \n",
    "    return [ r_map[d] for d in fcols ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Function that creates reset functions for multiselects in filter\n",
    "def ms_reset(cn, all_vals):\n",
    "    def reset_ms():\n",
    "        st.session_state[f\"{cn}_multiselect\"] = all_vals\n",
    "    return reset_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 15:02:22.822 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "#| exporti\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def get_filter_limits(_ldf,dims,dmeta,uid):\n",
    "    ldf = _ldf\n",
    "\n",
    "    if not isinstance(ldf,pl.LazyFrame): ldf = pl.DataFrame(ldf).lazy()\n",
    "\n",
    "    schema = ldf.collect_schema()\n",
    "\n",
    "    if dims is None: dims = schema.names()\n",
    "    else: dims = [ c for c in dims if c in schema.names() ]\n",
    "\n",
    "    c_meta = extract_column_meta(dmeta)\n",
    "\n",
    "    limits = {}\n",
    "    for d in dims:\n",
    "        if c_meta[d].get('continuous') and schema[d].is_numeric():\n",
    "            if c_meta[d].get('val_range'): limits[d] = { 'min': c_meta[d]['val_range'][0], 'max': c_meta[d]['val_range'][1] }\n",
    "            else: limits[d] = ldf.select([pl.min(d).alias('min'),pl.max(d).alias('max')]).collect().to_dicts()[0]\n",
    "            limits[d]['continuous'] = True\n",
    "        elif c_meta[d].get('categories'):\n",
    "            if c_meta[d].get('categories') == 'infer':\n",
    "                if schema[d].is_numeric():\n",
    "                    warn(f'Column {d} is numeric but marked as categorical. Skipping in filter as inferring categories is not possible.')\n",
    "                    continue\n",
    "                else:\n",
    "                    limits[d] = { 'categories': ldf.select(pl.all()).unique(d).collect().to_series().sort().to_list() }\n",
    "            else:\n",
    "                limits[d] = { 'categories': c_meta[d]['categories'] } \n",
    "                \n",
    "            limits[d]['ordered'] = c_meta[d].get('ordered',False)\n",
    "            \n",
    "        else:\n",
    "            warn(f\"Skipping {d}: {c_meta[d]} in filter\")\n",
    "    return limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# User interface that outputs a filter for the pp_desc\n",
    "def filter_ui(data, dmeta=None, dims=None, uid='base', detailed=False, raw=False, translate=None, force_choice=False):\n",
    "    \n",
    "    tfc = translate if translate else (lambda s,**kwargs: s)\n",
    "    tf = lambda s: tfc(s,context='data')\n",
    "\n",
    "    limits = get_filter_limits(data,dims,dmeta,uid)\n",
    "    dims = list(limits.keys())\n",
    "    \n",
    "    if dmeta is not None:\n",
    "        dims = list_aliases(dims, group_columns_dict(dmeta)) # Replace aliases like 'demographics'\n",
    "        c_meta = extract_column_meta(dmeta) # mainly for groups defined in meta\n",
    "    else: c_meta = defaultdict(lambda: {})\n",
    "    \n",
    "    if not force_choice: f_info = st.sidebar.container()\n",
    "    \n",
    "    stc = st.sidebar.expander(tfc('Filters',context='ui')) if not raw else st\n",
    "    \n",
    "    # Different selector for different category types\n",
    "    # Also - make sure filter is clean and only applies when it is changed from the default 'all' value\n",
    "    # This has considerable speed and efficiency implications\n",
    "    filters = {}\n",
    "    for cn in dims:\n",
    "        \n",
    "        # Shared prep for all cateogoricals\n",
    "        if limits[cn].get('categories'):\n",
    "            cats = limits[cn]['categories']\n",
    "            if len(cats)==1: continue\n",
    "            \n",
    "            # Do some prep for translations\n",
    "            r_map = dict(zip([tf(c) for c in cats],cats))\n",
    "            all_vals = list(r_map.keys()) # translated categories\n",
    "            grp_names = c_meta[cn].get('groups',{}).keys()\n",
    "            r_map.update(dict(zip([tf(c) for c in grp_names],grp_names)))\n",
    "        \n",
    "        # Multiselect\n",
    "        if detailed and limits[cn].get('categories'): \n",
    "            filters[cn] = stc.multiselect(tf(cn), all_vals, all_vals, key=f\"filter_{uid}_{cn}_multiselect\")\n",
    "            if set(filters[cn]) == set(all_vals): del filters[cn]\n",
    "            else: \n",
    "                stc.button(tf(\"Reset\"),key=f\"filter_{uid}_{cn}_ms_reset\",on_click=ms_reset(cn,all_vals))\n",
    "                filters[cn] = [ r_map[c] for c in filters[cn] ]\n",
    "\n",
    "        # Unordered categorical - selectbox\n",
    "        elif limits[cn].get('categories') and not limits[cn].get('ordered'): \n",
    "            choices = [gt for gt,g in r_map.items() if g in grp_names] + all_vals\n",
    "            if not force_choice: choices = [tf('All')] + choices\n",
    "            stss_safety(f'filter_{cn}_sel',choices)\n",
    "            filters[cn] = stc.selectbox(tf(cn),choices,key=f'filter_{uid}_{cn}_sel')\n",
    "            if filters[cn] == tf('All'): del filters[cn]\n",
    "            else: filters[cn] = r_map[filters[cn]]\n",
    "\n",
    "        # Ordered categorical - slider\n",
    "        # Use [None,<start>,<end>] for ranges, both categorical and continuous to distinguish them from list of values\n",
    "        elif limits[cn].get('categories') and limits[cn].get('ordered'): # Ordered categorical - slider\n",
    "            f_res = stc.select_slider(tf(cn),all_vals,value=(all_vals[0],all_vals[-1]),key=f'filter_{uid}_{cn}_ocat')\n",
    "            if f_res != (all_vals[0],all_vals[-1]): \n",
    "                filters[cn] = [None]+[r_map[f_res[0]],r_map[f_res[1]]]\n",
    "\n",
    "        # Numeric values - slider\n",
    "        elif limits[cn].get('continuous'): # Continuous\n",
    "            mima = limits[cn]['min'], limits[cn]['max']\n",
    "            if mima[0]==mima[1]: continue\n",
    "            f_res = stc.slider(tf(cn),*mima,value=mima,key=f'filter_{uid}_{cn}_cont')\n",
    "            if f_res[0]>mima[0] or f_res[1]<mima[1]: \n",
    "                filters[cn] = ( [None] + \n",
    "                                [ f_res[0] if f_res[0]>mima[0] else None] + \n",
    "                                [ f_res[1] if f_res[1]<mima[1] else None ] )\n",
    "            \n",
    "    if filters and not force_choice: f_info.warning('⚠️ ' + tfc('Filters active',context='ui') + ' ⚠️')\n",
    "            \n",
    "    return filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Use dict here as dicts are ordered as of Python 3.7 and preserving order groups things together better\n",
    "\n",
    "def translate_with_dict(d):\n",
    "    return (lambda s: d[s] if isinstance(s,str) and s in d and d[s] is not None else s)\n",
    "\n",
    "def log_missing_translations(tf, nonchanged_dict):\n",
    "    def ntf(s):\n",
    "        ns = tf(s)\n",
    "        if ns==s: nonchanged_dict[s]=None\n",
    "        return ns\n",
    "    return ntf\n",
    "\n",
    "def clean_missing_translations(nonchanged_dict, tdict={}):\n",
    "    # Filter out numbers that come in from data sometimes\n",
    "    return { s:v for s,v in nonchanged_dict.items() if s not in tdict and isinstance(s,str) and not re.fullmatch(r'[.\\d]+',s) }\n",
    "\n",
    "def add_missing_to_dict(missing_dict, tdict):\n",
    "    return {**tdict, **{ s:s for s in missing_dict}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup guide\n",
    "- User conf\n",
    "  - cookie key matters. generate a decent one random\n",
    "- Logfile - make sure to touch a local one so local logs don't pollute the deploy\n",
    "- Files: if deploy.json targets missing, notify. If files not present in s3, copy over. Add flag to have them updated\n",
    "- Translations: keep in repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
