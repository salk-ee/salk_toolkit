{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "> Builds up to a function that validates metadata annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "import numpy as np\n",
    "from typing import *\n",
    "from pydantic import BaseModel, ConfigDict, model_validator, BeforeValidator\n",
    "from pydantic_extra_types.color import Color\n",
    "from salk_toolkit.utils import replace_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Meta (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Define a new base that is more strict towards unknown inputs\n",
    "class PBase(BaseModel):\n",
    "    model_config = ConfigDict(extra='forbid', protected_namespaces=(),arbitrary_types_allowed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class ColumnMeta(PBase):\n",
    "\n",
    "    # Type specification\n",
    "    continuous: bool = False # For real numbers\n",
    "    datetime: bool = False # For datetimes\n",
    "    categories: Optional[Union[List,Literal['infer']]] = None # For categoricals: List of categories or 'infer'\n",
    "    ordered: bool = False # If categorical data is ordered\n",
    "\n",
    "    # Transformations\n",
    "    translate: Optional[Dict] = None # Translate dict applied to categories\n",
    "    transform: Optional[str] = None # Transform function in python code, applied after translate\n",
    "    translate_after: Optional[Dict] = None # Same as translate, but applied after transform\n",
    "\n",
    "    # Model extras\n",
    "    modifiers: Optional[List[str]] = None # List of columns that are meant to modify the responses on this col -> private_inputs\n",
    "    nonordered: Optional[List] = None # List of categories that are outside the order (Like \"Don't know\") -> nonordered in ordered_outputs\n",
    "\n",
    "    # Plot pipeline extras\n",
    "    label: Optional[str] = None # Longer description of the column for tooltips\n",
    "    labels: Optional[Dict[str,str]] = None # Dict matching categories to labels\n",
    "    groups: Optional[Dict[str,List[str]]] = None # Dict of lists of category values defining groups for easier filtering\n",
    "    colors: Optional[Dict[str,Color]] = None # Dict matching colors to categories\n",
    "    num_values: Optional[List[Union[float,None]]] = None # For categoricals - how to convert the categories to numbers\n",
    "    val_format: Optional[str] = None # Format string for the column values - only used with continuous display\n",
    "    val_range: Optional[Tuple[float,float]] = None # Range of possible values for continuous variables - used for filter bounds etc\n",
    "\n",
    "    likert: bool = False # For ordered categoricals - if they are likert-type (i.e. symmetric around center)\n",
    "    neutral_middle: Optional[str] = None # For ordered categoricals - if there is a neutral category, which one should be in the middle?\n",
    "\n",
    "    topo_feature: Optional[Tuple[str,str,str]] = None # Link to a geojson/topojson [url,type,col_name inside geodata]\n",
    "    electoral_system: Optional[Dict] = None # Information about electoral system (TODO: spec it out)\n",
    "    mandates: Optional[Dict] = None # Mandate count mapping for the electoral system\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def check_categorical(self) -> Self:\n",
    "        if self.categories is None:\n",
    "            #if not self.continuous and not self.datetime:\n",
    "            #    raise ValueError('Column type undefined: need either categories, continuous or datetime')\n",
    "            for f in ['ordered','groups','colors','num_values','likert','topo_feature']:\n",
    "                if getattr(self,f):\n",
    "                    raise ValueError(f'Field {f} only makes sense for categorical columns {getattr(self,f)}')\n",
    "        else: # Is categorical\n",
    "            if not self.ordered:\n",
    "                for f in ['likert']: # ['num_values'] can be situationally useful in non-ordered settings\n",
    "                    if getattr(self,f):\n",
    "                        raise ValueError(f'Field {f} only makes sense for ordered categorical columns')\n",
    "        return self\n",
    "\n",
    "\n",
    "# This is for the block-level 'scale' group - basically same as ColumnMeta but with a few extras\n",
    "class BlockScaleMeta(ColumnMeta):\n",
    "\n",
    "    # Only useful in 'scale' block\n",
    "    col_prefix: Optional[str] = None # If column name should have the prefix added. Usually used in scale block\n",
    "    question_colors: Optional[Dict[str,Color]] = None # Dict mapping columns to different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Transform the column tuple to (new name, old name, meta) format\n",
    "def cspec(tpl):\n",
    "    if type(tpl)==list:\n",
    "        cn = tpl[0] # column name\n",
    "        sn = tpl[1] if len(tpl)>1 and type(tpl[1])==str else cn # source column\n",
    "        o_cd = tpl[2] if len(tpl)==3 else tpl[1] if len(tpl)==2 and type(tpl[1])==dict else {} # metadata\n",
    "    else:\n",
    "        cn = sn = tpl\n",
    "        o_cd = {}\n",
    "    return [cn,sn,o_cd]\n",
    "\n",
    "# Transform list to dict for better error readability\n",
    "\n",
    "\n",
    "def cs_lst_to_dict(lst):\n",
    "    return { cn: [ocn,meta] for cn,ocn,meta in map(cspec,lst) }\n",
    "\n",
    "\n",
    "ColSpec = Annotated[Dict[str,Tuple[str,ColumnMeta]],BeforeValidator(cs_lst_to_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class ColumnBlockMeta(PBase):\n",
    "    name: str # Name of the block\n",
    "    scale: Optional[BlockScaleMeta] = None # Shared column meta for all columns inside the block\n",
    "\n",
    "    # List of columns, potentially with their ColummnMetas\n",
    "    columns: ColSpec\n",
    "\n",
    "    subgroup_transform: Optional[str] = None # A block-level transform performed after column level transformations\n",
    "\n",
    "    # Block level flags\n",
    "    generated: bool = False # This block is for data that is generated, i.e. not initially in the file.\n",
    "    hidden: bool = False # Use this to hide the block in explorer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Again, convert list to dict for easier debugging in case errors get thrown\n",
    "def cb_lst_to_dict(lst): return { c['name']:c for c in lst }\n",
    "\n",
    "\n",
    "BlockSpec = Annotated[Dict[str,ColumnBlockMeta],BeforeValidator(cb_lst_to_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class FileDesc(PBase):\n",
    "    file: str\n",
    "    opts: Optional[Dict] = None\n",
    "\n",
    "\n",
    "class DataMeta(PBase):\n",
    "\n",
    "    #########################################################\n",
    "    # Metadata\n",
    "    #########################################################\n",
    "\n",
    "    description: Optional[str] = None # Description of the data\n",
    "    source: Optional[str] = None # Source of the data\n",
    "    restrictions: Optional[str] = None # Restrictions on the data use\n",
    "\n",
    "    collection_start: Optional[str] = None # Date in a way pd.to_datetime can parse it\n",
    "    collection_end: Optional[str] = None # Date in a way pd.to_datetime can parse it\n",
    "\n",
    "    author: Optional[str] = None # AUthor of the metafile\n",
    "\n",
    "    ########################################################\n",
    "    # Data source(s)\n",
    "    ########################################################\n",
    "\n",
    "    # Single input file\n",
    "    file: Optional[str] = None # Name of the file, with relative path from this json file\n",
    "    read_opts: Optional[Dict] = None # Additional options to pass to reading function\n",
    "\n",
    "    # Multiple files\n",
    "    files: Optional[List[FileDesc]] = None\n",
    "\n",
    "    ########################################################\n",
    "    # Data processing\n",
    "    ########################################################\n",
    "\n",
    "    # Main meat of data annotations\n",
    "    structure: BlockSpec\n",
    "\n",
    "    # A set of values that can be referenced in the file below\n",
    "    constants: Optional[Dict] = None\n",
    "\n",
    "    # Different global processing steps\n",
    "    preprocessing: Optional[Union[str,List[str]]] = None # Performed on raw data\n",
    "    postprocessing: Optional[Union[str,List[str]]] = None # Performed after columns and blocks have been processed\n",
    "\n",
    "    weight_col: Optional[str] = None # Column to use for weighting - overriden by model to population weight column\n",
    "\n",
    "    # List of data points that should be excluded in alyses\n",
    "    excluded: List[Tuple[int,str]] = [] # Index of row + str  reason for exclusion\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    @classmethod\n",
    "    def replace_constants(cls, meta: Any) -> Any:\n",
    "        return replace_constants(meta)\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def check_file(self) -> Self:\n",
    "        if self.file is None and self.files is None:\n",
    "            raise ValueError(\"One of 'file' or 'files' has to be provided\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def hard_validate(m):\n",
    "    DataMeta.model_validate(m)\n",
    "\n",
    "\n",
    "def soft_validate(m):\n",
    "    try:\n",
    "        DataMeta.model_validate(m)\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DataDescription = ForwardRef('DataDescription')\n",
    "DataSpec = Union[str,DataDescription]\n",
    "\n",
    "\n",
    "class SingleMergeSpec(PBase):\n",
    "    file: DataSpec # Filename to merge with\n",
    "    on: Union[str,List[str]] # Column(s) on which to merge\n",
    "    add: Optional[List[str]] = None # Column names to add with merge. If None, add all.\n",
    "    how: Literal['inner','outer','left','right','cross'] = 'inner' # Type of merge. See pd.merge\n",
    "\n",
    "\n",
    "MergeSpec = Union[SingleMergeSpec,List[SingleMergeSpec]]\n",
    "\n",
    "# Make sure MergeSpec results in a list, even if input is a singular SingleMergeSpec\n",
    "\n",
    "\n",
    "def smc_ensure_list(v): return v if isinstance(v,list) else [v]\n",
    "\n",
    "\n",
    "MergeSpec = Annotated[List[SingleMergeSpec],BeforeValidator(smc_ensure_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This is the input for read_and_process_data, that allows some operations on top of data meta\n",
    "\n",
    "class DataDescription(BaseModel):\n",
    "    # NB! BaseModel not PBase to allow for extensions such as PopulationDescription\n",
    "    file: Optional[str] = None # Single file to read\n",
    "    files: Optional[List[Union[str,Dict]]] = None # Multiple files to parse\n",
    "    data: Optional[Dict[str,Any]] = None # Alternative to file, files. Dictionary of column {name: values} pairs.\n",
    "    preprocessing: Optional[Union[str,List[str]]] = None # String of python code that can reference df\n",
    "    filter: Optional[str] = None # String of python code that can reference df and is evaluated as df[filter code]\n",
    "    merge: MergeSpec = [] # Optionally merge another data source into this one\n",
    "    postprocessing: Optional[Union[str,List[str]]] = None # String of python code that can reference df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def soft_validate(m, pptype):\n",
    "    try:\n",
    "        pptype.model_validate(m)\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('../../sandbox/germany/data/fes_de_2024.json') as jf:\n",
    "#     m = json.load(jf)\n",
    "# soft_validate(m,DataMeta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
