{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Utility functions with wider use potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os, warnings, math, inspect, sys\n",
    "import itertools as it\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import datetime as dt\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.colors as mpc\n",
    "import colorsys, hsluv\n",
    "from copy import deepcopy\n",
    "from hashlib import sha256\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "# convenience for warnings that gives a more useful stack frame (fn calling the warning, not warning fn itself)\n",
    "warn = lambda msg,*args: warnings.warn(msg,*args,stacklevel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# I'm surprised pandas does not have this function but I could not find it. \n",
    "def factorize_w_codes(s, codes):\n",
    "    res = s.astype('object').replace(dict(zip(codes,range(len(codes)))))\n",
    "    if not s.dropna().isin(codes).all(): # Throw an exception if all values were not replaced\n",
    "        vals = set(s) - set(codes)\n",
    "        raise Exception(f'Codes for {s.name} do not match all values: {vals}')\n",
    "    return res.fillna(-1).to_numpy(dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Simple batching of an iterable\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# turn index values into order indices\n",
    "def loc2iloc(index, vals):\n",
    "    d = dict(zip(np.array(index),range(len(index))))\n",
    "    return [ d[v] for v in vals ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Round in a way that preserves total sum\n",
    "def match_sum_round(s):\n",
    "    s = np.array(s)\n",
    "    fs = np.floor(s)\n",
    "    diff = round(s.sum()-fs.sum())\n",
    "    residues = np.argsort(-(s%1))[:diff]\n",
    "    fs[residues] = fs[residues]+1\n",
    "    return fs.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert (match_sum_round([0.7,0.7,0.6]) == [1,1,0]).all()\n",
    "assert (match_sum_round([1,2,3]) == [1,2,3]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Find the minimum difference between two values in the array\n",
    "def min_diff(arr):\n",
    "    b = np.diff(np.sort(arr))\n",
    "    if len(b)==0 or b.max()==0.0: return 0\n",
    "    else: return b[b>0].min()\n",
    "\n",
    "# Turn a discretized variable into a more smooth continuous one w a gaussian kernel\n",
    "def continify(ar, bounded=False, delta=0.0):\n",
    "    mi,ma = ar.min()+delta, ar.max()-delta\n",
    "    noise = np.random.normal(0,0.5 * min_diff(ar),size=len(ar))\n",
    "    res = ar + noise\n",
    "    if bounded: # Reflect the noise on the boundaries\n",
    "        res[res>ma] = ma - (res[res>ma] - ma)\n",
    "        res[res<mi] = mi + (mi - res[res<mi])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert min_diff([0,0,2,1.5,3,3]) == 0.5\n",
    "assert min_diff([]) == min_diff([1,1]) == 0.0\n",
    "\n",
    "ar = np.array([0,2,4,1,5]+ [5]*1000 + [-1]*1000)\n",
    "c_ar = continify(ar,True)\n",
    "assert c_ar.min() >= ar.min() and c_ar.max()<=ar.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Replace categorical with dummy values,\n",
    "def replace_cat_with_dummies(df,c,cs):\n",
    "    return pd.concat([df.drop(columns=[c]),pd.get_dummies(df[c])[cs[1:]].astype(float)],axis=1)\n",
    "\n",
    "# Match data1 with data2 on columns cols as closely as possible\n",
    "def match_data(data1,data2,cols=None):\n",
    "    d1 = data1[cols].copy().dropna()\n",
    "    d2 = data2[cols].copy().dropna()\n",
    "\n",
    "    if len(d1)==0 or len(d2)==0: return [],[]\n",
    "\n",
    "    ccols = [c for c in cols if d1[c].dtype.name=='category']\n",
    "    for c in ccols:\n",
    "        if d1[c].dtype.ordered: # replace categories with their index\n",
    "            s1, s2 = set(d1[c].dtype.categories), set(d2[c].dtype.categories)\n",
    "            if s1-s2 and s2-s1: # one-way imbalance is fine\n",
    "                raise Exception(f\"Ordered categorical columns differ in their categories on: {s1-s2} vs {s2-s1}\")\n",
    "            \n",
    "            md = d1 if len(s2-s1)==0 else d2\n",
    "            mdict = dict(zip(md[c].dtype.categories, np.linspace(0,2,len(md[c].dtype.categories))))\n",
    "            d1[c] = d1[c].astype('object').replace(mdict)\n",
    "            d2[c] = d2[c].astype('object').replace(mdict)\n",
    "        else: # Use one-hot encoding instead\n",
    "            cs = list(set(d1[c].unique()) | set(d2[c].unique()))\n",
    "            d1[c], d2[c] = pd.Categorical(d1[c],cs), pd.Categorical(d2[c],cs)\n",
    "            # Use all but the first category as otherwise mahalanobis fails because of full colinearity\n",
    "            d1 = replace_cat_with_dummies(d1,c,cs)\n",
    "            d2 = replace_cat_with_dummies(d2,c,cs)\n",
    "\n",
    "\n",
    "    # Use pseudoinverse in case we have collinear columns\n",
    "    cov = np.cov(np.vstack([d1.values, d2.values]).T.astype('float'))\n",
    "    cov += np.eye(len(cov))*1e-5 # Add a small amount of noise to avoid singular matrix\n",
    "    pVI = np.linalg.pinv(cov).T\n",
    "    dmat = cdist(d1, d2, 'mahalanobis', VI=pVI)\n",
    "    i1, i2 = linear_sum_assignment(dmat, maximize=False)\n",
    "    ind1, ind2 = d1.index[i1], d2.index[i2]\n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "from salk_toolkit.io import process_annotated_data\n",
    "data = process_annotated_data('../data/master_meta.json')\n",
    "data1,data2 = data.iloc[:10], data.iloc[10:]\n",
    "cols = ['age','gender','education','nationality']\n",
    "\n",
    "# Make sure everything except age (cols[0]) gets exactly matched\n",
    "i1,i2 = match_data(data1,data2,cols)\n",
    "assert (data1.loc[i1,cols[1:]].reset_index(drop=True) == data2.loc[i2,cols[1:]].reset_index(drop=True)).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Allow 'constants' entries in the dict to provide replacement mappings\n",
    "# This leads to much more readable jsons as repetitions can be avoided\n",
    "def replace_constants(d, constants = {}, inplace=False):\n",
    "    if not inplace: d = deepcopy(d)\n",
    "    if type(d)==dict and 'constants' in d:\n",
    "        constants = constants.copy() # Otherwise it would propagate back up through recursion - see test6 below\n",
    "        constants.update(d['constants'])\n",
    "        del d['constants']\n",
    "\n",
    "    for k, v in (d.items() if type(d)==dict else enumerate(d)):\n",
    "        if type(v)==str and v in constants:\n",
    "            d[k] = constants[v]\n",
    "        elif type(v)==dict or type(v)==list:\n",
    "            d[k] = replace_constants(v,constants, inplace=True)\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Little function to do approximate string matching between two lists. Useful if things have multiple spellings. \n",
    "def approx_str_match(frm,to,dist_fn=None):\n",
    "    if not isinstance(frm,list): frm = list(frm)\n",
    "    if not isinstance(frm,list): to = list(to)\n",
    "    if dist_fn is None: dist_fn = Levenshtein.distance \n",
    "    dmat = scipy.spatial.distance.cdist(np.array(frm)[:,None],np.array(to)[:,None],lambda x,y: dist_fn(x[0],y[0]))\n",
    "    t1,t2 = scipy.optimize.linear_sum_assignment(dmat)\n",
    "    return dict(zip([frm[i] for i in t1],[to[i] for i in t2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert approx_str_match(['aaabc','xyz'],['xxy','ac','dfg']) == {'aaabc': 'ac', 'xyz': 'xxy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# Test replace_constants\n",
    "d = {\n",
    "    'constants': { 'a': {'a':1}, 'b':['b'] },\n",
    "    'test1': 'a',\n",
    "    'test2': [1,'b'],\n",
    "    'test3': { 'xy': 'a' },\n",
    "    'test4': { 'xy': [2, 'b'] },\n",
    "    'test5': { 'constants': {'a': ['a'] }, 'x':'a' },\n",
    "    'test6': 'a'\n",
    "}\n",
    "dr = replace_constants(d)\n",
    "assert dr == {'test1': {'a': 1}, 'test2': [1, ['b']], 'test3': {'xy': {'a': 1}}, 'test4': {'xy': [2, ['b']]}, 'test5': {'x': ['a']}, 'test6': {'a': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# JSON encoder needed to convert pandas indices into lists for serialization\n",
    "def index_encoder(z):\n",
    "    if isinstance(z, pd.Index):\n",
    "        return list(z)\n",
    "    else:\n",
    "        type_name = z.__class__.__name__\n",
    "        raise TypeError(f\"Object of type {type_name} is not serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "default_color = 'lightgrey' # Something that stands out so it is easy to notice a missing color\n",
    "\n",
    "# Helper function to turn a dictionary into an Altair scale (or None into alt.Undefined)\n",
    "# Also: preserving order matters because scale order overrides sort argument\n",
    "def to_alt_scale(scale, order=None):\n",
    "    if scale is None: scale = alt.Undefined\n",
    "    if isinstance(scale,dict):\n",
    "        if order is None: order = scale.keys()\n",
    "        #else: order = [ c for c in order if c in scale ]\n",
    "        scale = alt.Scale(domain=list(order),range=[ (scale[c] if c in scale else default_color) for c in order ])\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Turn a question with multiple variants all of which are in distinct columns into a two columns - one with response, the other with which question variant was used\n",
    "\n",
    "def multicol_to_vals_cats(df, cols=None, col_prefix=None, reverse_cols=[], reverse_suffixes=None, cat_order=None, vals_name='vals', cats_name='cats', inplace=False):\n",
    "    if not inplace: df = df.copy()\n",
    "    if cols is None: cols = [ c for c in df.columns if c.startswith(col_prefix)]\n",
    "    \n",
    "    if not reverse_cols and reverse_suffixes is not None:\n",
    "        reverse_cols = list({ c for c in cols for rs in reverse_suffixes if c.endswith(rs)})\n",
    "    \n",
    "    if len(reverse_cols)>0:\n",
    "        remap = dict(zip(cat_order,reversed(cat_order)))\n",
    "        df.loc[:,reverse_cols] = df.loc[:,reverse_cols].astype('object').replace(remap)\n",
    "    \n",
    "    tdf = df[cols]\n",
    "    cinds = np.argmax(tdf.notna(),axis=1)\n",
    "    df.loc[:,vals_name] = np.array(tdf)[range(len(tdf)),cinds]\n",
    "    df.loc[:,cats_name] = np.array(tdf.columns)[cinds]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "df = pd.DataFrame({ 'q1': ['a','b','c',None,None,None], 'q1b': [None,None,None,'c','b','a'] })\n",
    "ndf = multicol_to_vals_cats(df,col_prefix='q1',reverse_suffixes=['1b'],cat_order=['a','b','c'])\n",
    "assert (ndf['vals'] == ['a','b','c','a','b','c']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "default_bidirectional_gradient = ['#c30d24', '#e67f6c', '#c3b6af', '#74b0ce', '#1770ab']\n",
    "redblue_gradient = [\"#8D0E26\", \"#EA9379\", \"#F2EFEE\", \"#8FC1DC\", \"#134C85\"]\n",
    "greyscale_gradient = [\"#444444\", \"#ffffff\"]\n",
    "\n",
    "# Grad is a list of colors\n",
    "def gradient_to_discrete_color_scale( grad, num_colors):\n",
    "    cmap = mpc.LinearSegmentedColormap.from_list('grad',grad)\n",
    "    return [mpc.to_hex(cmap(i)) for i in np.linspace(0, 1, num_colors)]\n",
    "\n",
    "def gradient_subrange(grad, num_colors, range=[-1,1], bidirectional=True):\n",
    "    base = [-1,1] if bidirectional else [0,1]\n",
    "    wr = (range[1]-range[0])/(base[1]-base[0])\n",
    "    nt = round(num_colors/wr)\n",
    "    grad = gradient_to_discrete_color_scale(grad, nt)\n",
    "    \n",
    "    mi, ma = round(nt*(range[0]-base[0])/(base[1]-base[0])), round(nt*(range[1]-base[0])/(base[1]-base[0]))\n",
    "    return grad[mi:ma]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gradient_to_discrete_color_scale(['#ff0000','#ffff00','#00ff00'],4) == ['#ff0000', '#ffaa00', '#aaff00', '#00ff00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Create a color gradient from a given single color\n",
    "def gradient_from_color(color, l_value=0.3, n_points=7, range=[0,1]):\n",
    "    # Get hue and saturation for color (ignoring luminosity to make scales uniform on that)    \n",
    "    ch,cs,_ = hsluv.hex_to_hsluv(mpc.to_hex(color))\n",
    "\n",
    "    max_l = 94 # Set max luminosity to be slightly below pure white\n",
    "    l_diff = max_l - 100*l_value # Difference between max and min luminosity\n",
    "\n",
    "    beg_s, end_s = 3*cs*range[0], 3*cs*range[1] # As we use min(cs, s), this just desaturates on first 1/3 of the range\n",
    "    beg_l, end_l = max_l-l_diff*range[0], max_l-l_diff*range[1]\n",
    "\n",
    "    ls = np.linspace(0,1,n_points) # Create n_points steps in hsluv space\n",
    "    return [ hsluv.hsluv_to_hex((ch,min(cs,w*end_s+(1-w)*beg_s),(w*end_l+(1-w)*beg_l))) for w in ls ]\n",
    "\n",
    "# Alternative version - preserves colors slightly better, but at the cost of more washing out\n",
    "def gradient_from_color_alt(color, l_value=0.6, n_points=7, range=[0,1]):\n",
    "    # Get hue and saturation for color (ignoring luminosity to make scales uniform on that)    \n",
    "    ch,cs,cl = hsluv.hex_to_hsluv(mpc.to_hex(color))\n",
    "\n",
    "    max_l = 94 # Set max luminosity to be slightly below pure white\n",
    "    if cs<50: l_value = 0.3 # For very washed out tones, make sure we have enough luminosity contrast\n",
    "    l_diff = max_l - min(cl,100*l_value) # Difference between max and min luminosity\n",
    "\n",
    "    beg_s, end_s = 3*cs*range[0], 3*cs*range[1] # As we use min(cs, s), this just desaturates on lower part of range\n",
    "    beg_l, end_l = max_l-l_diff*range[0], max_l-l_diff*range[1]\n",
    "\n",
    "    ls = np.linspace(0,1,n_points) # Create n_points steps in hsluv space\n",
    "    return [ hsluv.hsluv_to_hex((ch,min(cs,w*end_s+(1-w)*beg_s),(w*end_l+(1-w)*beg_l))) for w in ls ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# This function is used to choose colors and positioning for likert bars with (potentially multiple) neutral categories\n",
    "def split_to_neg_neutral_pos(cats,neutrals):\n",
    "    cats,mid = list(cats),len(cats)//2\n",
    "    if not neutrals:\n",
    "        if len(cats)%2==1:\n",
    "            return cats[:mid], [cats[mid]], cats[mid+1:]\n",
    "        else: return cats[:mid], [], cats[mid:]\n",
    "    \n",
    "    # Find a neutral that is not at start or end\n",
    "    bi,ei = 0,0\n",
    "    while cats[bi] in neutrals: bi += 1\n",
    "    while cats[-ei-1] in neutrals: ei += 1\n",
    "    cn = [ c for c in neutrals if c in cats[bi:len(cats)-ei] ]\n",
    "\n",
    "    # If no such neutral, split evenly between positive and negative\n",
    "    if not cn:\n",
    "        posneg = [ c for c in cats if c not in neutrals ]\n",
    "        pnmid = len(posneg)//2\n",
    "        if len(posneg)%2==1:\n",
    "            return posneg[:pnmid], neutrals+[posneg[pnmid]], posneg[pnmid+1:]\n",
    "        else: return posneg[:pnmid], neutrals, posneg[pnmid:]\n",
    "    else: # Split around the first central neutral found\n",
    "        ci = cats.index(cn[0])\n",
    "        neg = [ c for c in cats[:ci] if c not in neutrals ]\n",
    "        pos = [ c for c in cats[ci:] if c not in neutrals ]\n",
    "        return neg, neutrals, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_datetime(col):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "        return pd.api.types.is_datetime64_any_dtype(col) or (col.dtype.name in ['str','object'] and pd.to_datetime(col,errors='coerce').notna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Convert a series of wave indices and a series of survey dates into a time series usable by our gp model\n",
    "def rel_wave_times(ws, dts, dt0=None):\n",
    "    df = pd.DataFrame({'wave':ws, 'dt': pd.to_datetime(dts)})\n",
    "    adf = df.groupby('wave')['dt'].median()\n",
    "    if dt0 is None: dt0 = adf.max() # use last wave date as the reference\n",
    "    \n",
    "    w_to_time = dict(((adf - dt0).dt.days/30).items())\n",
    "    \n",
    "    return pd.Series(df['wave'].replace(w_to_time),name='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from salk_toolkit.io import process_annotated_data\n",
    "data = process_annotated_data('../data/master_meta.json')\n",
    "assert (rel_wave_times(data['wave'],data['date'])-data['t']).std() < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Generate a random draws column that is deterministic in n, n_draws and uid\n",
    "def stable_draws(n, n_draws, uid):\n",
    "    # Initialize a random generator with a hash of uid\n",
    "    bgen = np.random.SFC64(np.frombuffer(sha256(str(uid).encode(\"utf-8\")).digest(), dtype='uint32'))\n",
    "    gen = np.random.Generator(bgen)\n",
    "    \n",
    "    n_samples = int(math.ceil(n/n_draws))\n",
    "    draws = np.tile(np.arange(n_draws),n_samples)[:n]\n",
    "    return gen.permuted(draws)\n",
    "\n",
    "# Use the stable_draws function to deterministicall assign shuffled draws to a df \n",
    "def deterministic_draws(df, n_draws, uid, n_total=None):\n",
    "    if n_total is None: n_total = len(df)\n",
    "    df.loc[:,'draw'] = pd.Series(stable_draws(n_total, n_draws, uid), index = np.arange(n_total))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (stable_draws(20,5,'test') == np.array([1, 2, 3, 3, 2, 3, 2, 2, 0, 0, 0, 3, 4, 4, 1, 1, 1, 0, 4, 4])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Clean kwargs leaving only parameters fn can digest\n",
    "def clean_kwargs(fn, kwargs):\n",
    "    aspec = inspect.getfullargspec(fn)\n",
    "    return { k:v for k,v in kwargs.items() if k in aspec.args } if aspec.varkw is None else kwargs\n",
    "\n",
    "def call_kwsafe(fn,*args,**kwargs):\n",
    "    return fn(*args,**clean_kwargs(fn,kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Simple one-liner to remove certain keys from a dict\n",
    "def censor_dict(d,vs):\n",
    "    return { k:v for k,v in d.items() if k not in vs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Create nice labels for a cut\n",
    "# Used by the cut_nice below as well as for a lazy polars version in pp\n",
    "def cut_nice_labels(breaks, mi=-np.inf, ma=np.inf, isint=False, format='', separator=' - '):\n",
    "\n",
    "    # Extend breaks if needed\n",
    "    lopen, ropen = False, False\n",
    "    if ma > breaks[-1]:\n",
    "        breaks.append(ma + 1)\n",
    "        ropen = True\n",
    "    if mi < breaks[0]:\n",
    "        breaks.insert(0, mi)\n",
    "        lopen = True\n",
    "\n",
    "    obreaks = breaks.copy()\n",
    "    \n",
    "    if isint:\n",
    "        breaks = list(map(int, breaks))\n",
    "        format = ''  # No need for decimal places if all integers\n",
    "        breaks[-1] += 1 # to counter the -1 applied below \n",
    "    \n",
    "    tuples = [(breaks[i], breaks[i + 1] - (1 if isint else 0)) for i in range(len(breaks) - 1)]\n",
    "    labels = [f\"{t[0]:{format}}{separator}{t[1]:{format}}\" if t[0] != t[1] else f\"{t[0]:{format}}\" for t in tuples]\n",
    "    \n",
    "    if lopen: labels[0] = f\"<{breaks[1]:{format}}\"\n",
    "    if ropen: labels[-1] = f\"{breaks[-2]:{format}}+\"\n",
    "\n",
    "    return obreaks, labels\n",
    "\n",
    "# A nicer behaving wrapper around pd.cut\n",
    "def cut_nice(s, breaks, format='', separator=' - '):\n",
    "    s = np.array(s)\n",
    "    mi, ma = s.min(), s.max()\n",
    "    isint = np.issubdtype(s.dtype, np.integer) or (s % 1 == 0.0).all()\n",
    "    breaks, labels = cut_nice_labels(breaks, mi, ma, isint, format, separator)\n",
    "    return pd.cut(s, breaks, right=False, labels=labels, ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (cut_nice([19,20,29,30,39,199],[0,20.0,30,40,50,60,70]) == ['0 - 19', '20 - 29', '20 - 29', '30 - 39', '30 - 39', '70+']).all()\n",
    "assert (cut_nice([19,20,29,30,39],[20.0,30,40,50,60,70]) == ['<20', '20 - 29', '20 - 29', '30 - 39', '30 - 39']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Utility function to rename categories in pre/post processing steps as pandas made .replace unusable with categories\n",
    "def rename_cats(df, col, cat_map):\n",
    "    if df[col].dtype.name == 'category': \n",
    "        df[col] = df[col].cat.rename_categories(cat_map)\n",
    "    else: df[col] = df[col].replace(cat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# Simplify doing multiple replace's on a column\n",
    "def str_replace(s,d):\n",
    "    s = s.astype('object')\n",
    "    for k,v in d.items():\n",
    "        s = s.str.replace(k,v)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (str_replace(pd.Series(['abc','bca','def']),{'a':'x', 'bc': 'y'}) == ['xy','yx','def']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# Merge values from multiple columns, iteratively replacing values\n",
    "# lst contains either a series or a tuple of (series, whitelist)\n",
    "def merge_series(*lst):\n",
    "    s = lst[0].astype('object').copy()\n",
    "    for t in lst[1:]:\n",
    "        if isinstance(t,tuple):\n",
    "            ns, whitelist = t\n",
    "            inds = ~ns.isna() & ns.isin(whitelist)\n",
    "            s.loc[inds] = ns[inds]\n",
    "        else: \n",
    "            ns = t\n",
    "            s.loc[~ns.isna()] = ns[~ns.isna()]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a': ['a','a','a'], 'b': [ 'x',None,None], 'c': ['d','e','f']})\n",
    "assert (merge_series(df['a'],df['b'],(df['c'],['f','g'])) == ['x','a','f']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Turn a list of selected/not seleced into a list of selected values in the same dataframe\n",
    "def aggregate_multiselect(df, prefix, out_prefix, na_vals=[], colnames_as_values=False, inplace=True):\n",
    "     cols = [ c for c in df.columns if c.startswith(prefix) ]\n",
    "     dfc = df[cols].astype('object').replace(dict(zip(na_vals,[None]*len(na_vals))))\n",
    "\n",
    "     # Turn column names into the values - this is sometimes necessary \n",
    "     # as values in col might be \"mentioned\"/\"not mentioned\"\n",
    "     if colnames_as_values:\n",
    "          dfc = dfc.copy()\n",
    "          for c in dfc.columns: \n",
    "               dfc.loc[~dfc[c].isna(),c] = c.removeprefix(prefix)\n",
    "          \n",
    "     lst = list(map(lambda l: [ v for v in l if v is not None ],dfc.values.tolist()))\n",
    "     n_res = max(map(len,lst))\n",
    "     columns = [f'{out_prefix}{i+1}' for i in range(n_res)]\n",
    "     if inplace: df[columns] = pd.DataFrame(lst)\n",
    "     else: return pd.DataFrame(lst, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Take a list of values and create a one-hot matrix of them. Basically the inverse of previous\n",
    "def deaggregate_multiselect(df, prefix, out_prefix=''):\n",
    "    cols = [ c for c in df.columns if c.startswith(prefix) ]\n",
    "\n",
    "    # Determine all categories\n",
    "    ocols = set()\n",
    "    for c in cols: ocols.update(df[c].dropna().unique())\n",
    "\n",
    "    # Create a one-hot column for each\n",
    "    for oc in ocols: df[out_prefix+oc] = (df[cols]==oc).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Groupby if needed - this simplifies things quite often\n",
    "def gb_in(df, gb_cols):\n",
    "    return df.groupby(gb_cols,observed=False) if len(gb_cols)>0 else df\n",
    "\n",
    "# Groupby apply if needed - similar to gb_in but for apply\n",
    "def gb_in_apply(df, gb_cols, fn, cols=None, **kwargs):\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    if len(gb_cols)==0: \n",
    "        res = fn(df[cols],**kwargs)\n",
    "        if type(res)==pd.Series: res = pd.DataFrame(res).T\n",
    "    else: res = df.groupby(gb_cols,observed=False)[cols].apply(fn,**kwargs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def stk_defaultdict(dv):\n",
    "    if not isinstance(dv,dict): dv = {'default':dv}\n",
    "    return defaultdict(lambda: dv['default'], dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def cached_fn(fn):\n",
    "    cache = {}\n",
    "    def cf(x):\n",
    "        if x not in cache:\n",
    "            cache[x] = fn(x)\n",
    "        return cache[x]\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def scores_to_ordinal_rankings(df, cols, name, prefix=''):\n",
    "\n",
    "    # If cols is a string, treat it as a prefix and find all columns that start with it\n",
    "    if isinstance(cols,str):\n",
    "        prefix = prefix or cols\n",
    "        cols = [ c for c in df.columns if c.startswith(cols) ]\n",
    "\n",
    "    sinds = np.argsort(-df[cols].values,axis=1)\n",
    "    \n",
    "    rmat = df[cols].rank(method='max', ascending=False, axis=1).values\n",
    "    #rmat = np.concatenate([rmat,np.full((len(rmat),1),0)],axis=1)\n",
    "    rvals = rmat[np.tile(np.arange(len(rmat)),(len(rmat[0]),1)).T,sinds]\n",
    "\n",
    "    names_a, ties_a = [], []\n",
    "    for cns, rs in zip(np.array(cols)[sinds],rvals):\n",
    "        if np.isnan(rs[-1]):\n",
    "            l = np.where(np.isnan(rs))[0][0]\n",
    "            cns, rs = cns[:l], rs[:l]\n",
    "        ties = (rs-np.arange(len(rs))-1).astype(int)\n",
    "        names_a.append([ c[len(prefix):] for c in cns ])\n",
    "        ties_a.append(list(ties))\n",
    "    df[f'{name}_orank'] = names_a\n",
    "    df[f'{name}_ties'] = ties_a\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scores to ordinal rankings\n",
    "\n",
    "df = pd.DataFrame({'a':[1,2], 'b':[2,2], 'c': [np.nan,1]})\n",
    "res_df = scores_to_ordinal_rankings(df, ['a','b','c'], 'test')\n",
    "expected_df = pd.DataFrame({'test_orank':[['b','a'],['a','b','c']], 'test_ties':[[0,0],[1,0,0]]})\n",
    "assert expected_df.equals(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Basic limited length cache\n",
    "class dict_cache(OrderedDict):\n",
    "\n",
    "    def __init__(self, size = 10, *args, **kwargs):\n",
    "        self.size = size\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "\n",
    "        # If we would hit the limit, remove the oldest item\n",
    "        if len(self) >= self.size:\n",
    "            old = next(iter(self))\n",
    "            super().__delitem__(old)\n",
    "\n",
    "        # Add the new item\n",
    "        super().__setitem__(key, value)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        res = super().__getitem__(key)\n",
    "        super().move_to_end(key) # Move it up to indicate recent use\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Borrowed from https://goshippo.com/blog/measure-real-size-any-python-object\n",
    "# Get the size of an object recursively\n",
    "def get_size(obj, seen=None):\n",
    "    \n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Vega Lite (Altair) will fail with certain column names, so we need to escape them\n",
    "# To do that, we use unicode symbols that are visually similar to the problematic characters\n",
    "def escape_vega_label(label):\n",
    "    return label.replace('.','․').replace('[','［').replace(']','］')\n",
    "\n",
    "def unescape_vega_label(label):\n",
    "    return label.replace('․','.').replace('［','[').replace('］',']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
