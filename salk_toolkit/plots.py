"""Common plots used in the dashboards"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_plots.ipynb.

# %% auto 0
__all__ = ['estimate_legend_columns_horiz_naive', 'estimate_legend_columns_horiz', 'boxplot_vals', 'boxplot_manual', 'columns',
           'stacked_columns', 'diff_columns', 'massplot', 'make_start_end', 'likert_bars', 'kde_bw', 'kde_1d',
           'density', 'violin', 'cluster_based_reorder', 'matrix', 'corr_matrix', 'lines', 'draws_to_hdis', 'lines_hdi',
           'area_smooth', 'likert_aggregate', 'likert_rad_pol', 'barbell', 'geoplot', 'geobest', 'fd_mangle',
           'facet_dist', 'ordered_population', 'marimekko']

# %% ../nbs/03_plots.ipynb 3
import json, os, math
import itertools as it
from collections import defaultdict

import numpy as np
import pandas as pd
import datetime as dt

from typing import List, Tuple, Dict, Union, Optional

import altair as alt
import scipy as sp
import scipy.stats as sps
from scipy.cluster import hierarchy
from KDEpy import FFTKDE
from KDEpy.bw_selection import *
import arviz as az

from salk_toolkit.utils import *
from salk_toolkit.io import extract_column_meta, read_json
from salk_toolkit.pp import registry, registry_meta, e2e_plot, stk_plot

from matplotlib import font_manager
from PIL import ImageFont

# %% ../nbs/03_plots.ipynb 7
# Find a sensible approximation to the font used in vega/altair
font = font_manager.FontProperties(family='sans-serif', weight='regular')
font_file = font_manager.findfont(font)
legend_font = ImageFont.truetype(font_file,10)

# %% ../nbs/03_plots.ipynb 8
# Legends are not wrapped, nor is there a good way of doing accurately it in vega/altair
# This attempts to estimate a reasonable value for columns which induces wrapping
def estimate_legend_columns_horiz_naive(cats, width):
    max_str_len = max(map(len,cats))
    n_cols = max(1,width//(15+5*max_str_len))
    # distribute them roughly equally to avoid last row being awkwardly shorter
    n_rows = int(math.ceil(len(cats)/n_cols))
    return int(math.ceil(len(cats)/n_rows))

# More sophisticated version that looks at lengths of individual strings across multiple rows
# ToDo: it should max over each column separately not just look at max(sum(row)). This is close enough though.
def estimate_legend_columns_horiz(cats, width, extra_text=[]):
    max_cols, restart = len(cats), True
    if extra_text: width -= max(map(legend_font.getlength,extra_text))
    lens = list(map(lambda s: 25+legend_font.getlength(s),cats))
    while restart:
        restart, rl, cc = False, 0, 0
        for l in lens:
            if cc >= max_cols: # Start a new row
                rl, cc = l, 1
            elif rl + l > width: # Exceed width - restart
                max_cols = cc
                # Start from beginning every thime columns number changes
                # This is because what ends up in second+ rows depends on length of first
                restart = True
            else: # Just append to existing row
                rl += l
                cc += 1
                
    # For very long labels just accept we can't do anything
    max_cols = max(max_cols,1)

    # distribute them roughly equally to avoid last row being awkwardly shorter
    n_rows = int(math.ceil(len(cats)/max_cols))
    return int(math.ceil(len(cats)/n_rows))

# %% ../nbs/03_plots.ipynb 13
# Regular boxplot with quantiles and Tukey whiskers
def boxplot_vals(s,extent=1.5):
    q1, q3 = s.quantile(0.25), s.quantile(0.75)
    return pd.DataFrame({
        'min': s.min(),
        'q1': q1,
        'median': s.median(),
        'q3': q3,
        'max': s.max(),
        # Tukey values
        'tmin': s[s>q1-extent*(q3-q1)].min(),
        'tmax': s[s<q3+extent*(q3-q1)].max()
    },index=['row'])


@stk_plot('boxplots', data_format='longform', draws=True, n_facets=(1,2), priority=50, group_sizes=True, args={'fit_beta_dist':'bool'})
def boxplot_manual(data, value_col='value', facets=[], val_format='%', width=800, tooltip=[], outer_factors=[], fit_beta_dist=False):
    f0, f1 = facets[0], facets[1] if len(facets)>1 else None

    if val_format[-1] == '%': # Boxplots being a compound plot, this workaround is needed for axis & tooltips to be proper
        data[value_col]*=100
        val_format = val_format[:-1]+'f'
    else: fit_beta_dist = False # Only use beta binomial for categoricals 

    f_cols = outer_factors+[f['col'] for f in facets[:2] if f is not None]
    if fit_beta_dist: 
        data['count'] = (data['group_size']*(data[value_col]/100)).round(0).astype('int')
        df = beta_binomial_fit(data,f_cols)
    else: df = data.groupby(f_cols,observed=True)[value_col].apply(boxplot_vals).reset_index()
    
    shared = {'y': alt.Y(f'{f0["col"]}:N', title=None, sort=f0['order']),
              **({'yOffset':alt.YOffset(f'{f1["col"]}:N', title=None, sort=f1['order'])} if f1 else {}),
              'tooltip': [ alt.Tooltip(f'{vn}:Q',format=val_format,title=f'{vn[0].upper()+vn[1:]} of {value_col}') for vn in ['min','q1','median','q3','max'] ] + tooltip[1:] }
    
    root = alt.Chart(df).encode(**shared)

    size = 12

    # Compose each layer individually
    lower_plot = root.mark_rule().encode(
        x=alt.X('tmin:Q', axis=alt.Axis(title=value_col, format=val_format)),
        x2=alt.X2('q1:Q'),
    )

    middle_plot = root.mark_bar(size=size).encode(
        x=alt.X('q1:Q'),
        x2=alt.X2('q3:Q'),
        **({
            'color': alt.Color(f'{f0["col"]}:N', scale=f0['colors'], legend=None)    
            } if not f1 else {
                'color': alt.Color(f'{f1["col"]}:N', scale=f1['colors'], 
                                    legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1['order'],width)))
        })
    )

    upper_plot = root.mark_rule().encode(
        x=alt.X('q3:Q'),
        x2=alt.X2('tmax:Q')
    )

    middle_tick = root.mark_tick(
        color='white',
        size=size
    ).encode(
        x='median:Q',
    )

    return (lower_plot + middle_plot + upper_plot + middle_tick)

# Also create a raw version for the same plot 
stk_plot('boxplots-raw', data_format="raw", n_facets=(1,2), priority=0)(boxplot_manual)

# %% ../nbs/03_plots.ipynb 16
@stk_plot('columns', data_format='longform', draws=False, n_facets=(1,2))
def columns(data, value_col='value', facets=[], val_format='%', width=800, tooltip=[]):
    f0, f1 = facets[0], facets[1] if len(facets)>1 else None
    plot = alt.Chart(data).mark_bar().encode(
        x=alt.X(
            f'{value_col}:Q',
            title=value_col,
            axis=alt.Axis(format=val_format),
        ),
        y=alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
        tooltip = tooltip,
        **({
                'color': alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=None)    
            } if not f1 else {
                'yOffset':alt.YOffset(f'{f1["col"]}:N', title=None, sort=f1["order"]),
                'color': alt.Color(f'{f1["col"]}:N', scale=f1["colors"],
                                    legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width)))
            }),
    )
    return plot

# %% ../nbs/03_plots.ipynb 19
@stk_plot('stacked_columns', data_format='longform', draws=False, nonnegative=True, n_facets=(2,2), agg_fn='sum', args={'normalized':'bool'})
def stacked_columns(data, value_col='value', facets=[], filtered_size=1, val_format='%', width=800, normalized=False, tooltip=[]):
    f0, f1 = facets[0], facets[1]
    
    data[value_col] = data[value_col]/filtered_size
    
    ldict = dict(zip(f1["order"], range(len(f1["order"]))))
    data['f_order'] = data[f1["col"]].astype('object').replace(ldict).astype('int')
    
    plot = alt.Chart(round(data, 3), width = 'container' \
    ).mark_bar().encode(
        x=alt.X(
            f'{value_col}:Q',
            title=value_col,
            axis=alt.Axis(format=val_format),
            **({'stack':'normalize'} if normalized else {})
            #scale=alt.Scale(domain=[0,30]) #see lõikab mõnedes jaotustes parema ääre ära
        ),
        y=alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
        tooltip = tooltip,
        **({
                'color': alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=None)    
            } if len(facets)<=1 else {
                'order': alt.Order('f_order:O'),
                'color': alt.Color(f'{f1["col"]}:N', scale=f1["colors"],
                                    legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width)))
            }),
    )
    return plot

# %% ../nbs/03_plots.ipynb 21
@stk_plot('diff_columns', data_format='longform', draws=False, n_facets=(2,2), args={'sort_descending':'bool'})
def diff_columns(data, value_col='value', facets=[], val_format='%', sort_descending=False, tooltip=[]):
    f0, f1 = facets[0], facets[1]
    
    ind_cols = list(set(data.columns)-{value_col,f1["col"]})
    factors = data[f1["col"]].unique() # use unique instead of categories to allow filters to select the two that remain
    
    idf = data.set_index(ind_cols)
    diff = (idf[idf[f1["col"]]==factors[1]][value_col]-idf[idf[f1["col"]]==factors[0]][value_col]).reset_index()
    
    if sort_descending: f0["order"] = list(diff.sort_values(value_col,ascending=False)[f0["col"]])
    
    plot = alt.Chart(round(diff, 3), width = 'container' \
    ).mark_bar().encode(
        y=alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
        x=alt.X(
            f'{value_col}:Q',
            title=f"{factors[1]} - {factors[0]}",
            axis=alt.Axis(format=val_format, title=f"{factors[0]} <> {factors[1]}"),
            #scale=alt.Scale(domain=[0,30]) #see lõikab mõnedes jaotustes parema ääre ära
            ),
        
        tooltip=[
            alt.Tooltip(f'{f0["col"]}:N'),
            alt.Tooltip(f'{value_col}:Q',format=val_format, title=f'{value_col} difference')
            ],
        color=alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=None)    
    )
    return plot

# %% ../nbs/03_plots.ipynb 23
# The idea was to also visualize the size of each cluster. Currently not very useful, may need to be rethought

@stk_plot('massplot', data_format='longform', draws=False, group_sizes=True, n_facets=(1,2), hidden=True)
def massplot(data, value_col='value', facets=[], filtered_size=1, val_format='%', width=800, tooltip=[]):
    f0, f1 = facets[0], facets[1] if len(facets)>1 else None

    data['group_size']=(data['group_size']/filtered_size)#.round(2)

    plot = alt.Chart(round(data, 3), width = 'container' \
    ).mark_circle().encode(
        y=alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
        x=alt.X(
            f'{value_col}:Q',
            title=value_col,
            axis=alt.Axis(format=val_format),
            #scale=alt.Scale(domain=[0,30]) #see lõikab mõnedes jaotustes parema ääre ära
            ),
        size=alt.Size('group_size:Q', legend=None, scale=alt.Scale(range=[100, 500])),
        opacity=alt.value(1.0),
        stroke=alt.value('#777'),
        tooltip = tooltip + [ alt.Tooltip('group_size:N',format='.1%',title='Group size') ],
        #tooltip=[
        #    'response:N',
            #alt.Tooltip('mean(support):Q',format='.1%')
        #    ],
        **({
                'color': alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=None)    
            } if not f1 else {
                'yOffset':alt.YOffset(f'{f1["col"]}:N', title=None, sort=f1["order"]), 
                'color': alt.Color(f'{f1["col"]}:N', scale=f1["colors"],
                                legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width)))
            }),
    )
    return plot

# %% ../nbs/03_plots.ipynb 25
# Make the likert bar pieces
def make_start_end(x,value_col,cat_col,cat_order):
    #print("######################")
    #print(value_col,cat_col)
    #print(x)
    if len(x) != len(cat_order):
        # Fill in missing rows with value zero so they would just be skipped
        x = pd.merge(pd.DataFrame({cat_col:cat_order}),x,on=cat_col,how='left').fillna({value_col:0})
    mid = len(x)//2

    x = x.sort_values(by=cat_col)
        
    if len(x)%2==1: # odd:
        scale_start=1.0
        x_mid = x.iloc[[mid],:]
        x_mid.loc[:,['start','end']] = -scale_start
        x_mid.loc[:,'end'] = -scale_start+x_mid[value_col]
        nonmid = [ i for i in range(len(x)) if i!=mid ]
        x_mid = [x_mid]
    else: # even - no separate mid
        nonmid = np.arange(len(x))
        x_mid = []
    
    x_other = x.iloc[nonmid,:].copy()
    x_other.loc[:,'end'] = x_other[value_col].cumsum() - x_other[:mid][value_col].sum()
    x_other.loc[:,'start'] = (x_other[value_col][::-1].cumsum()[::-1] - x_other[mid:][value_col].sum())*-1
    res = pd.concat([x_other] + x_mid).dropna(subset=[value_col]) # drop any na rows added in the filling in step
    return res

@stk_plot('likert_bars', data_format='longform', draws=False, requires=[{'likert':True}], n_facets=(1,3), sort_numeric_first_facet=True, priority=50)
def likert_bars(data, value_col='value', facets=[],  tooltip=[], outer_factors=[],width=800):
    # First facet is likert, second is labeled question, third is offset. Second is better for question which usually goes last, hence reorder
    if len(facets)==1: # Create a dummy second facet
        facets.append({ 'col': 'question', 'order': [facets[0]['col']], 'colors': alt.Undefined })
        data['question'] = facets[0]['col']
    if len(facets)>=3: f0, f1, f2 = facets[0], facets[2], facets[1]
    elif len(facets)==2: f0, f1, f2 = facets[0], facets[1], None

    gb_cols = outer_factors+[f["col"] for f in facets[1:]] # There can be other extra cols (like labels) that should be ignored
    options_cols = list(data[f0["col"]].dtype.categories) # Get likert scale names
    bar_data = data.groupby(gb_cols, group_keys=False, observed=False)[data.columns].apply(make_start_end, value_col=value_col,cat_col=f0["col"],cat_order=f0["order"],include_groups=False)
    
    plot = alt.Chart(bar_data).mark_bar() \
        .encode(
            x=alt.X('start:Q', axis=alt.Axis(title=None, format = '%')),
            x2=alt.X2('end:Q'),
            y=alt.Y(f'{f1["col"]}:N', axis=alt.Axis(title=None, offset=5, ticks=False, minExtent=60, domain=False), sort=f1["order"]),
            tooltip=tooltip,
            color=alt.Color(
                f'{f0["col"]}:N',
                legend=alt.Legend(
                    title='Response',
                    orient='bottom',
                    columns=estimate_legend_columns_horiz(f0['order'],width,extra_text=f1['order'])
                    ),
                scale=f0["colors"],
            ),
            **({ 'yOffset':alt.YOffset(f'{f2["col"]}:N', title=None, sort=f2["order"]),
                 #'strokeWidth': alt.value(3)
               } if f2 else {})
        )
    return plot

# %% ../nbs/03_plots.ipynb 29
# Calculate the bandwidth for KDE
def kde_bw(ar):
    # Lower-bound silverman by min_diff to smooth out categorical density plots
    return max(silvermans_rule(ar) or 0.0, 0.75*min_diff(ar[:,0]))

# Calculate KDE ourselves using a fast libary. This gets around having to do sampling which is unstable
def kde_1d(vc, value_col, ls, scale=False, bw=None):
    ar = vc.to_numpy()
    if bw is None: bw = kde_bw(ar) # This can be problematic in small segments, so best calculated globally
    y =  FFTKDE(kernel='gaussian',bw=bw).fit(ar).evaluate(ls)
    if scale: y*=len(vc)
    return pd.DataFrame({'density': y, value_col: ls})

@stk_plot('density', factor_columns=3, draws=True, aspect_ratio=(1.0/1.0), n_facets=(0,1), args={'stacked':'bool', 'bw':'float'}, no_question_facet=True)
def density(data, value_col='value', facets=[], tooltip=[], outer_factors=[], stacked=False, bw=None, width=800):
    f0 = facets[0] if len(facets)>0 else None
    gb_cols = [ c for c in outer_factors+[f['col'] for f in facets] if c is not None ] # There can be other extra cols (like labels) that should be ignored

    # Filter out extreme outliers (one thousandth on each side). 
    # Because at 100k+, these get very extreme even for normal distributions
    lims = list(data[value_col].quantile([.005,0.995]))
    data = data[(data[value_col]>=lims[0]) & (data[value_col]<=lims[1])]
    
    ls = np.linspace(data[value_col].min()-1e-10,data[value_col].max()+1e-10,100)
    if bw is None: bw = kde_bw(data[[value_col]].sample(10000,replace=True).to_numpy()) # Can get slow for large data otherwise
    ndata = gb_in_apply(data,gb_cols,cols=[value_col],fn=kde_1d,value_col=value_col,ls=ls,scale=stacked,bw=bw).reset_index()

    if f0: selection = alt.selection_point(fields=[f0["col"]], bind='legend')

    if stacked: 
        if f0:
            ldict = dict(zip(f0["order"], reversed(range(len(f0["order"])))))
            ndata.loc[:,'order'] = ndata[f0["col"]].astype('object').replace(ldict).astype('int')
        
        ndata['density'] /= len(data)
        plot=alt.Chart(ndata).mark_area(interpolate='natural').encode(
                x=alt.X(f"{value_col}:Q"),
                y=alt.Y('density:Q',axis=alt.Axis(title=None, format = '%'),stack='zero'),
                tooltip = tooltip[1:],
                **({'fill': alt.Fill(f'{f0["col"]}:N', scale=f0["colors"], 
                            legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f0["order"],width))), 
                    'order': alt.Order('order:O'), 'opacity': alt.condition(selection, alt.value(1), alt.value(0.15))
                    } if f0 else {})
            )
    else:
        plot = alt.Chart(ndata).mark_line().encode(
                x=alt.X(f"{value_col}:Q"),
                y=alt.Y('density:Q',axis=alt.Axis(title=None, format = '%')),
                tooltip = tooltip[1:],
                **({'color': alt.Color(f'{f0["col"]}:N', scale=f0["colors"], 
                    legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f0["order"],width))),
                    'order': alt.Order('order:O'), 'opacity': alt.condition(selection, alt.value(1), alt.value(0.15))
                    } if f0 else {})
            )

    if f0: plot = plot.add_params(selection)

    return plot

# Also create a raw version for the same plot 
stk_plot('density-raw', data_format="raw", factor_columns=3, aspect_ratio=(1.0/1.0), n_facets=(0,1), args={'stacked':'bool', 'bw':'float'}, no_question_facet=True, priority=0)(density)

# %% ../nbs/03_plots.ipynb 31
@stk_plot('violin', n_facets=(1,2), draws=True, as_is=True, args={'bw':'float'})
def violin(data, value_col='value', facets=[], tooltip=[], outer_factors=[], bw=None, width=800):
    f0, f1 = facets[0], facets[1] if len(facets)>1 else None
    gb_cols = outer_factors + [ f['col'] for f in facets ] # There can be other extra cols (like labels) that should be ignored
    
    ls = np.linspace(data[value_col].min()-1e-10,data[value_col].max()+1e-10,200)
    if bw is None: bw = kde_bw(data[[value_col]].sample(10000,replace=True).to_numpy())
    ndata = gb_in_apply(data,gb_cols,cols=[value_col],fn=kde_1d,value_col=value_col,ls=ls,scale=True,bw=bw).reset_index()
    
    if f1:
        ldict = dict(zip(f1["order"], reversed(range(len(f1["order"])))))
        ndata.loc[:,'order'] = ndata[f1["col"]].astype('object').replace(ldict).astype('int')

    ndata['density'] /= len(data)
    plot=alt.Chart(ndata).mark_area(interpolate='natural').encode(
            x=alt.X(f"{value_col}:Q"),
            y=alt.Y('density:Q',axis=alt.Axis(title=None, labels=False, values=[0], grid=False),stack='center'),
            row=alt.Row(f'{f0["col"]}:N',header=alt.Header(orient='top',title=None),spacing=5,sort=f0["order"]),
            tooltip = tooltip[1:],
            #color=alt.Color(f'{question_col}:N'),
            **({'color': alt.Color(f'{f1["col"]}:N', scale=f1["colors"], legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width))), 'order': alt.Order('order:O')} if f1 else 
               {'color': alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=None)})
        ).properties(width=width,height=70)

    return plot

# Also create a raw version for the same plot 
stk_plot('violin-raw', data_format='raw', n_facets=(1,2), as_is=True, args={'bw':'float'})(violin)

# %% ../nbs/03_plots.ipynb 33
# Cluster-based reordering
def cluster_based_reorder(X):
    pd = sp.spatial.distance.pdist(X)#,metric='cosine')
    return hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(hierarchy.ward(pd), pd))

@stk_plot('matrix', data_format='longform', aspect_ratio=(1/0.8), n_facets=(2,2), args={'reorder':'bool', 'log_colors':'bool'})
def matrix(data, value_col='value', facets=[], val_format='%', reorder=False, log_colors=False, tooltip=[]):
    f0, f1 = facets[0], facets[1]
    
    fcols = [c for c in data.columns if c not in [value_col,f0["col"]]]
    if len(fcols)==1 and reorder: # Reordering only works if no external facets
        X = data.pivot(columns=f1["col"],index=f0["col"]).to_numpy()
        f0["order"] = np.array(f0["order"])[cluster_based_reorder(X)]
        f1["order"] = np.array(f1["order"])[cluster_based_reorder(X.T)]
    
    if log_colors:
        data['val_log'] = np.log(data[value_col])
        data['val_log'] -= data['val_log'].min() # Keep it all positive 
        scale_v = 'val_log'
    else: scale_v = value_col

    # Find max absolute value to keep color scale symmetric
    mi, ma = data[scale_v].min(),data[scale_v].max() 
    dmax = max(-mi,ma)

    if mi<0: scale, smid, swidth = { 'scheme':'redyellowgreen', 'domainMid':0, 'domainMin':-dmax, 'domainMax':dmax }, 0, 2*dmax
    else: scale, smid, swidth = { 'scheme': 'yellowgreen', 'domainMin': 0, 'domainMax':dmax }, 0, 2*dmax, #dmax/2, dmax 

    # Draw colored boxes
    plot = alt.Chart(data).mark_rect().encode(
            x=alt.X(f'{f1["col"]}:N', title=None, sort=f1["order"]),
            y=alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
            color=alt.Color(f'{scale_v}:Q', scale=alt.Scale(**scale), legend=(alt.Legend(title=None) if not log_colors else None) ),
            tooltip=tooltip,
        )
    
    # Add in numerical values
    if len(f1["order"])<20: # only if we have less than 20 columns
        text = plot.mark_text().encode(
            text=alt.Text(f'{value_col}:Q', format=val_format),
            color=alt.condition(
                (alt.datum[f'{scale_v}']-smid)**2 > (0.25*swidth)**2,
                alt.value('white'),
                alt.value('black')
            ),
            tooltip=tooltip
        )
        plot += text
        
    return plot

# %% ../nbs/03_plots.ipynb 37
@stk_plot('corr_matrix', data_format='raw', aspect_ratio=(1/0.8), n_facets=(1,1))
def corr_matrix(data, value_col='value', facets=[], val_format='%', reorder=False, tooltip=[]):
    if 'id' not in data.columns: raise Exception("Corr_matrix only works for groups of continuous variables")

    # id is required to match the rows for correllations
    cm = data.pivot_table(index='id',columns=facets[0]['col'],values=value_col,observed=False).corr().reset_index(names='index')
    cm_long = cm.melt(id_vars=['index'],value_vars=cm.columns, var_name=facets[0]['col'], value_name=value_col)

    order = facets[0]['order'] 
    lower_tri = (cm_long['index'].map(lambda x: order.index(x)).astype(int)>cm_long[facets[0]['col']].map(lambda x: order.index(x)).astype(int))
    cm_long = cm_long[lower_tri]
    
    return matrix(cm_long, value_col=value_col, facets=[{'col':'index','order':facets[0]['order']},{'col':facets[0]['col'],'order':facets[0]['order']}], val_format=val_format,
                  tooltip=[alt.Tooltip(f'{value_col}:Q'),alt.Tooltip('index:N'),alt.Tooltip(f"{facets[0]['col']}:N")])

# %% ../nbs/03_plots.ipynb 39
@stk_plot('lines',data_format='longform', draws=False, requires=[{},{'ordered':True}], n_facets=(2,2), args={'smooth':'bool'})
def lines(data, value_col='value', facets=[], smooth=False, width=800, tooltip=[], val_format='.2f',):
    f0, f1 = facets[0], facets[1]
    if smooth:
        smoothing = 'basis'
        points = 'transparent'
    else:
        smoothing = 'natural'
        points = True
    plot = alt.Chart(data).mark_line(point=points, interpolate=smoothing).encode(
        alt.X(f'{f1["col"]}:N', title=None, sort=f1["order"]),
        alt.Y(f'{value_col}:Q', axis=alt.Axis(format=val_format)),
        tooltip=tooltip,
        color=alt.Color(f'{f0["col"]}:N', scale=f0["colors"], sort=f0["order"],
                        legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f0["order"],width)))
    )
    return plot

# %% ../nbs/03_plots.ipynb 41
def draws_to_hdis(data,vc,hdi_vals):
    gbc = [ c for c in data.columns if c not in [vc,'draw'] ]
    ldfs = []
    for hdiv in hdi_vals:
        ldf_v = data.groupby(gbc,observed=False)[vc].apply(lambda s: pd.Series(list(az.hdi(s.to_numpy(),hdi_prob=hdiv)),index=['lo','hi'])).reset_index()
        ldf_v['hdi']=hdiv
        ldfs.append(ldf_v)
    ldf = pd.concat(ldfs).reset_index(drop=True)
    df = ldf.pivot(index=gbc+['hdi'], columns=ldf.columns[-3],values=vc).reset_index()
    return df

@stk_plot('lines_hdi',data_format='longform', draws=True, requires=[{},{'ordered':True}], n_facets=(2,2), args={'hdi1':'float','hdi2':'float'})
def lines_hdi(data, value_col='value', facets=[], width=800, tooltip=[], val_format='.2f', hdi1=0.94, hdi2=0.5):
    f0, f1 = facets[0], facets[1]
    
    hdf = draws_to_hdis(data,value_col,[hdi1,hdi2])
    # Draw them in reverse order so the things that are first (i.e. most important) are drawn last (i.e. on top of others)
    # Also draw wider hdi before the narrower
    hdf.sort_values([f0["col"],'hdi'],ascending=[False,False],inplace=True)

    plot = alt.Chart(hdf).mark_area(interpolate='basis').encode(
        alt.X(f'{f1["col"]}:O', title=None, sort=f1["order"]),
        y=alt.Y('lo:Q',
            axis=alt.Axis(format=val_format),
            title=value_col
            ),
        y2=alt.Y2('hi:Q'),
        color=alt.Color(
            f'{f0["col"]}:N',
            sort=f0["order"],
            scale=f0["colors"]
            ),
        opacity=alt.Opacity('hdi:N',legend=None,scale=to_alt_scale({0.5:0.75,0.94:0.25})),
        tooltip=[
            alt.Tooltip('hdi:N', title='HDI', format='.0%'),
            alt.Tooltip('lo:Q', title='HDI lower', format=val_format),
            alt.Tooltip('hi:Q', title='HDI upper', format=val_format),] + tooltip[1:]
        )
    return plot

# %% ../nbs/03_plots.ipynb 43
@stk_plot('area_smooth',data_format='longform', draws=False, nonnegative=True, requires=[{},{'ordered':True}], n_facets=(2,2))
def area_smooth(data, value_col='value', facets=[], width=800, tooltip=[]):
    f0, f1 = facets[0], facets[1]
    ldict = dict(zip(f0["order"], range(len(f0["order"]))))
    data.loc[:,'order'] = data[f0["col"]].astype('object').replace(ldict).astype('int')
    plot=alt.Chart(data
        ).mark_area(interpolate='natural').encode(
            x=alt.X(f'{f1["col"]}:O', title=None, sort=f1["order"]),
            y=alt.Y(f'{value_col}:Q', title=None, stack='normalize',
                 scale=alt.Scale(domain=[0, 1]), axis=alt.Axis(format='%')
                 ),
            order=alt.Order('order:O'),
            color=alt.Color(f'{f0["col"]}:N',
                legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f0["order"],width)),
                sort=f0["order"], scale=f0["colors"]
                ),
            tooltip=tooltip
        )
    return plot

# %% ../nbs/03_plots.ipynb 45
def likert_aggregate(x, cat_col, cat_order, value_col):
    
    cc, vc = x[cat_col], x[value_col]
    cats = cat_order
    
    mid, odd = len(cats)//2, len(cats)%2
    
    nonmid_sum = vc[cc !=  cats[mid]].sum() if odd else vc.sum()
    
    #print(len(x),x.columns,x.head())
    pol = ( np.minimum(
                vc[cc.isin(cats[:mid])].sum(),
                vc[cc.isin(cats[mid+odd:])].sum()
            ) / nonmid_sum )

    rad = ( vc[cc.isin([cats[0],cats[-1]])].sum() /
            nonmid_sum )

    rel = 1.0-nonmid_sum/vc.sum()
    
    return pd.Series({ 'polarisation': pol, 'radicalisation':rad, 'relevance':rel})

@stk_plot('likert_rad_pol',data_format='longform', requires=[{'likert':True}], args={'normalized':'bool'}, n_facets=(1,2))
def likert_rad_pol(data, value_col='value', facets=[], normalized=True, width=800, outer_factors=[], tooltip=[]):
    f0, f1 = facets[0], facets[1] if len(facets)>1 else None
    #gb_cols = list(set(data.columns)-{ f0["col"], value_col }) # Assume all other cols still in data will be used for factoring
    gb_cols = outer_factors + [f['col'] for f in facets[1:]] # There can be other extra cols (like labels) that should be ignored
    likert_indices = gb_in_apply(data,gb_cols,likert_aggregate,cat_col=f0["col"],cat_order=f0["order"],value_col=value_col).reset_index()
    
    if normalized and len(likert_indices)>1: likert_indices.loc[:,['polarisation','radicalisation']] = likert_indices[['polarisation','radicalisation']].apply(sps.zscore)
    
    if f1: selection = alt.selection_point(fields=[f1["col"]], bind='legend')
        
    plot = alt.Chart(likert_indices).mark_point().encode(
        x=alt.X('polarisation:Q'),
        y=alt.Y('radicalisation:Q'),
        size=alt.Size('relevance:Q', legend=None, scale=alt.Scale(range=[100, 500])),
        #stroke=alt.value('#777'),
        tooltip=[
            alt.Tooltip('radicalisation:Q', format='.2'),
            alt.Tooltip('polarisation:Q', format='.2'),
            alt.Tooltip('relevance:Q', format='.2')
        ] + tooltip[2:],
        **({'color': alt.Color(f'{f1["col"]}:N', scale=f1["colors"], 
                               legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width))),
            'opacity':alt.condition(selection, alt.value(1), alt.value(0.15)),
            } if f1 else {})
        )
    if f1: plot = plot.add_params(selection)
    
    return plot

# %% ../nbs/03_plots.ipynb 47
@stk_plot('barbell', data_format='longform', draws=False, n_facets=(2,2))
def barbell(data, value_col='value', facets=[], filtered_size=1, val_format='%', width=800, tooltip=[]):
    f0, f1 = facets[0], facets[1]
    
    chart_base = alt.Chart(data).encode(
        alt.X(f'{value_col}:Q', title=None, axis=alt.Axis(format=val_format)),
        alt.Y(f'{f0["col"]}:N', title=None, sort=f0["order"]),
        tooltip=tooltip
    )

    chart = chart_base.mark_line(color='lightgrey', size=1, opacity=1.0).encode(
        detail=f'{f0["col"]}:N'
    )
    selection = alt.selection_point(fields=[f1["col"]], bind='legend')

    chart += chart_base.mark_point(
        size=50,
        opacity=1,
        filled=True
    ).encode(
        color=alt.Color(f'{f1["col"]}:N',
            #legend=alt.Legend(orient='right', title=None),
            legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f1["order"],width)),
            scale=f1["colors"],
            sort=f1["order"]
        ),
        opacity=alt.condition(selection, alt.value(1), alt.value(0.15)),
    ).add_params(
        selection
    )#.interactive()
    
    return chart

# %% ../nbs/03_plots.ipynb 50
@stk_plot('geoplot', data_format='longform', factor_columns=2, n_facets=(1,1), requires=[{'topo_feature':'pass'}], no_faceting=True, aspect_ratio=(4.0/3.0), no_question_facet=True, args={'separate_axes':'bool'})
def geoplot(data, topo_feature, value_col='value', facets=[], val_format='.2f', tooltip=[],
                separate_axes=False, outer_factors=[], outer_colors={}, value_range=None):
    f0 = facets[0]

    json_url, json_meta, json_col = topo_feature
    if json_meta == 'geojson':
        source = alt.Data(url=json_url, format=alt.DataFormat(property='features',type='json'))
    else:
        source = alt.topo_feature(json_url, json_meta)

    lmi,lma = data[value_col].min(),data[value_col].max() 
    mi, ma = value_range if value_range and not separate_axes else (lmi,lma)
    
    # Only show maximum on legend if min and max too close together
    legend_vals = [lmi,lma] if (lma-lmi)/(ma-mi) > 0.5 else [lma]
    rel_range = [(lmi-mi)/(ma-mi), (lma-mi)/(ma-mi)]

    ofv = data[outer_factors[0]].iloc[0] if outer_factors else None
    # If colors provided, create a gradient based on that
    if (outer_factors and outer_colors and  
        data[outer_factors[0]].nunique() == 1 and
        ofv in outer_colors):
        
        grad = gradient_from_color(outer_colors[ofv],range=rel_range)
        scale = { 'domain': [lmi,lma], 'range': grad}
    else: # Blues for pos, reds for neg, redblue for both

        # If axis spans both directions
        dmax = max(-mi,ma)
        if mi<0.0 and ma>0.0: rel_range = [lmi/dmax, lma/dmax] # Spans both sides, so scale by dmax
        elif ma<0.0: rel_range = [-rel_range[1], rel_range[0]] # Use negative part i.e. red scale
        
        grad = gradient_subrange(redblue_gradient, 11, range=rel_range)
        scale = { 'domain': [lmi,lma], 'range': grad}
        
        # if mi<0 and ma>0: scale = { 'scheme':'redblue', 'domainMid':0, 'domainMin':-dmax, 'domainMax':dmax, 'rangeMax': 0.1 }
        # elif ma<0: scale = { 'scheme': 'reds', 'reverse': True }#, 'domainMin': 0, 'domainMax':dmax }
        # else: scale = { 'scheme': 'blues' }#, 'domainMin': 0, 'domainMax':dmax }

    plot = alt.Chart(source).mark_geoshape(stroke='white', strokeWidth=0.1).transform_lookup(
        lookup = f"properties.{json_col}",
        from_ = alt.LookupData(
            data=data,
            key=f0["col"],
            fields=list(data.columns)
        ),
    ).encode(
        tooltip=tooltip, #[alt.Tooltip(f'properties.{json_col}:N', title=f1["col"]),
                #alt.Tooltip(f'{value_col}:Q', title=value_col, format=val_format)],
        color=alt.Color(
            f'{value_col}:Q',
            scale=alt.Scale(**scale), # To use color scale, consider switching to opacity for value
            legend=alt.Legend(format=val_format, title=None, orient='top-left',gradientThickness=6, 
                                values=[lmi,lma]),
        )
    ).project('mercator')
    return plot

# %% ../nbs/03_plots.ipynb 51
@stk_plot('geobest', data_format='longform', factor_columns=2, n_facets=(2,2), requires=[{},{'topo_feature':'pass'}], no_faceting=True,aspect_ratio=(4.0/3.0))
def geobest(data, topo_feature, value_col='value', facets=[], val_format='.2f', tooltip=[], width=800):
    f0, f1 = facets[0], facets[1]

    json_url, json_meta, json_col = topo_feature
    if json_meta == 'geojson':
        source = alt.Data(url=json_url, format=alt.DataFormat(property='features',type='json'))
    else:
        source = alt.topo_feature(json_url, json_meta)

    data = data.sort_values(value_col, ascending=False).drop_duplicates([f1['col']])
    colormap = f0['colors']

    plot = alt.Chart(source).mark_geoshape(stroke='white', strokeWidth=0.1).transform_lookup(
        lookup = f"properties.{json_col}",
        from_ = alt.LookupData(
            data=data,
            key=f1["col"],
            fields=list(data.columns)
        ),
    ).encode(
        tooltip=tooltip, #[alt.Tooltip(f'properties.{json_col}:N', title=f1["col"]),
                #alt.Tooltip(f'{value_col}:Q', title=value_col, format=val_format)],
        color=alt.Color(
            f'{f0['col']}:N',
            scale=f0['colors'],
            legend=alt.Legend(orient='top',columns=estimate_legend_columns_horiz(f0["order"],width)),
        )
    ).project('mercator')
    return plot

# %% ../nbs/03_plots.ipynb 55
# Assuming ns is ordered by unique row values, find the split points
def split_ordered(cvs):
    if len(cvs.shape)==1: cvs = cvs[:,None]
    unique_idxs = np.empty(len(cvs), dtype=np.bool_)
    unique_idxs[:1] = False
    unique_idxs[1:] = np.any(cvs[:-1, :] != cvs[1:, :], axis=-1)
    return np.arange(len(unique_idxs))[unique_idxs]

# Split a series of weights into groups of roughly equal sum
# This algorithm is greedy and does not split values but it is fast and should be good enough for most uses
def split_even_weight(ws, n):
    cws = np.cumsum(ws)
    cws = (cws/(cws[-1]/n)).astype('int')
    return (split_ordered(cws)+1)[:-1]

# %% ../nbs/03_plots.ipynb 57
def fd_mangle(vc, value_col, factor_col, n_points=10): 
    
    vc = vc.sort_values(value_col)
    
    ws = np.ones(len(vc))
    splits = split_even_weight(ws, n_points)
    
    ccodes, cats = vc[factor_col].factorize()
    
    ofreqs = np.stack([ np.bincount(g, weights=gw, minlength=len(cats))/gw.sum()
                        for g,gw in zip(np.split(ccodes,splits),np.split(ws,splits)) ],axis=0)
    
    df = pd.DataFrame(ofreqs, columns=cats)
    df['percentile'] = np.linspace(0,1,n_points)
    return df.melt(id_vars='percentile',value_vars=cats,var_name=factor_col,value_name='density')

@stk_plot('facet_dist', data_format='raw', factor_columns=3,aspect_ratio=(1.0/1.0), n_facets=(1,1), no_question_facet=True)
def facet_dist(data, value_col='value',facets=[], tooltip=[], outer_factors=[]):
    f0 = facets[0]
    gb_cols = [ c for c in outer_factors if c is not None ] # There can be other extra cols (like labels) that should be ignored
    ndata = gb_in_apply(data,gb_cols,cols=[value_col,f0["col"]],fn=fd_mangle,value_col=value_col,factor_col=f0["col"]).reset_index()
    plot=alt.Chart(ndata).mark_area(interpolate='natural').encode(
            x=alt.X(f"percentile:Q",axis=alt.Axis(format='%')),
            y=alt.Y('density:Q',axis=alt.Axis(title=None, format = '%'),stack='normalize'),
            tooltip = tooltip[1:],
            color=alt.Color(f'{f0["col"]}:N', scale=f0["colors"], legend=alt.Legend(orient='top')),
            #order=alt.Order('order:O')
    )

    return plot

# %% ../nbs/03_plots.ipynb 59
# Vectorized multinomial sampling. Should be slightly faster
def vectorized_mn(prob_matrix):
    s = prob_matrix.cumsum(axis=1)
    s = s/s[:,-1][:,None]
    r = np.random.rand(prob_matrix.shape[0])[:,None]
    return (s < r).sum(axis=1)

def linevals(vals, value_col, n_points, dim, cats, ccodes=None, ocols=None, boost_signal=True,gc=False, weights=None):
    ws = weights if weights is not None else np.ones(len(vals))
    
    order = np.lexsort((vals,ccodes)) if dim and gc else np.argsort(vals)
    splits = split_even_weight(ws[order], n_points)
    aer = np.array([ g.mean() for g in np.split(vals[order],splits) ])
    pdf = pd.DataFrame(aer, columns=[value_col])
    
    if dim:
        # Find the frequency of each category in ccodes
        osignal = np.stack([ np.bincount(g, weights=gw, minlength=len(cats))/gw.sum()
                            for g,gw in zip(np.split(ccodes[order],splits),np.split(ws[order],splits)) ],axis=0)
        
        ref_p = osignal.mean(axis=0)+1e-10
        np.random.seed(0) # So they don't change every re-render

        signal = osignal + 1e-10 # So even if values are zero, log and vectorized_mn would work
        if boost_signal: # Boost with K-L, leaving only categories that grew in probability boosted by how much they did
            klv = signal*(np.log(signal/ref_p[None,:]))
            signal = np.maximum(1e-10,klv)
            pdf['kld'] = np.sum(klv,axis=1)

        #pdf[dim] = cats[signal.apply(lambda r: np.random.multinomial(1,r/r.sum()).argmax() if r.sum()>0.0 else 0,axis=1)]
        cat_inds = vectorized_mn(signal)
        pdf[dim] = np.array(cats)[cat_inds]
        pdf['probability'] = osignal[np.arange(len(cat_inds)),cat_inds]

        #pdf[dim] = pdf[cats].idxmax(axis=1)
        #pdf['weight'] = np.minimum(pdf[cats].max(axis=1),pdf['matches'])

    pdf['pos'] = np.arange(0,1,1.0/len(pdf))
    
    if ocols is not None:
        for iv in ocols.index:
            pdf[iv] = ocols[iv]

    return pdf

# %% ../nbs/03_plots.ipynb 60
@stk_plot('ordered_population', data_format='raw', factor_columns=3, aspect_ratio=(1.0/1.0), plot_args={'group_categories':'bool'}, n_facets=(0,1), no_question_facet=True)
def ordered_population(data, value_col='value', facets=[], tooltip=[], outer_factors=[], group_categories=False):
    f0 = facets[0] if len(facets)>0 else None
    
    n_points, maxn = 200, 1000000
    
     # TODO: use weight if available. linevals is ready for it, just needs to be fed in. 
    
    # Sample down to maxn points if exceeding that
    if len(data)>maxn: data = data.sample(maxn,replace=False)
    
    data = data.sort_values(outer_factors)
    vals = data[value_col].to_numpy()

    if len(facets)>=1:
        fcol = f0["col"]
        cat_idx, cats = pd.factorize(data[f0["col"]])
        cats = list(cats)
    else:
        fcol = None
        cat_idx, cats = None, []
        
    if outer_factors:
        
        # This is optimized to not use pandas.groupby as it makes it about 2x faster - which is 2+ seconds with big datasets
        
        # Assume data is sorted by outer_factors, split vals into groups by them
        ofids = np.stack([ data[f].cat.codes.values for f in outer_factors ],axis=1)
        splits = split_ordered(ofids)        
        groups = np.split(vals,splits)
        cgroups = np.split(cat_idx,splits) if len(facets)>=1 else groups
        
        # Perform the equivalent of groupby
        ocols = data.iloc[[0]+list(splits)][outer_factors]
        tdf = pd.concat([linevals(g,value_col=value_col,dim=fcol, ccodes=gc, cats=cats, n_points=n_points,ocols=ocols.iloc[i,:],gc=group_categories) 
                         for i,(g,gc) in enumerate(zip(groups,cgroups))])

        #tdf = data.groupby(outer_factors,observed=True).apply(linevals,value_col=value_col,dim=fcol,cats=cats,n_points=n_points,gc=group_categories,include_groups=False).reset_index()
    else:
        tdf = linevals(vals,value_col=value_col,dim=fcol,ccodes=cat_idx,cats=cats,n_points=n_points, gc=group_categories)
        #tdf = linevals(data,value_col=value_col,cats=cats,dim=fcol,n_points=n_points,gc=group_categories)
        
    #if boost_signal:
    #    tdf['matches'] = np.minimum(tdf['matches'],tdf['kld']/tdf['kld'].quantile(0.75))

    base = alt.Chart(tdf).encode(
        x=alt.X('pos:Q',
            title="",
            axis=alt.Axis(
                labels=False,
                ticks=False,
                #grid=False
            )
        )
    )
    #selection = alt.selection_multi(fields=[dim], bind='legend')
    line = base.mark_circle(size=10).encode(
        y=alt.Y(f"{value_col}:Q",impute={'value':None}, title='', axis=alt.Axis(grid=True)),
        #opacity=alt.condition(selection, alt.Opacity("matches:Q",scale=None), alt.value(0.1)),
        color=alt.Color(
            f'{f0["col"]}',
            sort=f0["order"],
            scale=f0["colors"]
            ) if len(facets)>=1 else alt.value('red'),
        tooltip=tooltip+([alt.Tooltip('probability:Q',format='.1%',title='category prob.')] if len(facets)>=1 else [])
    )#.add_selection(selection)


    rule = alt.Chart().mark_rule(color='red', strokeDash=[2, 3]).encode(
        y=alt.Y('mv:Q')
    ).transform_joinaggregate(
        mv = f'mean({value_col}):Q',
        groupby=outer_factors
    )

    plot = alt.layer(
        rule,
        line,
        data=tdf,
    )
    return plot

# %% ../nbs/03_plots.ipynb 62
@stk_plot('marimekko', data_format='longform', draws=False, group_sizes=True, args={'separate':'bool'}, n_facets=(2,2))
def marimekko(data, value_col='value', facets=[], val_format='%', width=800, tooltip=[], outer_factors=[], separate=False):
    f0, f1 = facets[0], facets[1]

    #xcol, ycol, ycol_scale = f1["col"], f0["col"], f0["colors"]
    xcol, ycol, ycol_scale = f0["col"], f1["col"], f1["colors"]
     
    data['w'] = data['group_size']*data[value_col]
    data.sort_values([xcol,ycol],ascending=[True,False],inplace=True)

    if separate: # Split and center each ycol group so dynamics can be better tracked for all of them
        ndata = data.groupby(outer_factors+[xcol],observed=False)[[ycol,value_col,'w']].apply(lambda df: pd.DataFrame({ ycol: df[ycol], 'yv': df['w']/df['w'].sum(), 'w': df['w']})).reset_index()
        ndata = ndata.merge(ndata.groupby(outer_factors + [ycol],observed=True)['yv'].max().rename('ym').reset_index(),on=outer_factors + [ycol]).fillna({'ym':0.0})
        ndata = ndata.groupby(outer_factors+[xcol],observed=False)[[ycol,'w','yv','ym']].apply(lambda df: pd.DataFrame({ ycol: df[ycol], 'yv': df['yv'], 'w': df['w'].sum(),
                                                                                                                        'y1': (df['ym'].cumsum()- df['ym']/2 - df['yv']/2)/df['ym'].sum(),
                                                                                                                        'y2': (df['ym'].cumsum()- df['ym']/2 + df['yv']/2)/df['ym'].sum(), })).reset_index()
    else: # Regular marimekko
        ndata = data.groupby(outer_factors+[xcol],observed=False)[[ycol,value_col,'w']].apply(lambda df: pd.DataFrame({ ycol: df[ycol], 'w': df['w'].sum(),
                                                                                                                       'yv': df['w']/df['w'].sum(), 
                                                                                                                       'y2': df['w'].cumsum()/df['w'].sum()})).reset_index()
        ndata['y1'] = ndata['y2']-ndata['yv']
    
    ndata = ndata.groupby(outer_factors+[ycol],observed=False)[[xcol,'yv','y1','y2','w']].apply(lambda df: pd.DataFrame({ xcol: df[xcol], 'xv': df['w']/df['w'].sum(), 'x2': df['w'].cumsum()/df['w'].sum(), 'yv':df['yv'], 'y1':df['y1'], 'y2':df['y2']})).reset_index()
    ndata['x1'] = ndata['x2']-ndata['xv']
    

    #selection = alt.selection_point(fields=[yvar], bind="legend")
    STROKE = 0.25
    plot = alt.Chart(ndata).mark_rect(
            strokeWidth=STROKE,
            stroke="white",
            xOffset=STROKE / 2,
            x2Offset=STROKE / 2,
            yOffset=STROKE / 2,
            y2Offset=STROKE / 2,
        ).encode(
            x=alt.X(
                "x1:Q",
                axis=alt.Axis(
                    zindex=1, format="%", title=[f"{xcol} (% of total)", " "], grid=False
                ),
                scale=alt.Scale(domain=[0, 1]),
            ),
            x2="x2:Q",
            y=alt.Y(
                "y1:Q",
                axis=alt.Axis(
                    zindex=1, format="%", title=f"{ycol} (% of total)", grid=False, labels=not separate
                    ),
                scale=alt.Scale(domain=[0, 1])
            ),
            y2="y2:Q",
            color=alt.Color(
                f"{ycol}:N",
                legend=alt.Legend(title=None, symbolStrokeWidth=0), #title=f"{yvar}"),
                scale=ycol_scale,
            ),
            tooltip=[
                alt.Tooltip("yv:Q", title=f'{ycol} proportion', format='.1%' ),
                alt.Tooltip("xv:Q", title=f'{xcol} proportion', format='.1%' ),
            ]+tooltip[1:],
            #opacity=alt.condition(selection, alt.value(1), alt.value(0.3)),
        )
        #.add_params(selection)
    
    return plot

# %% ../nbs/03_plots.ipynb 68
# Beta binomial fitting using PyMC with a partially pooled model
use_partial_pooling = False # bypass pymc and just use method of moments

# Estimate boxplot for p by fitting beta binomial distribution
def estimate_box_from_beta_pk(row,beta_max_k=1000):
    p,k = row['p'],row['k']
    qs = sps.beta.ppf([0.03,0.25,0.5,0.75,0.97],p*k,(1-p)*k)*100

    return pd.Series({
        'tmin': qs[0],
        'min': qs[0],
        'q1': qs[1],
        'median': qs[2],
        'q3': qs[3],
        'max': qs[4],
        'tmax': qs[4]
    })

# Beta-binomial partially pooled fitting function
bb_pp_ff = None
def compile_bb_pp_ff():
    import pytensor, pytensor.tensor as pt
    import pymc as pm

    log_pv = pt.dvector('log_pv')
    pv = pt.exp(log_pv)
    mm,s,mv = pv[0],pv[1],pt.minimum(pv[2:],10000) # Thresholding is important to avoid large mv-s for betabinomial

    c,n = pt.dmatrix('c'), pt.dmatrix('n')
    p = (c.sum(axis=0)+1e-3)/(n.sum(axis=0)+2e-3)
    lp = (
        pm.BetaBinomial.logp(value=c,n=n,alpha=p*mv,beta=(1-p)*mv).sum() + #+ #p*mv,beta=(1-p)*mv).sum() +
        pm.LogNormal.logp(mv, mu=mm, sigma=s).sum() + # Partially pooled means
        pm.Normal.logp(mm, mu=4, sigma=3).sum() + # Prior for mean 
        pm.Gamma.logp(s, alpha=2, scale=1).sum() # Prior for scale
    )

    lpf = pytensor.function([log_pv,c,n], lp, on_unused_input='warn')
    lpf_jac = pytensor.function([log_pv,c,n], pytensor.gradient.jacobian(lp,[log_pv]))
    return lambda v,c,n: (-lpf(v,c,n), -np.clip(lpf_jac(v,c,n)[0],-1e3,1e3)) # Clipping is importatnt to avoid exploding gradients

def beta_binomial_fit(data,f_cols,beta_max_k=5000):
    from scipy.optimize import minimize
    global bb_pp_ff

    data.to_csv('~/boxplot_data.csv') # If needed for debugging

    cdf = data.pivot(index='draw', columns=f_cols, values='count').fillna(0)
    cv, res_inds = cdf.to_numpy(), cdf.columns
    nv = data.pivot(index='draw', columns=f_cols, values='group_size').fillna(0).to_numpy()

    # Calculate method-of-moments estimates for k    
    m, m2 = cv.mean(axis=0), (cv**2).mean(axis=0)
    n, n2 = nv.mean(axis=0), (nv**2).mean(axis=0)
    p = m/(n+1e-5) # p = a/(a+b)
    kmm = (p*n2 - m2)/(m2 - n2*p*p - p*(1-p)*n + 1e-5) # k = a+b
    kmm[(kmm<=0) | (kmm>beta_max_k)] = beta_max_k

    if use_partial_pooling:

        if bb_pp_ff is None: bb_pp_ff = compile_bb_pp_ff()

        res = minimize(bb_pp_ff, [3, 1.5] + list(np.log(kmm)), args=(cv,nv), jac=True, method='L-BFGS-B')
        if not res.success: raise ValueError(f"Beta-binomial fit failed: {res.message}")

        print(np.exp(res.x[:2]), np.exp(res.x[2:]).mean())
    
    rdf = pd.DataFrame({ 'p':p,'k':kmm },index=res_inds).reset_index()

    return rdf.groupby(f_cols,observed=True)[['p','k']].apply(estimate_box_from_beta_pk).reset_index()

