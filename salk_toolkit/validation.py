"""Builds up to a function that validates metadata annotations"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_validation.ipynb.

# %% auto 0
__all__ = ['DataDescription', 'DataSpec', 'MergeSpec', 'FileDesc', 'DataMeta', 'hard_validate', 'soft_validate',
           'SingleMergeSpec', 'smc_ensure_list']

# %% ../nbs/06_validation.ipynb 3
import numpy as np
from typing import *
from pydantic import BaseModel, ConfigDict, model_validator, BeforeValidator
from pydantic_extra_types.color import Color
from salk_toolkit.utils import replace_constants

# %% ../nbs/06_validation.ipynb 5
# Define a new base that is more strict towards unknown inputs
class PBase(BaseModel):
    model_config = ConfigDict(extra='forbid', protected_namespaces=(),arbitrary_types_allowed=True)

# %% ../nbs/06_validation.ipynb 6
class ColumnMeta(PBase):

    # Type specification
    continuous: bool = False # For real numbers
    datetime: bool = False # For datetimes
    categories: Optional[Union[List,Literal['infer']]] = None # For categoricals: List of categories or 'infer'
    ordered: bool = False # If categorical data is ordered

    # Transformations
    translate: Optional[Dict] = None # Translate dict applied to categories
    transform: Optional[str] = None # Transform function in python code, applied after translate
    translate_after: Optional[Dict] = None # Same as translate, but applied after transform

    # Model extras
    modifiers: Optional[List[str]] = None # List of columns that are meant to modify the responses on this col -> private_inputs
    nonordered: Optional[List] = None # List of categories that are outside the order (Like "Don't know") -> nonordered in ordered_outputs

    # Plot pipeline extras
    label: Optional[str] = None # Longer description of the column for tooltips
    labels: Optional[Dict[str,str]] = None # Dict matching categories to labels
    groups: Optional[Dict[str,List[str]]] = None # Dict of lists of category values defining groups for easier filtering
    colors: Optional[Dict[str,Color]] = None # Dict matching colors to categories
    num_values: Optional[List[Union[float,None]]] = None # For categoricals - how to convert the categories to numbers
    val_format: Optional[str] = None # Format string for the column values - only used with continuous display
    val_range: Optional[Tuple[float,float]] = None # Range of possible values for continuous variables - used for filter bounds etc

    likert: bool = False # For ordered categoricals - if they are likert-type (i.e. symmetric around center)
    neutral_middle: Optional[str] = None # For ordered categoricals - if there is a neutral category, which one should be in the middle?

    topo_feature: Optional[Tuple[str,str,str]] = None # Link to a geojson/topojson [url,type,col_name inside geodata]
    electoral_system: Optional[Dict] = None # Information about electoral system (TODO: spec it out)
    mandates: Optional[Dict] = None # Mandate count mapping for the electoral system

    @model_validator(mode='after')
    def check_categorical(self) -> Self:
        if self.categories is None:
            #if not self.continuous and not self.datetime:
            #    raise ValueError('Column type undefined: need either categories, continuous or datetime')
            for f in ['ordered','groups','colors','num_values','likert','topo_feature']:
                if getattr(self,f):
                    raise ValueError(f'Field {f} only makes sense for categorical columns {getattr(self,f)}')
        else: # Is categorical
            if not self.ordered:
                for f in ['likert']: # ['num_values'] can be situationally useful in non-ordered settings
                    if getattr(self,f):
                        raise ValueError(f'Field {f} only makes sense for ordered categorical columns')
        return self


# This is for the block-level 'scale' group - basically same as ColumnMeta but with a few extras
class BlockScaleMeta(ColumnMeta):

    # Only useful in 'scale' block
    col_prefix: Optional[str] = None # If column name should have the prefix added. Usually used in scale block
    question_colors: Optional[Dict[str,Color]] = None # Dict mapping columns to different colors


# %% ../nbs/06_validation.ipynb 7
# Transform the column tuple to (new name, old name, meta) format
def cspec(tpl):
    if type(tpl)==list:
        cn = tpl[0] # column name
        sn = tpl[1] if len(tpl)>1 and type(tpl[1])==str else cn # source column
        o_cd = tpl[2] if len(tpl)==3 else tpl[1] if len(tpl)==2 and type(tpl[1])==dict else {} # metadata
    else:
        cn = sn = tpl
        o_cd = {}
    return [cn,sn,o_cd]

# Transform list to dict for better error readability
def cs_lst_to_dict(lst):
    return { cn: [ocn,meta] for cn,ocn,meta in map(cspec,lst) }

ColSpec = Annotated[Dict[str,Tuple[str,ColumnMeta]],BeforeValidator(cs_lst_to_dict)]

# %% ../nbs/06_validation.ipynb 8
class ColumnBlockMeta(PBase):
    name: str # Name of the block
    scale: Optional[BlockScaleMeta] = None # Shared column meta for all columns inside the block
    
    # List of columns, potentially with their ColummnMetas
    columns: ColSpec

    subgroup_transform: Optional[str] = None # A block-level transform performed after column level transformations

    # Block level flags
    generated: bool = False # This block is for data that is generated, i.e. not initially in the file. 
    hidden: bool = False # Use this to hide the block in explorer.py
    virtual: bool = False # This block is virtual (i.e. just used in display. NB! Ignores all transformations on values)

# %% ../nbs/06_validation.ipynb 9
# Again, convert list to dict for easier debugging in case errors get thrown
def cb_lst_to_dict(lst): return { c['name']:c for c in lst }
BlockSpec = Annotated[Dict[str,ColumnBlockMeta],BeforeValidator(cb_lst_to_dict)]

# %% ../nbs/06_validation.ipynb 10
class FileDesc(PBase):
    file: str
    opts: Optional[Dict] = None

class DataMeta(PBase):

    #########################################################
    # Metadata
    #########################################################

    description: Optional[str] = None # Description of the data
    source: Optional[str] = None # Source of the data
    restrictions: Optional[str] = None # Restrictions on the data use

    collection_start: Optional[str] = None # Date in a way pd.to_datetime can parse it
    collection_end: Optional[str] = None # Date in a way pd.to_datetime can parse it

    author: Optional[str] = None # AUthor of the metafile

    ########################################################
    # Data source(s)
    ########################################################

    # Single input file
    file: Optional[str] = None # Name of the file, with relative path from this json file
    read_opts: Optional[Dict] = None # Additional options to pass to reading function

    # Multiple files
    files: Optional[List[FileDesc]] = None

    ########################################################
    # Data processing
    ########################################################

    # Main meat of data annotations
    structure: BlockSpec

    # A set of values that can be referenced in the file below
    constants: Optional[Dict] = None

    # Different global processing steps
    preprocessing: Optional[Union[str,List[str]]] = None # Performed on raw data
    postprocessing: Optional[Union[str,List[str]]] = None # Performed after columns and blocks have been processed
    
    # Deprecated as this does not work well for lazy loading
    #virtual_preprocessing: Optional[Union[str,List[str]]] = None # Same as preprocessing, but only in virtual step
    #virtual_postprocessing: Optional[Union[str,List[str]]] = None # Same as postprocessing, but only in virtual step

    weight_col: Optional[str] = None # Column to use for weighting - overriden by model to population weight column

    # List of data points that should be excluded in alyses
    excluded: List[Tuple[int,str]] = [] # Index of row + str  reason for exclusion

    @model_validator(mode='before')
    @classmethod
    def replace_constants(cls, meta: Any) -> Any:
        return replace_constants(meta)

    @model_validator(mode='after')
    def check_file(self) -> Self:
        if self.file is None and self.files is None:
            raise ValueError("One of 'file' or 'files' has to be provided")
        return self


# %% ../nbs/06_validation.ipynb 11
def hard_validate(m):
    DataMeta.model_validate(m)

def soft_validate(m):
    try:
        DataMeta.model_validate(m)
    except ValueError as e:
        print(e)

# %% ../nbs/06_validation.ipynb 13
DataDescription = ForwardRef('DataDescription')
DataSpec = Union[str,DataDescription]

class SingleMergeSpec(PBase):
    file: DataSpec # Filename to merge with
    on: Union[str,List[str]] # Column(s) on which to merge
    add: Optional[List[str]] = None # Column names to add with merge. If None, add all.
    how: Literal['inner','outer','left','right','cross'] = 'inner' # Type of merge. See pd.merge

MergeSpec = Union[SingleMergeSpec,List[SingleMergeSpec]]

# Make sure MergeSpec results in a list, even if input is a singular SingleMergeSpec
def smc_ensure_list(v): return v if isinstance(v,list) else [v]
MergeSpec = Annotated[List[SingleMergeSpec],BeforeValidator(smc_ensure_list)]

# %% ../nbs/06_validation.ipynb 14
# This is the input for read_and_process_data, that allows some operations on top of data meta

class DataDescription(BaseModel):
    # NB! BaseModel not PBase to allow for extensions such as PopulationDescription
    file: Optional[str] = None # Single file to read
    files: Optional[List[Union[str,Dict]]] = None # Multiple files to parse
    data: Optional[Dict[str,Any]] = None # Alternative to file, files. Dictionary of column {name: values} pairs.
    preprocessing: Optional[Union[str,List[str]]] = None # String of python code that can reference df
    filter: Optional[str] = None # String of python code that can reference df and is evaluated as df[filter code]
    merge: MergeSpec = [] # Optionally merge another data source into this one
    postprocessing: Optional[Union[str,List[str]]] = None # String of python code that can reference df



# %% ../nbs/06_validation.ipynb 16
def soft_validate(m, pptype):
    try:
        pptype.model_validate(m)
    except ValueError as e:
        print(e)
